---
published: false
layout: post
title: "CLIP论文精读"
categories: 我的AI新书
date: 2023-09-21 00:00:00 +0800
excerpt: "多模态篇-CLIP"
---


**CLIP论文精读**
https://www.bilibili.com/video/BV1SL4y1s7LQ/?spm_id_from=333.999.0.0&vd_source=14011b7fd9822163ee46211772e57565

CLIP没有使用明确的标签，实际上采用了一个“句子-图片”配对的方式为模型提供了一定的监督。这解决了需要绝对标签进行训练的限制，增强了模型的泛化性能。

是不是可以理解为一条句子是图片标签的升级版，除了对图中主体的标识外，提供了一个更加完整的语义信息。模型也确实达到了非常好的性能，说明语言作为指导视觉数据训练的想法是非常好的，但是收集制作一个如此规模的高质量地语言图片配对数据集是不是比简单的标签标注更加的费力呢？

文章的本意之一是想达到像NLP领域中，可以随便拿来语言，不用过多处理和标注就能训练的目的，但是收集到大量的、高质量的句子和图片配对的数据这个事情这是不是与上述的出发点相反呢？

Answer：是的，一个句子可以理解成单个标签的升级版。至于label cost，比起直接让人工去label（比如让人看图说话），直接从网上爬还是轻松很多的。但如果为了保证高质量，每个图片文本对都有人去校对的话，那这个cost还真有可能比简单的分类标注要费力。至于CLIP的出发点，他还是想卖图片与文本结合的威力，比如强大的zero shot transfer，并不是卖无监督，所以可能不算相反吧。