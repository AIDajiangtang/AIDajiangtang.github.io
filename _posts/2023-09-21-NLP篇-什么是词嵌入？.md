---
published: false
layout: post
title: "字节对编码（Byte-Pair Encoding，BPE）：基于子词的分词算法"
categories: 我的AI新书
date: 2023-09-21 00:00:00 +0800
excerpt: "字节对编码（Byte-Pair Encoding，BPE）：基于子词的分词算法"
---



1、什么是词嵌入
假如现在我们有如下三个单词：

1、apple.
2、orange.
3、iphone.
现在我们要将其聚成两类（当然现实中不可能只有三个样本来聚类，这里只是举一个例子为了方便），应该怎么做？

一般地，第一步我们会将每一个单词转换为一个onehot向量，假如我们使用的词典有10000个单词，那么每个单词转换为onehot向量后都是一个10000*1的向量，这个向量中只有单词所在的位置为1其余位置都为0；第二步便可以将这三个单词进行比较相似度了（采用余弦相似度或者欧式距离等）。如不出意外，我们发现这三个单词两两的相似度是相同的，那这是否意味着这3个句子都是不相关或者相关度是一样的呢？显然不是的。为什么会这样呢，我们在下面进行讨论。

在将这三个句子进行聚类时，我们首先将这个句子中的每个单词进行了onehot向量化，但是这个onehot向量把每个词都孤立起来，这样使得算法对相关词的泛化能力不强。例如apple和orange都属于水果，他们的形状都是圆的，都是可以可吃的等等具有多个相似性的特征，而iPhone属于手机，形状一般是方形，但是向量化onehot后并不能将这些特征表示出来，它只是显示出他们在词典中的位置。显然，如果我们能换一种表示，将这些单词的特征都表示出来，对我们的聚类无疑是非常有帮助的。

词嵌入（word embedding）便是这样将一个单词的表示特征化的技术。词嵌入相当于将一个单词嵌入到一个多维的特征矩阵中，在每个维度的的值代表着这个单词在这个维度的特征。下面通过另一个例子来更具体的看看词嵌入是怎么表达出词的相关性的。

<img src="assets/images/new book/NLP/word embedding/1.webp?raw=true" >

使用特征来表示每个单词
如上图，我们使用特征来表示各个单词：man、women、king、queen、apple、orange。

对于这些词，比如我们想知道这些词与 Gender（ 性别）的关系。假定男性的性别为-1，女性的性别为+1，那么 man 的性别值可能就是-1，而 woman 就是-1。最终根据经验 king 就是-0.95， queen 是+0.97， apple 和 orange 没有性别可言。

另一个特征可以是这些词有多 Royal（ 高贵），所以这些词， man， woman 和高贵没太关系，所以它们的特征值接近 0。而 king 和 queen 很高贵， apple 和 orange 跟高贵也没太大关系。

那么 Age（ 年龄）呢？ man 和 woman 一般没有年龄的意思，也许 man 和 woman 隐含着成年人的意思，但也可能是介于 young 和 old 之间，所以它们（ man 和 woman）的值也接近 0。而通常 king 和 queen 都是成年人， apple 和 orange 跟年龄更没什么关系了。

还有一个特征，这个词是否是 Food（ 食物）， man 不是食物， woman 不是食物， king和 queen 也不是，但 apple 和 orange 是食物。当然还可以有很多的其他特征，从 Size（ 尺寸大小）， Cost（ 花费多少），这个东西是不是 alive（ 活的），是不是一个 Action（ 动作），或者是不是 Noun（ 名词）或者是不是 Verb（ 动词），还是其他的等等。

所以你可以想很多的特征，为了说明，我们假设有 300 个不同的特征，这样的话你就有了这一列数字（上图编号 1 所示），这里只写了几个，实际上是 300 个数字，这样就组成了一个 300 维的向量来表示 man 这个词。

假设词典的单词量为10000，所以，我们的词表征矩阵（或称为特征矩阵）就是这么一个300*10000的矩阵，而词嵌入（word embedding）的过程其实也就是将这个词特征矩阵与onehot后的词向量相乘，这样就得到了某个单词的特征向量。你可以将某个词的词嵌入过程视为将某个单词嵌入到一个300维的特征空间中的一个点，这也是术语嵌入的来源。那么词表征矩阵是怎么得来的呢，实际上他也是通过神经网络学习得到的，后面将会介绍。

2、使用词嵌入
如果你没有学习过NLP和词嵌入，给你一个句子"I visited BeiJing last week"，使用RNN从其中提取地点名，最常见的做法就是将这个句子中的每个词转换为一个onehot向量后输入到RNN中，但是现在有了词嵌入后，我们在讲各个词送入RNN之前一般都要先将其乘以一个特征矩阵，以特征化的表示各个单词，然后将其送入RNN中。

参考：深度学习吴恩达
https://lilianweng.github.io/posts/2017-10-15-word-embedding/



BOW词袋：稀疏编码
N-Gram:稀疏编码
TF-IDF：稀疏编码
Word2Vec:神经网络，稠密编码
包括：CBOW，Skip-Gram
CBOW从上下文预测当前词,Skip-Gram从当前词预测上下文。