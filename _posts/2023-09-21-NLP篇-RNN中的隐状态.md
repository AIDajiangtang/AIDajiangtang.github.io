---
published: false
layout: post
title: "字节对编码（Byte-Pair Encoding，BPE）：基于子词的分词算法"
categories: 我的AI新书
date: 2023-09-21 00:00:00 +0800
excerpt: "字节对编码（Byte-Pair Encoding，BPE）：基于子词的分词算法"
---

隐状态一个很好的比喻是：人要说的话是受思想支配的，其他人可以听到你说的话，但无法直接看到你的思想，所以，你的思想是隐状态，说的话是观测变量


既然已经有马尔可夫模型和自回归网络处理时时序数据，为什么还需要RNN，Transformer呢？马尔可夫模型有什么缺点？后者又有什么优势？

尽管马尔可夫模型和自回归网络都可以处理时间序列数据,但它们有以下缺点:

马尔可夫模型基于概率图模型,需要人工构建状态图,不够灵活。参数估计比较困难,不适合大规模时间序列。
传统自回归模型如ARIMA等都是线性模型,表达能力有限,无法捕捉时间序列中的非线性模式。
马尔可夫模型和自回归模型难以处理长期依赖问题,无法记住长期上下文。
相比之下,RNN和Transformer有以下优势:

RNN通过循环结构捕捉时间序列的连续性,并通过隐状态传递长期信息。
RNN和Transformer都运用了神经网络,具有强大的非线性表达能力,可自适应地学习时间序列的复杂模式。
Transformer引入了自注意力机制,更好地建模了时间序列的全局上下文和长距离依赖。
RNN和Transformer都可以端到端训练,不需要人为构建图模型。
计算效率更高,可扩展到大规模时间序列场景。
总之,RNN和Transformer克服了马尔可夫和自回归模型的局限,可获得更强大和灵活的时序建模能力,因此被广泛使用。但在某些场景下,马尔可夫和自回归模型由于解析性和简单性,也具有其价值。