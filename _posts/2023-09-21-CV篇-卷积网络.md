---
published: false
layout: post
title: "卷积的前世今生"
categories: 我的AI新书
date: 2023-09-21 00:00:00 +0800
excerpt: "卷积的前世今生"
---

**卷积的历史**
卷积是一种数学运算，它可以用来描述两个函数之间的相互作用，或者说是两个函数的重叠程度。卷积在信号处理、图像处理、机器学习等领域有着广泛的应用。卷积的历史可以追溯到18世纪，以下是一些卷积的重要发展时间点：

1754年，达朗贝尔在推导泰勒定理时首次使用了类似卷积的表达式1。
1797年-1800年，西尔维斯特·拉克鲁瓦在他的著作《微分与级数论文》中使用了类型的表达式2。
19世纪初，拉普拉斯、傅里叶和泊松等人在他们的著作中使用了卷积运算2。
1903年，卷积这个术语首次出现，但是其定义在早期使用中是相当生僻的2。
1950年代或1960年代，卷积运算开始广泛使用，特别是在信号处理和系统分析中2。
1989年，LeNet是第一个使用卷积神经网络（CNN）进行手写数字识别的模型3。
2012年，AlexNet在ImageNet图像分类竞赛中大放异彩，开启了深度学习的热潮3。
2014年，GoogleNet和VGG分别提出了Inception模块和深层网络结构，提高了CNN的性能
2015年，ResNet提出了残差连接的概念，解决了深度网络的退化问题。
2016年，DenseNet提出了密集连接的概念，进一步增强了特征的传递和重用。
2017年，CapsuleNet提出了胶囊网络的概念，试图解决CNN的视角不变性和姿态敏感性的问题。
2018年，MobileNetV2提出了倒置残差模块，优化了移动端的CNN性能。
2019年，EfficientNet提出了一种基于复合缩放的方法，均衡了网络的深度、宽度和分辨率。
2020年，Vision Transformer提出了一种基于自注意力机制的视觉模型，挑战了CNN在图像分类任务上的优势。
2021年，ViTGAN提出了一种基于Vision Transformer的生成对抗网络，实现了高质量的图像生成。


**卷积在信号，图像处理中的应用**

在信号处理中，卷积可以用来分析线性时不变系统（LTI）的性质和响应。一个LTI系统就是一个输入信号和输出信号之间的映射，它满足两个条件：线性性和时不变性。线性性意味着系统对输入信号的加权和等于输出信号的加权和；时不变性意味着系统对输入信号的响应不随时间而改变。一个LTI系统可以用一个单位冲激响应（impulse response）来完全描述，它表示当输入信号为一个单位冲激（即只在某一时刻为1，其他时刻为0的信号）时，系统的输出信号。那么，当输入信号为任意形式时，系统的输出信号就等于输入信号和单位冲激响应的卷积。这样，卷积就可以用来计算LTI系统的输出，或者反过来，根据输入和输出信号推断出LTI系统的单位冲激响应。

在图像处理中，卷积可以用来实现各种图像滤波器（filter），即对图像进行某种特定的变换或增强。一个图像滤波器就是一个小矩阵，称为卷积核（kernel）或滤波器（filter），它在输入图像上滑动，并对每个位置的像素值进行加权求和，得到输出图像的相应位置的像素值。这样，输出图像就是输入图像和卷积核的卷积。不同的卷积核可以实现不同的效果，例如平滑、锐化、边缘检测、模糊等。

在机器学习中，卷积神经网络（CNN）是一种利用卷积运算来提取特征和进行分类或回归的深度学习模型。CNN由多个卷积层、池化层、全连接层等组成，可以处理高维的输入数据，例如图像、语音、文本等。CNN通过学习不同层次和抽象程度的特征，可以实现高效和准确的识别、检测、分割等任务。

以上就是我对卷积在信号和图像处理领域中的一些应用的简要介绍，希望对你有所帮助。


**卷积核的参数是怎么来的？**
卷积核的参数反映了卷积核的功能和作用。卷积核的参数有两种来源：

- 一种是人为设计的，根据一些数学原理或者图像特征，选择合适的卷积核参数来实现某种特定的效果，例如平滑、锐化、边缘检测等。这种卷积核的参数是固定的，不会随着训练而改变。
**传统的图像处理**

- 另一种是通过训练得到的，根据数据和目标函数，利用反向传播算法来更新卷积核的参数，使得卷积核能够提取出有用的特征，提高模型的性能。这种卷积核的参数是可变的，会随着训练而优化。
**卷积神经网络**


**CNN有什么特点，为什么它在CV中这么有用？**
卷积神经网络（CNN）是一种深度学习的算法，它可以直接处理图像、语音或音频信号等高维的输入数据，提取出有用的特征，并进行分类或回归等任务。CNN有以下几个特点：

CNN由多个卷积层、池化层、全连接层等组成，形成一个分层的结构。卷积层负责执行卷积运算，提取输入数据中的局部特征；池化层负责执行下采样操作，降低数据的维度和复杂度；全连接层负责执行分类或回归等任务，输出最终的结果。
CNN使用了参数共享和稀疏连接的技巧，大大减少了网络中的参数数量和计算量。参数共享指的是卷积层中的每个过滤器（filter）在不同位置上使用相同的权重；稀疏连接指的是每个输出单元只与输入数据中的一小部分区域相连，而不是与所有输入单元相连。
CNN具有平移不变性（translation invariance），即当输入数据发生平移变换时，CNN可以识别出相同的特征。这是因为CNN通过卷积运算和池化运算，可以保留输入数据中的空间信息，并且对位置的微小变化不敏感。
CNN可以端到端地训练，即从原始输入数据到最终输出结果，只需要一次前向传播和一次反向传播，无需人工干预或预处理。这使得CNN可以自动地学习到适合特定任务的特征，而不需要人为地指定或选择特征。
CNN在计算机视觉中非常有用，因为它可以有效地处理图像这种高维、结构化、含有丰富信息的数据。CNN可以从图像中提取出各种层次和抽象程度的特征，从简单的边缘、颜色、纹理到复杂的物体、场景、语义等。CNN可以应用于各种计算机视觉任务，例如图像分类、物体检测、人脸识别、图像分割、图像生成等，并且在许多任务中达到了最先进（state-of-the-art）的性能。12345


**前面提到CNN具有平移不变性，那如果是一个分类网络，也就是后接FC层，如果不做数据增强，物体平移后，还能分类正确吗？**
你好，这是一个很有意思的问题。CNN的平移不变性并不是绝对的，而是有一定的限制和条件。如果物体的平移量很小，或者在网络的降采样倍数的整数倍范围内，那么CNN可能还能正确地分类。但是如果物体的平移量很大，或者超出了网络的降采样倍数的整数倍范围，那么CNN就可能分类错误。这是因为CNN本身的结构只能带来非常弱的平移不变性，而真正能学习到平移不变性的主要是网络的参数。而网络的参数又依赖于数据和数据增强。如果数据集中物体的平移分布不均匀，或者没有使用合适的数据增强方式（比如裁切），那么CNN就难以学习到平移不变性。你可以参考以下几个网页来了解更多关于CNN平移不变性的研究和讨论：


**都有哪些类型的CNN，每种CNN的特点，提出时间，背景，以及要解决的问题**
CNN是一种深度学习的算法，它可以处理高维的输入数据，如图像、语音、文本等，并提取出有用的特征。CNN的发展历史可以追溯到上世纪八九十年代，从最早的Neocognitron到现在的各种变体，CNN在不同的任务和领域都有着广泛的应用。以下是我对一些比较经典和新颖的CNN类型的简要介绍：

Neocognitron1：这是CNN的鼻祖，由日本学者福岛邦彦于1980年提出，受到生物视觉系统的启发，采用了分层结构和局部连接方式，模拟了人类对图像特征的提取过程。Neocognitron由多个S层和C层组成，S层负责检测特定模式，C层负责对位置进行平移不变性处理。Neocognitron主要用于手写数字识别等任务。
LeNet-52：这是CNN的第一个成功实现，由法国学者Lecun于1998年提出，结合了反向传播算法和梯度下降法来训练网络参数。LeNet-5由两个卷积层、两个池化层和三个全连接层组成，主要用于美国邮政编码识别等任务。
AlexNet3：这是CNN的一个历史突破，由加拿大学者Krizhevsky于2012年提出，在ImageNet图像分类竞赛中大幅度超越了其他算法。AlexNet由五个卷积层、三个池化层和三个全连接层组成，使用了ReLU激活函数、Dropout正则化、数据增强等技术，同时利用了GPU并行计算来加速训练过程。
VGG4：这是一种简单而有效的CNN架构，由英国学者Simonyan于2014年提出，也在ImageNet图像分类竞赛中取得了优异的成绩。VGG主要特点是使用了多个3*3的卷积核来代替大尺寸的卷积核，从而增加了网络深度和非线性程度。VGG有多个版本，其中VGG-16和VGG-19最为常用。
GoogLeNet5：这是一种创新性的CNN架构，由谷歌学者Szegedy于2014年提出，也在ImageNet图像分类竞赛中获得了冠军。GoogLeNet引入了Inception模块，即在同一层中使用不同尺寸的卷积核和池化操作来提取多尺度的特征，并通过1*1的卷积核来降低通道数和计算量。GoogLeNet还使用了全局平均池化层来代替全连接层，从而减少了参数数量和过拟合风险。
ResNet：这是一种突破性的CNN架构，由微软学者He于2015年提出，也在ImageNet图像分类竞赛中刷新了记录。ResNet引入了残差连接（residual connection）的概念，即在每个卷积块中添加一个跨层的直接连接，使得输出等于输入加上残差函数。这样可以解决深度网络的退化问题（degradation problem），即随着网络深度增加而导致性能下降。ResNet可以训练非常深的网络，最深的版本达到了152层。


**卷积都有哪些类型？每种卷积的特点，要解决的问题**
常见的卷积类型及其特点和应用如下:

普通卷积(Standard Convolution):最基本的卷积,通过卷积核提取空域特征。
1*1卷积：通常用于通道数降维。
扩张卷积(Dilated Convolution):使用空洞卷积核来扩大感受野,保持分辨率。
深度可分离卷积(Depthwise Separable Convolution):分两步完成,减少参数量。
组卷积(Group Convolution):将通道分组,在组间不交互,降低复杂度。
转置卷积(Transpose Convolution):用于上采样,常在解码器中使用。
空间卷积(Spatial Convolution):在二维特征图上进行操作,最常见的卷积类型。
时序卷积(Temporal Convolution):在一维时间序列上进行的卷积。
因果卷积(Causal Convolution):无后效性,仅使用当前及之前的输入,常在transformer中使用。
可变形卷积(Deformable Convolution):可适应特征空间不对齐情况。
选择合适的卷积类型可以提高网络对不同类型数据的建模能力,是卷积网络设计的重要手段。

**请概括一下CNN的前向计算和反向传播过程**
CNN的前向计算和反向传播过程是这样的：

前向计算是指从输入层到输出层，按照网络的结构，依次执行卷积、激活、池化等操作，得到网络的输出和预测值。前向计算的目的是根据输入数据和网络参数，计算出网络的输出，并与真实标签进行比较，得到损失函数的值。
反向传播是指从输出层到输入层，按照网络的结构，依次计算损失函数对每一层的参数和激活值的偏导数（梯度），并根据梯度下降法或其他优化算法，更新网络的参数。反向传播的目的是根据损失函数的值，调整网络参数，使得损失函数最小化。
具体来说，CNN的前向计算和反向传播过程可以分为以下几个步骤：

- 前向计算：

输入层：将输入数据（如图像）转换为一个三维张量（行×列×通道数），并作为下一层的输入。
卷积层：将一个四维张量（卷积核数×行×列×通道数）作为卷积核，在输入张量上进行卷积运算，得到一个新的三维张量（行×列×卷积核数），并作为下一层的输入。卷积运算可以提取输入数据中的局部特征。
激活层：对卷积层的输出张量逐元素地应用一个非线性函数（如ReLU、Sigmoid等），得到一个新的三维张量，并作为下一层的输入。激活函数可以增加网络的非线性程度，使得网络能够拟合复杂的函数。
池化层：对激活层的输出张量进行下采样操作（如最大池化、平均池化等），得到一个新的三维张量，并作为下一层的输入。池化操作可以降低数据的维度和复杂度，同时保留重要的特征信息。
全连接层：将池化层或卷积层的输出张量展平为一个一维向量，并与一个权重矩阵相乘，再加上一个偏置向量，得到一个新的一维向量，并作为下一层或输出层的输入。全连接层可以执行分类或回归等任务，输出最终的结果。
输出层：根据不同的任务，对全连接层或卷积层的输出向量进行相应的处理，如使用Softmax函数进行多分类，使用Sigmoid函数进行二分类等，并得到预测值。
损失函数：根据预测值和真实标签，计算出损失函数（如交叉熵、均方误差等）的值，反映了网络输出与期望之间的差距。

- 反向传播：

输出层：根据损失函数对输出层激活值（即预测值）的偏导数，计算出输出层误差（即损失函数对输出层未激活值（即z）的偏导数），并传递给上一层。输出层误差反映了输出层对损失函数值变化的贡献程度。
全连接层：根据上一层传递过来的误差和当前层权重矩阵，利用链式法则，计算出当前层误差，并传递给上一层。同时根据当前层误差和当前层激活值，利用链式法则，计算出当前层权重矩阵和偏置向量的梯度，并根据优化算法，更新当前层的参数。
池化层：根据上一层传递过来的误差和当前层的池化方式，将误差还原到与当前层输入张量相同的尺寸，并传递给上一层。如果是最大池化，那么只有池化区域中的最大值对应的位置有误差，其余位置为零；如果是平均池化，那么每个位置的误差都为池化区域误差的平均值。
激活层：根据上一层传递过来的误差和当前层激活函数的导数，利用链式法则，计算出当前层误差，并传递给上一层。
卷积层：根据上一层传递过来的误差和当前层卷积核，利用卷积运算的性质，计算出当前层误差，并传递给上一层。同时根据当前层误差和当前层输入张量，利用卷积运算的性质，计算出当前层卷积核和偏置向量的梯度，并根据优化算法，更新当前层的参数。
输入层：根据上一层传递过来的误差，得到输入数据对损失函数的影响程度。


**对于CNN，输入图像如果不进行归一化操作会有什么影像？**
对于CNN，输入图像如果不进行归一化操作会有什么影响？这是一个很好的问题，归一化操作是指将输入图像的像素值缩放到一个较小的范围，例如0到1或者-1到1，通常可以通过除以255或者减去均值除以标准差来实现。归一化操作的目的是为了使输入图像的数值分布更加均匀和稳定，从而有利于网络的训练和收敛。如果不进行归一化操作，可能会导致以下几个问题：

如果输入图像的像素值过大，例如在0到255之间，那么在卷积层和全连接层中，会产生很大的内积和加权和，从而使得激活函数的输入过大，导致激活函数饱和或者梯度消失。这样会影响网络的非线性表达能力和梯度传播效率。
如果输入图像的像素值分布不均匀，例如有些通道或者区域的像素值较大，有些较小，那么在卷积层和池化层中，会导致特征提取的不平衡和不稳定。这样会影响网络对输入图像的特征表示和学习能力。
如果输入图像的像素值与网络参数的初始值或者优化算法的学习率不匹配，例如像素值过大而参数初始值或者学习率过小，那么在训练过程中，会导致网络参数更新缓慢或者震荡。这样会影响网络的收敛速度和性能。
因此，为了避免上述问题，一般建议对输入图像进行归一化操作，使得输入图像的数值分布更加接近于标准正态分布或者均匀分布。这样可以提高网络的训练效率和泛化能力。你可以参考以下几个网页来了解更多关于归一化操作的原理和方法：

https://www.zhihu.com/question/359146314
https://www.zhihu.com/question/293640354
https://www.zhihu.com/question/59430291


**如果训练时对图像进行了归一化，而推理时没有归一化，会有什么后果？**
如果训练时对图像进行了归一化，而推理时没有归一化，会有什么后果？这是一个很重要的问题，因为归一化是一种常用的数据预处理方法，它可以加快神经网络的训练速度和提高泛化能力。归一化的原理是将输入图像的像素值缩放到一个较小的范围，例如0到1或者-1到1，通常可以通过除以255或者减去均值除以标准差来实现。归一化的目的是为了使输入图像的数值分布更加均匀和稳定，从而有利于网络的训练和收敛。

如果训练时对图像进行了归一化，而推理时没有归一化，会有什么后果呢？简单来说，就是会导致网络的输出结果不准确或者不稳定。这是因为：

如果训练时对图像进行了归一化，那么网络的参数就是根据归一化后的数据来学习和调整的，网络对输入数据的分布有了一定的假设和适应。如果推理时没有归一化，那么输入数据的分布就会与训练时不同，可能会超出网络的预期范围，导致网络无法正确地识别或者响应输入数据中的特征和信息。
如果训练时对图像进行了归一化，那么网络的输出结果也是基于归一化后的数据来生成的，网络对输出数据的分布也有了一定的假设和适应。如果推理时没有归一化，那么输出数据的分布就会与训练时不同，可能会偏离真实标签或者期望值，导致网络无法正确地评估或者优化输出结果。
因此，为了保证网络的性能和稳定性，我们应该在训练和推理时都对图像进行相同的归一化操作。这样可以保证输入和输出数据的分布一致性，使得网络能够更好地学习和预测。你可以参考以下几个网页来了解更多关于归一化操作的原理和方法：

