---
published: false
layout: post
title: "预告"
categories: 我的AI新书
date: 2023-09-21 00:00:00 +0800
excerpt: "预告"
---


数值稳定性



神经网络的梯度




t表示第t层
h（t-1）表示第t-1层的输出
所有的h都是向量，向量关于向量的导数是一个矩阵
太多的矩阵乘法会导致：





举例：MLP




梯度爆炸：

梯度爆炸所带来的问题

16位浮点数最大的问题是它的数值区间比较小
不是不能训练，只是对于学习率的调整比较难调
梯度消失：



梯度消失的问题

对顶部的影响比较小，因为做矩阵乘法的次数比较少
仅仅顶部层的训练的较好，无法让神经网络更深，就等价于是浅层的神经网络








总结




尽量避免梯度过大或者过小








让训练更加稳定












合理的权重初始化和激活函数



让每一层的方差是一个常数


E：均值
Var：方差




权重初始化




在训练开始的时候更容易有数值不稳定

iid：independent identically distributed，独立同分布
前一层的输出（当前层的输入）和当前层的权重是相互独立的
当X和Y相互独立时， E(XY)=E(X)E(Y)

输入方差和输出方差相同，则可以推出紫色圈出的内容





Xavier初始化




第一个条件是使得每次前向输出的方差是一致的
第二个条件是使得梯度是一样的
除非输入等于输出，否则无法同时满足这两个条件
γt：第t层的权重的方差
给定当前层的输入和输出的大小，就能确定权重所要满足的方差的大小
Xavier是常用的权重初始化方法：权重初始化的时候的方差是根据输入和输出维度来定的




在有激活函数的情况下




激活函数的输入和输出的方差有a^2倍的关系，激活函数如果将值放大a倍的话，它的方差会被放大a^2倍
如果要使激活函数不改变输入输出的方差，则a=1：为了使得前向输出的均值和方差都是均值为0，方差为固定的话，激活函数只能是β=0，a=1，即激活函数必须是等于本身



在0点附近的时候tanh和relu基本满足f(x)=x的关系，sigmoid需要做出调整
图中蓝色曲线是经过调整之后的sigmoid激活函数，调整之后就能够解决sigmoid激活函数本身所存在的一些问题








总结




使得每一层的输出和每一层的梯度都是均值为0，方差为固定数的随机变量
权重初始化可以使用Xavier，激活函数可以使用tanh和relu，如果选择sigmoid的话需要进行调整








----to be continued---- 作者：如果我是泡橘子 https://www.bilibili.com/read/cv14319738/?from=readlist&jump_opus=1 出处：bilibili