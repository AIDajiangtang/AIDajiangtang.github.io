---
published: false
layout: post
title: "0.序"
categories: 我的AI新书
banner:
  video: https://vjs.zencdn.net/v/oceans.mp4
  loop: true
  volume: 0.8
  start_at: 8.5
  image: https://bit.ly/3xTmdUP
  opacity: 0.618
  background: "#000"
  height: "100vh"
  min_height: "38vh"
  heading_style: "font-size: 4.25em; font-weight: bold; text-decoration: underline"
  subheading_style: "color: gold"
excerpt: "序"
top: 2

---

**是否能使用RadImageGAN，BigDatasetGAN生成MRI 头部数据用于训练胼胝体分割模型？**

RadImageNet，RadImageGAN，BigDatasetGAN是三个不同的项目，它们之间的联系是都涉及到了医学图像的生成和分析。具体来说：

- RadImageNet 是一个开放的医学图像数据库，包含了135万张CT、MRI和超声波图像，涵盖了多种器官和病理。RadImageNet还提供了一些预训练的卷积神经网络，可以用于医学图像的迁移学习。RadImageNet的目的是为医学图像人工智能应用提供一个有效的数据集和基准模型。
- RadImageGAN是一个基于GAN（生成对抗网络）的医学图像生成模型，可以从噪声中生成高质量的CT、MRI和超声波图像。RadImageGAN使用了RadImageNet作为训练数据，利用了GAN的强大能力来模拟真实的医学图像分布。RadImageGAN的目的是为医学图像增强、数据增广、模拟和教育等场景提供一个有用的工具。
- BigDatasetGAN是一个基于GAN的大规模数据集生成模型，可以从少量的样本中生成大量的新样本。BigDatasetGAN使用了一种新颖的自监督学习方法，结合了对比学习和循环一致性损失，来提高GAN的稳定性和多样性。BigDatasetGAN可以用于任何类型的数据集，包括医学图像、自然图像、文本等。BigDatasetGAN的目的是为数据稀缺或隐私敏感的领域提供一个有效的数据集扩充方法。




https://www.youtube.com/watch?v=byNaO_zn2fI  how to self study pure math



https://www.youtube.com/watch?v=NHszJiL-9pM   Generative AI in Biology and HealthCare



https://www.youtube.com/watch?v=q6kJ71tEYqM machine learning VS deep learning



https://catalog.ngc.nvidia.com/orgs/nvidia/teams/playground/models/clip Nvidia CLIP在线PlayGround


**如何在UNET中添加注意力机制的？**



**基于AI的图像生成路线**
基于AI的图像生成技术是指利用人工智能技术，根据给定的数据或条件，生成新的图像的过程。这种技术有很多应用领域，例如平面设计、游戏制作、动画制作、医学影像、化合物合成等。目前，主流的图像生成技术主要有以下几种：

- 生成对抗网络（GAN）：这是一种利用两个神经网络进行博弈训练，使生成器能够产生逼真的图像的方法。GAN有很多变种和改进，例如条件GAN、自编码器GAN、循环GAN等。
- 扩散模型（Diffusion Model）：这是一种通过向数据添加随机噪声，然后逐步去除噪声来生成图像的方法。扩散模型能够更好地保留图像细节和真实性，但也存在采样速度慢和泛化能力弱的问题。
- 自回归模型（Autoregressive Model）：这是一种利用Transformer结构中的自注意力机制来对图像进行逐像素生成的方法。自回归模型能够提高图像生成的准确性和合理性，但也存在推理速度慢和训练成本高的问题。
AE-DAE-VAE-VQVAE
- 对比学习模型（Contrastive Learning Model）：这是一种利用文本-图像跨模态预训练来建立文本和图像之间的联系的方法。对比学习模型能够根据文本描述生成符合语义的图像，同时提高图像的多样性和风格化。
以上是我从网上搜索到的一些关于基于AI的图像生成技术的信息123




**The worlds I see by Li Fei Fei**
Commented by Geoffrey Hinton
Fei-Fei Li has written a book. She was the first computer vision researcher to truly understand the power of big data and her work opened the floodgates for deep learning. She delivers a clear-eyed account of the awesome potential and danger of AI.


**集成了GPT4和DALLE-3的好网站**
https://rochat.ai/




**2023基于胼胝体分割的自动定位像任务**
原始PaddlePaddle版本2.1.2和PaddleSeg版本2.4.0

2023.11.14号，PaddlePaddle版本2.4.0和PaddleSeg版本2.6.0，python版本3.7，状态：成功

一定要去官方网站查PaddlePaddle和PaddleSeg的匹配情况

通过下面代码支持导出动态输入大小的C++推理模型，但输入图像尺寸不能相差太大，否则会影响效果。
from paddleseg.models import BiSeNetV2
model = BiSeNetV2(num_classes=2,
                 lambd=0.25,
                 align_corners=False,
                 pretrained='/home/aistudio/Myoutput2/best_model/model.pdparams')
model.eval()
from paddle.static import InputSpec
import paddle
path = 'func/model'
paddle.jit.save(
    model,
    path,
    input_spec=[
        InputSpec(
            shape=[1,3, -1,-1], dtype='float32', name='x'),
    ])


**全脊柱拼接ROI如何支持旋转**




1.如何通过mousedown，mousemove，moouseup计算鼠标绕某个中心点的旋转角度
假设我们要计算鼠标围绕中心点 (cx, cy) 的旋转角度,可以这么实现:

在mousedown时,记录起始点位置(x1, y1)
在mousemove时,获取当前点位置(x2, y2)
计算起始点到中心点的向量v1 = (x1 - cx, y1 - cy)
计算当前点到中心点的向量v2 = (x2 - cx, y2 - cy)
使用向量叉积计算两个向量的夹角:
js

Copy code

let v1 = [x1 - cx, y1 - cy];
let v2 = [x2 - cx, y2 - cy];

let cross = v1[0]*v2[1] - v1[1]*v2[0];
let angle = Math.atan2(cross, v1[0]*v2[0] + v1[1]*v2[1]);
在mouseup时,angle就是鼠标拖拽的总旋转角度
将弧度转为角度的方法同上
这种方法通过计算向量之间的叉积,可以很好地处理鼠标围绕中心点的旋转角度计算。

2.单独旋转定位框
// 创建Graphics对象
Graphics g = this.CreateGraphics();

// 保存当前坐标系统 
g.Save();

// 旋转坐标系45度
g.RotateTransform(45);  

// 绘制旋转后的矩形
g.DrawRectangle(Pens.Black, 100, 100, 200, 100);

// 恢复坐标系统
g.Restore();

或者使用
// 旋转角度
double angle = 45;

// 计算旋转矩阵
double cos = Math.Cos(angle);
double sin = Math.Sin(angle);
double[,] rotateMatrix = {
    {cos, -sin},
    {sin, cos}
};

// 矩形的4个顶点
Point[] points = { new Point(100, 100), new Point(300, 100), 
                   new Point(300, 200), new Point(100, 200) };

// 变换顶点坐标
for (int i = 0; i < 4; ++i)
{
    double x = points[i].X;
    double y = points[i].Y;
    points[i].X = x * cos - y * sin;
    points[i].Y = x * sin + y * cos;
}

// 绘制变换后的矩形
g.DrawPolygon(Pens.Black, points);

或者
GraphicsPath path = new GraphicsPath();
path.AddRectangle(new Rectangle(100, 100, 200, 100));

// 设置旋转矩阵
Matrix m = new Matrix();
m.Rotate(45);
path.Transform(m);

// 绘制路径  
g.DrawPath(Pens.Black, path);



https://youtu.be/Q1yu6TQZ79w?si=rSVPrfz-QIulE8pi 假设检验
https://youtu.be/c06FZ2Yq9rk?si=tdVHRyc3WHvFmwwa 概率基础知识
https://youtu.be/xTpHD5WLuoA?si=A6Y0hQJEBcs4snYM 相关性和回归


https://mp.weixin.qq.com/s/_n63gt7lRZdiaZGFIk4-fQ  十大优化算法


C++Release模式下断点调试：
调试信息格式：程序数据库 (/Zi)
关闭优化
生成调试信息


**3D动画制作工具**
开源免费
Blender
https://github.com/blender/blender?tab=readme-ov-file


**与Blender一样开源免费的工具还有哪些，并给出一个排名**

与Blender类似的开源且免费的3D制作工具包括:

Blender:功能完善,社区活跃,插件丰富,被誉为3D创作的瑞士军刀。
MakeHuman:专注于人物建模和人脸渲染。逼真的人体和面部特征使其在人物创作领域排名第二。
Wings 3D:操作简单易上手。虽然功能不如Blender,但轻量化的界面吸引了许多初学者。
MeshLab:聚焦于3D模型处理和编辑。可进行网格修复、渲染、纹理化等,但不含建模工具,被广泛用作模型后期处理工具。
OpenSCAD:使用编程语法进行各种几何体的建模。容易实现参数化建模和迭代优化,适用于工程和机械建模。
Daz Studio:侧重人物角色设计,提供大量预制资产。易上手但扩展性不如Blender。
综合功能、社区支持以及应用范围,Blender可以被公认为最佳开源免费3D制作工具。其余工具各有侧重,搭配Blender可发挥更大作用。

**Cinema 4D和Blender有什么区别？**
Cinema 4D和Blender都是很流行的3D建模和动画软件,主要有以下几点区别:

价格:Cinema 4D需要购买授权,价格昂贵,而Blender是开源免费软件。
操作界面:Cinema 4D的界面更加简洁干净,上手较为容易。Blender的界面更复杂,功能更丰富,学习曲线更加陡峭。
渲染能力:Cinema 4D自带的物理渲染器更加专业和易用。Blender需要依赖第三方渲染器,但功能也很强大。
视觉效果:Cinema 4D更擅长商业级的动画和视觉特效。Blender在角色、静态场景更占优势。
扩展能力:Blender的Python API非常强大,可以无限扩展。Cinema 4D的插件系统也比较成熟。
工作流:Cinema 4D与After Effects的联动更加紧密。Blender需要转换格式,轮廓也不如C4D稳定。
总的来说,Cinema 4D商业化程度更高,Blender更注重开源社区。喜欢自由开源的选Blender,想要简单好用的商业软件选C4D。

**在3D动画制作过程中，Modeling，Rendering，Shading的区别**
在3D动画的制作流程中,Modeling、Rendering和Shading这三个概念区别如下:

Modeling(建模)
指使用3D软件创建物体的几何形状和物理结构的过程。包括人物、场景、道具等的3D模型都需要经过建模师的Modeling。

Rendering(渲染)
指计算机根据模型的几何结构、材质、颜色、光照等数据,生成2D图像的过程。常见的渲染技术有光线追踪、路径追踪等。这是动画最终形成画面的关键步骤。

Shading(着色)
指为3D模型赋予材质、贴图和灯光效果的过程,以实现最终的视觉表现。涉及有色调、纹理、明暗、高光等方面。这在渲染前决定了最终效果的样子。

简单来说,Modeling决定了几何形状,Rendering生成图像,Shading实现视觉表现。它们是3D动画制作的三大基础环节。


**当我们说一台电脑是否支持OpeGL时，指的是操作系统还是CPU还是GPU硬件？**
当我们说一台电脑是否支持OpenGL时，指的是显卡（GPU）硬件和驱动程序是否支持OpenGL的特定版本。OpenGL是一个开放的图形库，它提供了一系列的接口，用于绘制2D和3D图形。不同的显卡和驱动程序可能支持不同的OpenGL版本，这会影响到一些图形应用程序的运行效果。如果您想知道您的电脑支持哪个版本的OpenGL


**不用图像函数库，仅用纯C#语言实现一个图像平移操作，使用双线性插值**
using System;
					
public class ImageTranslator {

  // Input image pixels
  private double[,] inputPixels;	
  private int width;
  private int height;

  // Output translated image pixels
  private double[,] outputPixels;
	
  // Translation distances  
  private double dx; 
  private double dy;

  public ImageTranslator(double[,] input, int w, int h) {
    inputPixels = input;
    width = w;  
    height = h;
  }

  // Set translation distance 
  public void SetTranslation(double x, double y) {
    dx = x;  
    dy = y;
    outputPixels = new double[width, height]; 
  }

  // Bilinear interpolation implementation
  private double Interpolate(double x, double y) {
    int x1 = (int) x;
    int x2 = x1 + 1;
    int y1 = (int) y;
    int y2 = y1 + 1;
		
    double f11 = GetPixel(x1, y1);
    double f12 = GetPixel(x1, y2);
    double f21 = GetPixel(x2, y1);
    double f22 = GetPixel(x2, y2);

    double fxy = (x2 - x) / (x2 - x1) * f11 + (x - x1) / (x2 - x1) * f21 + 
               (x2 - x) / (x2 - x1) * f12 + (x - x1) / (x2 - x1) * f22; 

    return fxy; 
  }

  private double GetPixel(int x, int y) {
    if (x < 0 || x >= width || y < 0 || y >= height)  
      return 0;	
    else
      return inputPixels[x, y];	
  }

  // Translate image 	
  public void TranslateImage() {				
    for (int x = 0; x < width; x++) {
      for (int y = 0; y < height; y++) {	
        double newX = x + dx;	
        double newY = y + dy;
        outputPixels[x, y] = Interpolate(newX, newY);  
      }
    }
  } 	
}


**20231229**
1.制作VR DICOM视频，上传到视频号
2.学习Hololens 2开发流程 youtube
3.自然语言处理入门 pdf，加强NLP基础
4.面向数据科学家的使用统计学 pdf，加强数据科学的基础



**20240102**
LLM Fine-Turning and Prompt Engineering White Papers
LLM Training


**20240103**
Speech Recognition：parakeet beats Whispe


**20240104**
微信公众号：万字通俗讲解大语言模型内部运行原理
https://www.understandingai.org/p/large-language-models-explained-with


**20240105**
Understand DeepLearning :12 Transformer and LLM
Sliders发微信公众号

https://huggingface.co/spaces/modelscope/AnyText：文本生成图片


神经辐射场NeRF

**20240108**
ChatGPT：Explained to kids
Understanding deeplearning :Chapter12 Transformer
欧姆社漫画机器学习系列

**20240109**
商君书：贫，疲，辱，弱，愚
墙：域名污染，IP:端口连接重置
【一起学AI】 6-AI就那点事:图像，表格

字节跳动版GPTs：Coze
试玩了一些模型

凯文凯利-失控 AI解读


根据我从网络上搜索到的信息，截止到2023年，中国一共有3275家三级医院，其中1651家是三级甲等医院，以及10848家二级医院


**20240110**
图像放大和增强
https://magnific.ai/
【一起学AI】 6-AI就那点事:文字，需要学习

98年房改，国有土地可以出让，土地财政，土地金融，房地产拉开序幕

计算机科学地图

小Lin说上市
好处：融资，套现，知名度，降低再融资门槛
坏处：股权稀释，信息公开，维护成本，上市周期
A股：大陆人民币股，上交所，北交所，深交所
3D扫描仪：输出点云，网格等

**20240111**
小Lin说游戏产业
游戏产业链
![不同游戏设备市场大小]({{ "assets/images/小Lin说/不同游戏设备市场大小.png" | relative_url }})

![游戏产业链]({{ "assets/images/小Lin说/游戏产业链.png" | relative_url }})

小Lin说英伟达
3D游戏
挖矿
人工智能
大模型

小Lin说退市
资源退市：重组，调整经营方向
强制退市：经营，违法

小Lin说Web3.0
Web1.0
1991年诞生：只读
搜狐
yahoo

Web2.0
2004年诞生：可写，可交互，留言，评论
FaceBook
电报
youtube

用户信息推送广告赚钱

Web3.0
信息不想被泄漏
区块链


**20240112**
编译gglm clip
Linux GNU支持可变长数组，MSVC不支持
编译whisper.cpp和whisper.net



AI+,即将AI作为核心技术与其它行业进行深度融合，随着生成式AI的出现，尤其是大语言模型，AI+也将步入一个新台阶。
与基础大模型相比，AI+更适合垂类大模型。

AI+医疗，更进一步，AI+医疗影像。

SAM：分割，标注
Whisper：语音识别，电子病历
CLIP：多模态
LLAMA：Chatbot，医生助理


**20240115**
深度学习革命
llama以及微调

**20240116**
深度学习革命
bert以及微调


**20240117**
深度学习革命
PhotoMaker
BLIP:图生文，Coca，BeiT


**20240118**
多模态论文
Transformer：从0开始
https://nlp.seas.harvard.edu/annotated-transformer/
.NetFramework中录音
https://github.com/naudio/NAudio 


**20240119**
AlphaGeometry 


**20240122**
Elucis: the VR platform for creating, visualizing, manipulating, and discussing patient-specific 3D medical models.

Hugging Face：Transformer

三种Tokenlization的优缺点：
https://huggingface.co/learn/nlp-course/chapter2/4?fw=pt


**20240123**
李宏毅 Transformer
https://www.bilibili.com/video/BV1v3411r78R

LLM技术路线：
https://github.com/mlabonne/llm-course?tab=readme-ov-file

LLM可视化：
https://bbycroft.net/llm


**20240124**
NamePipe client server
how chatGPT is trained?

西蒙普林斯推荐的书：
https://shepherd.com/best-books/machine-learning-and-deep-neural-networks

Probabilistic Machine Learning: An Introduction
https://probml.github.io/pml-book/book1.html


**20240125**
anylabeling
llama 2

Probabilistic Machine Learning chapt 8 细节搜索


**20240126**
Probabilistic Machine Learning: Advanced topics
https://probml.github.io/pml-book/book2.html

书的目录
由粗到细 coase to fine

CheXAgent:医疗领域多模态大模型
https://stanford-aimi.github.io/chexagent.html

Yann LeCun, Yoshua Bengio& Geoffrey Hinton
论文：DeepLearning

传统机器学习不擅长处理自然数据：图像，文本


所有大佬唯一留在学术界，坚持开源开放思想的人。
Yoshua Bengio所有资料
https://yoshuabengio.org/presentations/


**20240218**
图解机器学习
https://www.theinsaneapp.com/2021/11/machine-learning-algorithms-for-beginners.html

**diffusion-explainer**
https://poloclub.github.io/diffusion-explainer/

**各种可视化**
https://poloclub.github.io/

**CNN可视化**
https://adamharley.com/nn_vis/cnn/3d.html
https://github.com/aharley/nn_vis

**Vison Pro**
https://twitter.com/SteveFlanders22/status/1758491479461581211


**20240219**
langChain
ChatGLM
RAG

**20240220**
涌现
漫画趣味化学，物理，数学，语文启蒙书
倒排索引：搜索
Faiss：Facebook AI Similarity Search 快速向量相似性搜索

从openai提供的接口出发，看这些接口

llama-index ：创建数据索引

向量数据库

LangFlow:界面版LangChain


**20240221**
FreeSurfer：开源的MRI头部处理软件
找分割MRI胼胝体
https://github.com/freesurfer/freesurfer
https://surfer.nmr.mgh.harvard.edu/

B1 bias field correction 是一种用于改善磁共振图像质量的方法，它可以消除由磁场不均匀性导致的图像亮度变化。这种变化会影响图像中的组织对比度和灰度值，从而干扰图像分析和处理。B1 bias field correction 的原理是通过估计和除去图像中的偏置场（bias field），从而恢复图像的真实信号。B1 bias field correction 的方法有很多，例如基于信号模型和先验假设的传统方法，或者基于深度学习的数据驱动方法。如果您想了解更多关于 B1 bias field correction 的信息。

SynthStrip: Skull-Stripping for Any Brain Image
https://surfer.nmr.mgh.harvard.edu/docs/synthstrip/

https://github.com/freesurfer/freesurfer/tree/dev/mri_watershed

itk-snap
开源并且包括颅骨剥离算法


**20240222**
接下来几天学习opencv Mat的内存管理，ONNXRuntime推理参数设置。



**20240223**
LM Studio 本地推理大模型
接下来几天学习opencv Mat的内存管理，ONNXRuntime推理参数设置。


https://docs.opencv.org/4.x/de/d7a/tutorial_table_of_content_core.html


**20240228**
优化拼接算法

 Dropout 、LayerNorm 等接口在 train() 和 eval() 的行为存在较大的差异,所以在保存模型进行推理时，需要先切换到eval模式。


 **20240229**
 Globe Explorer:基于大模型的AI搜索引擎
 https://explorer.globe.engineer/


New breakthrough from Microsoft: 1-bit LLMs.
New models that use ternary values (-1, 0, 1) instead of 16-bit.
This makes them 2.7x faster, use 3.5x less GPU memory, and 71x less energy.
Bitnet also matches or outperformed traditional models like LLaMA 3B.

https://arxiv.org/abs/2402.17764v1


**20240301**
扩散模型DDPM
隐空间扩散模型LDM
stable diffusion

https://zhuanlan.zhihu.com/p/532402983


基于扩散模型的视频生成编辑
Runway Gen1
Runway Gen2

Lumeria


EMO：
Audio2Video
https://humanaigc.github.io/emote-portrait-alive/


**20240306**
仅从一张图片就能重建3D模型
https://huggingface.co/spaces/stabilityai/TripoSR

5个AI神器：
kimi chat
天工AI，AI搜索引擎
weshop，作画工具
海螺问问，声音克隆https://hailuoai.com/
灵感引擎


Alphafold：蛋白质结构预测，加速新药研发


**20240307**
chat with RTX视频
https://twitter.com/heyronir/status/1764886081126936756


https://huggingface.co/learn/nlp-course/zh-CN
抱抱脸NLP课程

结合NLP的处理流程
管道


**20240308**
Suno AI:
用AI生成音乐

run diffusion：
AI换脸
嘴型同步

光子技术CT的原理：
https://mp.weixin.qq.com/s/TTOcvmwdIhS7i__V08myUQ

光子计数探测器CT的基本原理

PCD由碲化镉、碲化镉锌或硅等半导体组成。入射的X射线量子被这种半导体吸收并直接转换成电信号。这是与以前用于医用CT的闪烁探测器的本质区别，在闪烁探测器中，X射线量子首先产生可见光，然后再转换成电信号。



**20240312**
1.推荐国产大模型kimi chat:
国产大模型后起之秀，性能不输chatgpt，claude2，免费，不需要魔法

2.整理一些AI搜索引擎：
与传统搜索引擎仅给出网页链接不同，AI搜索引擎能针对搜索结果进一步整理，总结。
Globe expolore：
https://explorer.globe.engineer
phind：
https://www.phind.com/
perplexity AI：
https://perplexity.ai/



**20240313**
deeplearning算法hudson网址
http://10.0.3.251:8080/

用户名和密码：
yubao
123

**20240318**
Grok开源了模型检查点和推理代码，但不包括训练代码
Grok-1 比其他通常带有使用限制的开放权重模型更加开源，但是它的开源程度不如 Pythia、Bloom 和 OLMo，后者附带训练代码和可复现的数据集。


OLMo:真正开源的大模型
https://allenai.org/olmo



**20240319**
DoMo AI：
视频风格转换
https://domoai.app/


360大模型：
智能浏览器：
输入视频路径，生成视频总结和思维导图

structure-from-motion，简称sfm，是一种计算机视觉技术，它通过分析多个从不同角度拍摄的同一场景的照片来重建该场景的三维结构。

Structure-from-Motion（SfM）和神经辐射场（Neural Radiance Fields，简称NeRF）是两种不同的三维重建技术，它们在处理三维场景重建时的方法和侧重点有所不同，但也可以相互补充。

SfM 是一种传统的三维重建技术，它主要依赖于从多个视角拍摄的二维图像来重建物体或场景的三维结构。SfFM的过程通常包括以下几个步骤：

特征点检测与匹配：在这一步，算法会在不同的照片之间找到相同的特征点，以便确定不同图像之间的对应关系。
相机位姿估计：通过特征点匹配，算法可以推算出拍摄每张照片时相机的位置和朝向。
三维点云重建：利用相机位姿和特征点匹配信息，重建出场景的三维点云。
网格重建与纹理映射：根据点云数据构建三维模型的网格，并将其与图像纹理相结合。
NeRF 是一种基于深度学习的新型三维重建技术，它使用神经网络来学习场景的连续体积表示，并通过这个表示来渲染新视角的图像。NeRF的关键特点包括：

隐式场景表示：NeRF使用一个神经网络来隐式地表示场景的体积密度和颜色。
体积渲染：通过体积渲染技术，NeRF可以从任意视角渲染出高质量的图像。
端到端学习：NeRF可以通过端到端的训练过程来优化神经网络的参数，以提高渲染图像的真实性。


**20240320**

在前面的多轮对话的聊天机器人中，为了获取更准确的回答，我们将历史聊天记录作为上下文传递给大模型，其实，这个模式也可以扩展到其它应用场景，今天我们来讲讲RAG，也就是检索增强生成。


像ChatGPT这类大模型，有时会产生幻觉，也就是针对用户的问题胡编乱造，产生这种现象的一个原可能因是对上下文的理解能力不足；除了幻觉问题，如果你问它最近的新闻，它可能完全不知道，因为它的训练数据只包括一两年以前的。

为了解决这个问题，像Copilot或者Kimi Chat都会先搜索互联网，然后根据搜索内容进行回答，我们称这种模式为搜索增强生成。

除了搜索互联网数据，也可以搭建本地知识库，这种模式被称为RAG，或者检索增强生成。


**20240321**
NVIDIA各种AI PlayGround
https://build.nvidia.com/explore/discover


详细阅读DDPM论文
而DDPM的本质作用，就是学习训练数据的分布，产出尽可能符合训练数据分布的真实图片。具备了产出逼真图片的能力，模型才可能在下一步中去学习语义信息(guidance)，进一步产生符合人类意图的图片。

字节跳动文字生成视频
https://huggingface.co/spaces/ByteDance/AnimateDiff-Lightning


**20240322**
什么是生成式AI？
https://www.nvidia.com/en-us/glossary/generative-ai/


**20240325**
Suno AI：AI生成音乐
GateKeep AI：AI教你学数学，物理

Open Pose:
通过图片识别姿势

筹划一个新的专栏：CV大模型原理与应用


**20240326**
Tools
getremixai: image generation 
TripoSR: 2D to 3D Object Conversion
Apple:Reality Composer Pro: to bring them together & add a few particle effects. 



**20240327**
2024年关于大语言模型的综述性论文
https://github.com/RUCAIBox/LLMSurvey


推理能力的基础模型
https://github.com/reasoning-survey/Awesome-Reasoning-Foundation-Models

**20240328**
Champ
基于图像生成3D动画
https://fudan-generative-vision.github.io/champ/#/
https://github.com/fudan-generative-vision/champ
https://arxiv.org/pdf/2403.14781.pdf



DriveEnv-NeRF：高逼真自动驾驶场景，离线测试！利用神经辐射场（NeRF）技术构建自动驾驶仿真环境的研究论文
论文链接：https://arxiv.org/pdf/2403.15791.pdf
项目链接：https://github.com/muyishen2040/DriveEnvNeRF


COLMAP：
COLMAP（Structure-from-Motion Reconstruction and 3D Modeling Software）是一个开源的软件，专门用于从多张图片中重建三维场景和进行结构光运动（Structure-from-Motion，简称SfM）分析。COLMAP能够处理大量的图片，通过识别和匹配图片之间的共同特征点，重建出场景的三维结构，并估计出相机的位置和方向。

能否用于图像拼接？


UniTS:
哈佛大学&MIT联合发布，最新时间序列统一大模型UniTS，秒杀各类时序任务！
下载地址：https://arxiv.org/pdf/2403.00131v1.pdf



**20240329**
LVM
CNN累加更多的层已经达不到性能提升了，Transformer不受Scale定律限制，CNN是局部注意力机制，Transformer是全局注意力

ViT：将Transformer从NLP搬到了CV领域
BeiT，MAE：将NLP中BERT无监督训练搬到了CV领域
CLIP：打通了语言和图像之间的鸿沟


LVM依旧遵循LLM的范式，先从大规模无监督数据中预训练，然后再用领域数据进行微调


如何获取图像表征？
有监督学习
CLIP：语言-图像对比预训练
自监督：对比学习，非对比学习，掩码学习


MAE，随机遮盖70%，仍能恢复原图，可见图像与自然语言相比，具有稀疏性


大模型综述性论文：
A Survey of Large Language Models.pdf
A Survey of Reasoning with Foundation Models.pdf
Multimodal Foundation Models Survery.pdf
A Survey on Visual Transformer.pdf
Vision-Language Models for Vision Tasks_a surey.pdf


为什么我们需要扩散模型：DDPM

两种模型都有不同的问题，虽然GANs擅长于生成与训练集中的图像非常相似的逼真图像，但VAEs擅长于创建各种各样的图像，尽管有产生模糊图像的倾向。但是现有的模型还没有成功地将这两种功能结合起来——创造出既高度逼真又多样化的图像。这一挑战给研究人员带来了一个需要解决的重大障碍。


**20240401**
App Vision Pro新玩法，发个视频
https://twitter.com/AngryTomtweets/status/1774388090762592656


InterDreamer: Zero-Shot Text to 3D Dynamic Human-Object Interaction
https://sirui-xu.github.io/InterDreamer/



**为什么说CNN是局部注意力？Transformer是全局注意力？**
CNN（卷积神经网络）和Transformer是两种不同的深度学习架构，它们在处理数据时的注意力机制有所区别。

CNN的局部注意力：

局部感受野：CNN通过卷积层来处理数据，每个卷积核只关注输入数据的一个局部区域，这个局部区域被称为感受野（receptive field）。这意味着CNN在任何给定的时间点只关注输入数据的一小部分，即局部特征。
层次化特征提取：CNN通常包含多个卷积层，每一层都会提取不同层次的特征。较低层次的卷积层可能关注边缘和角点等基本特征，而较高层次的卷积层则能够捕捉更复杂的模式和对象部分。
空间不变性：CNN通过池化层（如最大池化）来降低数据的空间分辨率，同时保持特征的不变性。这样，即使对象在图像中的位置发生变化，CNN也能够识别出相同的特征。
Transformer的全局注意力：

自注意力机制：Transformer架构的核心是自注意力（self-attention）机制，它允许模型在处理每个元素时考虑输入序列中的所有位置。这意味着模型可以捕捉到全局依赖关系，而不是仅仅局限于局部区域。
位置编码：由于Transformer没有卷积层，它需要一种方式来理解序列中元素的位置信息。位置编码（positional encoding）被添加到输入序列中，以确保模型能够区分序列中不同位置的特征。
并行处理：与CNN相比，Transformer能够更有效地并行处理序列数据。在CNN中，信息需要通过层级结构逐层传递，而在Transformer中，所有位置的信息都可以同时被处理。
总结来说，CNN通过局部感受野和层次化特征提取来关注输入数据的局部特征，而Transformer通过自注意力机制来捕捉序列中的全局依赖关系，从而实现全局注意力。这两种架构各有优势，适用于不同类型的任务和数据类型。


**如果增加CNN的层数，特征图的感受野会增加，CNN会变成全局注意力吗？**
增加CNN的层数确实会使得特征图的感受野增大，因为每增加一层，特征图中的每个元素就会包含更大范围的输入信息。这是因为在每一层，卷积核会捕捉到来自前一层更广泛区域的特征，而且通过堆叠多层，信息可以从输入数据的更深层次被整合起来。

然而，即使感受野增大，CNN的注意力机制本质上仍然是局部的，原因如下：

局部感受野的累积：虽然每一层的特征图感受野在增大，但CNN的每一层仍然是基于局部区域进行操作的。每一层的特征图是对前一层局部特征的进一步抽象和组合，而不是全局考虑整个输入。

层级结构：CNN通过层级结构逐步提取特征，每一层通常只与其前一层的特征图有关，而不是直接与输入数据的全局特征有关。这种层级结构使得CNN在每一层都保持了局部性。

缺乏全局依赖捕捉：CNN的设计并没有直接在每一层捕捉整个输入空间的全局依赖关系。虽然深层特征图中包含了全局信息，但这些信息是通过局部操作的连续组合得到的，而不是通过显式的全局注意力机制。

全局池化层：虽然CNN可以在网络的末端使用全局池化层（如全局平均池化）来聚合空间特征，但这并不等同于全局注意力。全局池化是对特征图进行降维处理，以便进行分类或其他全局任务，而不是在特征提取过程中考虑全局依赖。

因此，尽管增加CNN的层数可以扩大感受野并捕捉更复杂的局部特征，但CNN并不会因此变成具有全局注意力的模型。全局注意力通常是指模型在处理每个元素时都考虑整个输入序列或图像的所有部分，这一点在Transformer架构中得到了体现。如果需要CNN具有类似Transformer的全局注意力能力，可以考虑将CNN与自注意力机制结合，或者使用其他能够捕捉全局依赖的网络结构。


**foundation models的分类**
Foundation models（基础模型）是一类深度学习模型，它们具有强大的能力来理解和处理各种数据类型和任务。这些模型通常需要大量的数据和计算资源来训练，并且它们可以被应用于多种下游任务。以下是一些主要的基础模型分类：

预训练语言模型（Pre-trained Language Models, PLMs）:

BERT (Bidirectional Encoder Representations from Transformers): 通过在大量文本上进行预训练，BERT能够捕捉到丰富的语言特征，并在各种NLP任务中取得了显著的成果。
GPT (Generative Pre-trained Transformer): GPT系列模型通过预测下一个词的方式进行预训练，生成连贯的文本，并能够执行多种语言任务。
T5 (Text-to-Text Transfer Transformer): T5将所有任务统一为文本到文本的转换问题，通过预训练来学习不同任务之间的共享表示。
预训练视觉模型（Pre-trained Vision Models）:

ImageNet预训练的CNNs: 通过在大规模图像数据集（如ImageNet）上进行预训练，这些模型能够学习到丰富的视觉特征，用于图像分类、目标检测等任务。
ViT (Vision Transformer): ViT将图像分割成多个patch，并将它们作为序列输入到Transformer架构中，从而利用Transformer的能力来处理图像数据。
多模态预训练模型（Multi-modal Pre-trained Models）:

CLIP (Contrastive Language–Image Pre-training): CLIP通过对比学习的方式，同时学习图像和文本之间的关联，能够用于图像分类、文本描述生成等多模态任务。
DALL-E: DALL-E是一个基于GPT-3的生成模型，能够根据文本描述生成相应的图像。
大规模自监督学习模型:

SimCLR: 用于无监督学习的模型，通过对比不同增强版本的同一图像来学习特征表示。
MAE (Masked Autoencoder): 通过遮蔽输入数据的一部分并训练模型来重建这些部分，MAE在自监督学习中取得了良好的效果。
强化学习模型:

DQN (Deep Q-Network): 通过学习策略来最大化累积奖励，DQN在解决各种序列决策问题中表现出色。
AlphaGo: 通过结合深度学习和蒙特卡洛树搜索，AlphaGo在围棋领域取得了突破性的成就。
这些基础模型通常在大规模数据集上进行预训练，以学习通用的特征和表示，然后可以通过微调（fine-tuning）或特定任务的适应来解决各种下游任务。随着研究的深入和技术的发展，基础模型的种类和应用范围还在不断扩大。



VTK学习资料
https://examples.vtk.org/site/



简介

定义：Sora 是由 OpenAI 于 2024 年 2 月发布的文本到视频生成 AI 模型。
特点：能够根据文本指令生成现实或想象的场景视频，展示模拟物理世界的潜力。
背景

历史：从传统图像生成技术到深度学习革命。
先进技术概念：探讨大型语言模型（LLMs）的扩展定律和突现能力。
技术

Sora 概述：基于扩散变换器的架构。
数据预处理：处理不同时长、分辨率和宽高比的视频和图像。
建模：使用扩散变换器模型进行视频生成。
语言指令遵循：通过训练视频字幕生成器来提高遵循文本指令的能力。
提示工程：设计和优化输入以指导模型生成特定输出。
可信度：讨论安全性、其他利用、对齐和信任度的挑战。
应用

电影制作：自动化内容生成和电影制作。
教育：定制和动态教育材料。
游戏：创建动态、高保真视频内容和响应性游戏环境。
医疗保健：用于早期疾病检测和治疗规划。
机器人技术：机器人的视觉和决策能力。
讨论

限制：物理真实性、空间和时间复杂性、人机交互和使用限制。
机会：学术界、行业和社会的潜在影响。
结论

对开发者和研究者的综述，基于技术报告和现有文献的逆向工程。
期待开源社区的合作，共同开发 Sora 的开源版本。



**20240402**
吴恩达Agentic Reasioning
大模型中的集成学习方法


**20240403**
前面我们基于Openai的API开发聊天机器人时，为了记住上下文，我们通过LangChain给LLM加上了记忆功能，后来，我们又通过RAG让LLM回答地更准确。除了记忆力和检索能力，还可以为LLM添加更多地功能，这样就从单纯地语言模型逐步演变成一个智能体，也就是本文要讲的AI Agent。


声音合成，音频+图像生成视频

Vison Pro开始支持空间角色，是否想让你最思念的人
https://twitter.com/BenGeskin/status/1775166712519807109


**20240407**
基于弹性可形变模型的胼胝体分割，开源C++代码
论文：
https://www.nitrc.org/docman/view.php/466/1131/SPIE_2012_Vachet_CCSeg.pdf
软件包和模型数据
https://www.nitrc.org/frs/?group_id=466#
代码
https://github.com/NIRALUser/CCSeg?tab=readme-ov-file



**20240408**
一般 
LIU XIAO YING
LIU JIA FA
SUWEI
WANG ZHU MING   39

box center: [-7.86767, 39.2794]
box degrees: 4.39884
line center: [-7.5, 34.5]
line degrees: -0.971051

box angle: -3.87863
line center: [-11.5, 54]
line degrees: -2.57344
final degrees: -3.22603


WANG YU REN
MENG GUANG WEI
WANG YUAN TING
WANG WEN QIN
CUI MEI JUAN
CHEN CHUN JIE
LI CHUN LING
XU RAN
ZHAO GUOBIN
XIU CHANG LING
YUWEI RUN
ZHAOXUE XIA
SUN SHU XIA
ZHU ZHEN
ZHANG HUI
LI HONG YI   22

image size:256 x 256
box center: [-16.709, -6.79919]
box degrees: -5.19456
line center: [-17, -11]
line degrees: -1.90921


image size:384 x 384
box center: -24.7409,-10.3582
box angle: -6.51999
line center: [-25.5, -17]
line angle: -5.02422
final degrees: -5.77211
final angle: -25.1205,-13.6791

GAO LI JUAN
杨亮亮
WANG QIU HUA
YUAN PEI XIANG
REN PEI MING
WU XING YAO
WANG HU SHENG
DUAN LIN XIU
LIU XI E
DU SHI ZONG
SU CHENG ZHONG
刘遂莲
RW.SHEN
Chu Ya Rong   4

box center: [-26.9319, 56.2694]
box degrees: -1.19352
line center: [-27, 52.5]
line degrees: -2.86249

box center: -19.9649,42.2629
box angle: -1.59119
line center: [-20, 41]
line angle: -7.7654
final degrees: -4.67829
final angle: -19.9825,41.6315
JAING XI HUA


好
HAN RONG GUO
WANG JUN


**20240410**
未来对于大模型的发张，目前有两个方向：
AI Agent
训练通用大模型，卷参数一般公司玩不起，所以，能不能通过协作方式实现1+1>2的效果呢？AI Agent是一种方法，另外一种是，训练多个专业领域大模型，每个模型可能只精通某个领域，然后通过类似杂交的方式实现基因的遗传和变异，来实现多个专业模型的融合，最终实现AI的进化。


一部分孩子补课，提高的是分数，所有孩子补课，提高的是分数线，花80%的时间去提高最后那20&的分数，对于大部分人没有意义，不要定义成功，不要成功学，生命需要多样性。



GPT Pilot

大模型发展到今天，趋势逐渐明朗，有资源的走技术路线去卷通用大模型，没资源的走专业路线去做领域大模型。

​

训练通用大模型，卷参数一般公司玩不起，所以，能不能通过协作方式实现1+1>2的效果呢？AI Agent是一种方法，另外一种是，训练多个专业领域大模型，每个模型可能只精通某个领域，然后通过类似杂交的方式实现基因的遗传和变异，来实现多个专业模型的融合，最终实现AI的进化。



**20240411**
AI真的在重塑各行各业，娱乐，办公，创作，到处都有大模型的身影。各行各业都在想把自己的产品用AI重塑一遍。

在搜索领域，传统搜索引擎返回的是网页列表，质量好的会靠前，但这里面也有很多猫腻，所以，内容的好坏需要用户点进去判断。

基于大模型的搜索引擎先是借助传统搜索引擎收集与用户搜索相关的内容，然后基于搜索内容进一步分析，这有点类似增强检索生成RAG，传统搜索引擎搜集的内容就是上下文，让大模型基于上下文回答就会减小胡说的可能。

如何基于搜索内容进一步分析，能分析到什么程度，就是不同AI搜索引擎的区别了。

前面我们介绍过Globe Explorer，它能利用大模型将用户搜索内容划分为不同主题，然后发送给搜索引擎，最后以一种分层的可视化方法展示搜索内容分解后的主题。

大家可能会发现我会介绍同种类型的不同产品，之所以要推荐重复的东西，一是随着技术的发展，产品迭代更新很快，一年前还是以大模型为主，现在又冒出智能体的概念。二是不同产品之间也能起到相互补充的作用。

今天给大家介绍另一个AI搜索引擎：秘塔AI搜索，在3月份AI产品榜单上，秘塔AI搜索排名第三，紧随之前介绍了kimi chat。

当你输入一条搜索内容，例如“帮我介绍一下当前流行的深度学习框架”。
它一开始也是搜索互联网山的网页，然后对网页内容进行概括总结。

但令人惊艳的还在后头。

它能列出深度学习框架相关的历史事件和相关知识。


更不可思议的是它能生成一张思维导图。
人类对于文字和视觉信息的理解能力是不一样的，尤其是这种具有逻辑结构的，告诉概括的视觉信，有助于帮助用户形成知识脉络，留下深刻的第一印象。

到这里你觉得结束了吗？还没有，它还能帮你生成一张PPT。



谷歌停止造车计划后，转投AI陪伴机器人领域，这些大公司的行为是行业风向标，AI陪伴机器人的出现很大程度上能解决老人，婴幼儿的陪伴问题。

但机器人一般适用固定场所，很难方便随身携带，而眼下几乎人手一部手机，借助端侧大模型的能力，让手机像智能伴侣一样，来感知用户意图并作出合理响应。

小米


荣耀MagicOS


仅通过应用层面难以实现，要通过OS和硬件层面相互协作。


人机交互的变革



大模型如何在资源受限的手机终端运行？
既然不是调用API，而是端侧部署大模型，就必须要考虑一个问题？





**20240412**
前面我们专门写了一个专栏介绍大语言模型的技术原理和应用，大语言模型，顾名思义就是对语言建模，输入的是文本提示，模型输出的也是文本，属于单模态模型。从今天开始我们将跨入多模态领域。但作为第一讲，不想涉及太多技术细节，先从整体上对多模态有个了解。

所以这一讲就一个目的：多模态是什么？

多模态简单来讲就是模型要处理多种数据模态，文本，图像，视频，音频。

能用来文生图，例如，Stable Diffusion和DALL-E，输入一段描述文字，模型输出一张图片。

也能用来图生文，例如，VQA，视觉问答，输入一张图片，再输入一个问题，模型输出答案。

当然，还有很多应用，文生视频，视觉理解等等。

不同的模型有不同的结构和训练方法，但今天的目的不过多涉及技术细节，所以，我们就以图生文作为认识多模态的切入点来进行讲解。

图生文多模态模型的输入是图像，如果是视觉问答，还有文本输入，输出则是文本。

**表征提取**

所以，模型需要能处理图像，因为图像作为输入，所以模型要能提取表征，也就是提取特征。
能够提取图像特征的你首先想到的肯定是CNN，图像每经过一层卷积层都会输出特征图，随着层数的增加，特征图语义信息月充足，最后可以用来做分类或者坐标框回归。卷积神经网络也会被称为表示学习。

随着Transformer在NLP中取得成功，目前几乎所有的大语言模型背后都是基于Transformer架构，所以就有了ViT。

在NLP中，当Transformer处理一个单词时，它会计算这个单词与其它所有词的注意力分数，而CNN只会考虑卷积核范围内的像素，即使具有多层卷积层的网络，特征图的感受野会随着层数增加越来越大，但CNN仍是局部注意力，而ViT则是全局注意力，全局注意力能更好的提取图像特征。

除了模型架构，成为大模型的另一个因素是能在大规模语料上预训练，ViT首先将图像划分位patches，Transformer将文本划分为tokens，剩下的两者基本一致，基于ViT的MAE和BeiT等模型，也可以像NLP中的BERT和GPT一样以掩码子监督或者自回归的方式在大规模语料上进行预训练，预训练是成为大模型的基本前提。

总结一下，CNN或者ViT是常用的提取图像表征的模型架构。


对于VQA任务，还会输入一个问题，多以还需要对文本进行表征提取，肯定是Tranformer了，例如，BerT。

模型的输出是文本，这个一般的自回归大模型，例如，GPT都能完成。

**多模态表征对齐**
在推理阶段，总体上就是图像编码器提取图像表征，然后将表征与用户Query一起输入到类似GPT的大语言模型，然后以自回归的方式生成文本。

这里图像的表征，就像是RAG中的上下文，也就是告诉大语言模型，要基于图像表征的内容，针对我的问题给出答案。

编码器提取的表征，和大语言模型文本输入根本就不在一个嵌入空间啊，所谓驴唇不对马嘴。怎么办？

这就需要将图像表征和文本表征进行对齐。也就是需要在训练阶段调整图像编码器的参数。

这就需要一种方法：对比学习。


目标跟踪框架
https://supervision.roboflow.com/


**20240416**
将模型的前向推理过程构建一个信息流图，图的节点表示token向量表征，图的边表示操作。

Transparency Tool是facebook开源的大语言模型可视化工具，用于分析基于Transformer架构的语言模型。


​Transformer是由多个注意力块堆叠而成，每个注意力块视为一层，每个层包含一个多头注意力层和一个前馈网络。token向量经过注意力层和前馈网络后，会修改向量的值。


在huggingface上可以体验一下这个在线demo，在左侧选择模型，默认gpt2，在上方选择输入提示文本，默认“When Mary and John went to the store, John gave a drink to
”，

huggingface只提供了gpt2，distilgpt2，facebook/opt-125m三个模型，如何加载自己的模型呢？

Transparency Tool是基于TransformerLens开发的，TransformerLens是一个专注于生成语言模型（如GPT-2风格的模型）的可解释性的库。其核心目标是利用训练好的模型，通过分析模型的内部工作机制，来提供对模型行为的深入理解。




the initials GPT
stand for generative pre trained transformer
GPT是generative pre-trained transformer首字母缩写
so that first word is straightforward enough

these are bots that generate new text
第一个词很明显地指出了它是用来生成新文本的
pretrained prefers to
how the model went through
a process of learning from a massive amount of data
pre-trained表明模型需要经历在大量数据上预训练过程
and the prefix insinuates that
there's more room to fine
tune it on specific tasks with additional training
这个词也暗示了模型后续在某个特定任务上继续进一步微调的可能
but the last word that's the real key piece
最后一个词才是最重要的部分
a transformer is a specific kind of neural network
transformer是一种特定类型的神经网络，一种机器学习模型
a machine learning model
and it's the core invention
underlying the current boom in AI
它是现今AI高速发展的核心创新
what I want to do with this video
and the following chapters
is go through a visually driven explanation
for what actually happens inside a transformer
本视频以及后续视频我们将通过可视化的方式探索Transformer内部原理
we're going to follow the data that flows through
it and go step by step
我们将逐步跟随数据流经模型的整个过程
there are many different kinds of models that you can
build using transformers
你可以基于transformers构建多种类型的模型
some models take in audio and produce a transcript
有的模型接受音频输入并生成文本
“this sentence comes from a model going the other way
around producing synthetic speech just from text”
这句话来自一个反向工作的模型，它将根据文本生成语音
all those tools that took the world by storm in 2022
like Dolly and midjourney
that take in a text description
and produce an image are based on transformers
在2022年风靡全球的绘画工具DALLE和Midjourney，能够根据文本生成图像，也是基于transformers的
even if I can't quite get it to
understand what a pie creature is supposed to be
I'm still blown away
that this kind of thing is even remotely possible
即使有时我对它生成的内容不是很理解，但我对此仍感到惊讶
and the original transformer introduced in 2017
by Google
最初的transformer是谷歌在2017年提出的。
was invented for the specific use case of translating
text from one language into another
起初主要用于将一种语言翻译成另一种语言
but the variant that you and I will focus on
which is the type that underlies tools like ChatGPT
will be a model that's
trained to take in a piece of text
maybe even with some surrounding images or sound
accompanying it
and produce a prediction
for what comes next in the passage
但是像其它一些变体，例如ChatGPT，是一个接受一段文本（可能也会伴随图像和声音）的模型，然后预测文章接下来的内容。
that prediction
takes the form of a probability distribution
这种预测呈现概率分布形式
over many different chunks of text that might follow
first glance
用来预测接下来可能出现的单词
you might think that predicting the next word
feels like a very
different goal from generating new text
乍一看，预测下一个词与生成新文本是不同的
but once you have a prediction model like this
a simple thing you could try to make it
generate a longer piece of text
但一旦你有了这样的模型，你可以试着让它生成一段长文本
is to give it an initial snippet to work with
have it take a random
sample from the distribution it just generated
方法就是给它一个初始文本片段，然后随机从刚才生成的概率分布中选取一个样本
append that sample to the text
将这个样本追加到文本中
and then run the whole process again
然后重复这个过程去继续生成新的预测
to make a new prediction
based on all the new text including what it just added
新的预测要基于所有的文本，包括刚刚添加的
I don't know about you
but it really doesn't feel like this should actually
我不知道你怎么看，但我觉得这种方法的效果可能并不理想
work in this animation for example
举个例子
I'm running GPT2 on my laptop
我在我的电脑中运行GPT2
and having it repeatedly predict and
sample the next
chunk of text to
generate a story based on the seed text
and the story
然后让它不断地去生成下一个词，尝试基于一段文本生成一段故事
just doesn't actually really make that much sense
结果，这个故事基本没什么逻辑可言
but if I swap it out for API calls to GPT3 instead
which is the same basic model just much bigger
但是，如果我换成GPT3，这是同样的模型，只是规模更大，
suddenly almost magically we do get a sensible story
突然间就像变魔法一样，不仅故事逻辑合理
one that even seems to infer that a pie
creature would live in a land of math and computation
甚至暗示出一个Π生物可能居住在一个充满数学和计算的世界里
this process here of repeated prediction and sampling
is essentially what's happening when you interact with
这个过程就是通过重复的预测和选取来生成文本
ChatGPT和其它大模型也是一样，逐字地去生成文本
Chatchipt or any of these other large language models
and you see them producing one word at a time
但是我特别希望能看到它在选择每个词时的概率分布
in fact one feature that I would very much enjoy
is the ability to see the underlying distribution
for each new word that it chooses
let's kick things off with a very high level
让我们先从宏观上看数据是如何在transformer中流转的
preview of how data flows through a transformer
we will spend
much more time
motivating and interpreting and
expanding on the details of each step
然后，我们将详细探讨解释每一个步骤，并对其进行扩展
but in broad strokes
when one of these chat bots generates a given word
here's what's going on under the hood
大体上讲，当生成某个词时，下面就是它底层运行机制
first
the input is broken up into a bunch of little pieces
these pieces are called tokens
首先，输入文本会被拆分成小片段，这些小片段被称为token
and in the case of text
these tend to be words or little pieces of words or
对于文本来说，这些token通常是单词或者单词的一部分或者是其它常见的字符组合
other common character combinations
if images or sound are involved
then tokens could be
little patches of that image
or little chunks of that sound
如果包含图片或者声音，token可能是图像被划分为一个个小区域，或者是声音片段
each one of these tokens is then associated with a
vector meaning some list of numbers
然后，每个token都对应到一个向量上，也就是一串数字
which is meant to somehow
encode the meaning of that piece
这串数字的意义是从某种方式来表达这个片段的含义
if you think of these vectors as given
coordinates in some very high dimensional space
如果你把这些向量看作高维空间中的坐标
words with similar meanings tend
to land on vectors that are close to each other
那么，含义相似的词汇倾向彼此接近的向量上
in that space
this sequence of vectors then passes through an
operation that's known as an attention block
这些向量序列接下来会经过一个称为“注意力块”的处理过程
and this allows the
vectors to talk to
each other and pass information back
使得向量之间能够相互交流
and forth to update their values
并根据彼此信息更新自身的值
for example the meaning of the word
model in the phrase a machine learning model
is different
例如，model这个词，在machine learning model中的意思和a fashion model中意思是不同的
from its meaning in the phrase a fashion model
the attention
block is what's responsible for figuring out
注意力块的作用就是确定上下文中的哪些词对更新其它词的意义有关
which words in the context are relevant to updating
the meanings of which
以及该如何准确的更新其含义
other words and
how exactly those meaning should be updated
and again whenever I use the word meaning
每当我说道“含义”这个词时，完全是通过向量中的数字来表达的。
this is somehow entirely encoded in the
entries of those vectors
after that these vectors pass through
a different kind of operation
之后，这些向量会经过另一种处理
and depending on the source that you're reading
this will be referred to as a
这个过程可能被称作多层感知机或者前馈神经网络
multilayer perceptron or maybe a feed forward layer
and here the vectors don't talk to each other
这个阶段向量之间不再产生交流
they all go through the same operation in parallel
而是并行的经历同一处理
and while this block is a little bit
harder to interpret
虽然这个步骤比较难理解
later on
we'll talk about how the step is a little bit like
asking a long list of questions about each vector
and then
但我们会在后面讨论，这个步骤有点像对每个向量提出一系列问题
updating them based on the answers to those questions
然后根据这些问题的答案更新向量
all of the operations in both of these blocks
look like a giant pile of matrix multiplications
这两个处理阶段的操作本质上都是大量的矩阵乘法
and our primary job is going to be to understand
我们主要学习的是：如何解读这些背后的矩阵
how to read the underlying matrices
I'm glossing over
some details about some normalization
在讲解中，我省略了一些中间步骤的归一化细节
steps that happen in between
but this is after all a high level preview
这毕竟只是宏观概览
after that the process essentially repeats
接下来的过程就是重复的了
you go back and forth between
tension blocks and multilayer perceptron blocks
就是在注意力模块和多层感知机模块之间不断切换
until at the very end
the hope is that all of the
直到最后，我们期望通过某种方式，文章的核心意义已经被完全融入到序列的最后一个向量中
essential meaning of the passage
has somehow been baked into the very
然后对最后一个向量进行特定操作
last vector in the sequence
we then perform a certain operation on that last vector
产生一个覆盖所有可能token的概率分布
that produces a probability distribution
over all possible tokens
while possible little chunks of text
that might come next
这些token代表接下来可能出现的小段文本
and like I said
就像我说的
once you have a tool that predicts what comes next
given a snippet of text
一旦你有了这样的工具，根据一段文本预测下一个token
you can feed it a little bit of seed text
你可以给它输入一段初始文本，让它不断地预测下一个token
and have it repeatedly play this game
of predicting what comes next
sampling from the distribution
然后从概率分布中取样，追加到现有文本中
appending it and then repeating over and over
然后不断重复这个过程
some of you in the know may remember how
了解这一点的人可能还记得，早在ChatGPT出现以前，GPT3是这样工作的
long before Chat GPT came into the
this is what early demos of gpt3 looked like
you would have it auto complete stories and
根据一段起始文本自动补全故事和文章
essays based on an initial snippet
to make a tool like this into a chatbot
讲这样的工具转化为机器人的一个简单方法是，准备一段文本，设定一个场景，也就是所谓的系统提示，
the easiest starting point
is to have a little bit of text that
establishes the setting

of a user interacting with a helpful AI assistant
what you would call the system prompt
and then you would use the user's initial question
or prompt as the first bit
然后将用户的初始问题或者提示词作为对话的开头。
of dialog and then you have it start predicting
what such a helpful AI assistant
would say in response
接着让AI开始预测，这个有用的AI助手会如何回应
there is more to say
about an added step of
training that's required to make this work well
为了使这个过程运行得更好，还需要额外的训练步骤
but at a high level this is the general idea
不过总的来说，这就是基本思路
in this chapter
you and I are going to
expand on the details of what happens
at the very beginning of the network
at the very end of the network
在这一章，我们将详细探讨网络开始和网络结束发生的事情
and I also want to spend a lot of time reviewing some
important bits of background knowledge
things that would have been second
同时，我也会花大量的时间回顾一些重要的背景知识
nature to any machine learning engineer
by the time Transformers came around
这些知识对于熟悉Transformer的工程师来说都是基础知识
if you're comfortable with that background knowledge
and a little impatient
如果你对背景知识比较熟悉，而且迫不及待了解更多
you could probably feel free to
skip to the next chapter
你可以直接跳到下一章节
which is going to focus on the attention blocks
generally considered the heart of the transformer
这一章将会关注Transformer的核心部分，即注意力模块
after that I want to talk more about these multi layer
perceptron blocks
在这之后，我还会详细讨论多层感知机模块，训练过程以及一些被忽略的细节
how training works and a number of other details that
will have been skipped up to that point

for broader context
these videos are additioned to a miniseries
about deep learning
同时，这些视频也是深度学习系列视频的补充
and it's okay if you haven't watched the previous ones
I think you can do it out of order
你不一定非得按照顺序来看
but before diving into Transformers specifically
I do think it's worth making
但是在深入研究Transformer之前，我认为有必要确保对深度学习的基本概念和模型有一些了解
sure that we're on the same page about the basic
premise and structure of deep learning
at the risk of stating the obvious
this is one approach to machine learning
这里需要明确的是，机器学习是一种方法论
which describes any model where you are using
data to somehow determine how a model behaves
它涉及到使用数据来指导模型行为的模式
what I mean by that is
具体来说
let's say you want a function that
takes in an image
你可能需要一个函数，输入一张图片
and it produces a label describing it
输出对应的标签描述
or our example of predicting the next word
given a passage of text
或者预测文本片段的下一个词
or any other task that seems to require some
element of intuition and pattern recognition
或者其它需要直觉和模式识别的任务
we almost take this for granted these days
虽然我们现在已经习以为常
but the idea with machine learning is that rather than
trying to
但是机器学习的核心思想在于，我们不再尝试去编写固定的程序来完成这些任务
explicitly define a procedure for how to do that task
in code which is what people would have done in the
earliest days of AI
这是AI在最早阶段人们会做的事情
instead you set up a very flexible structure with
tunable parameters
而是构建一个具有可调节参数的灵活结构
like a bunch of knobs and dials
就像一系列旋钮和调节器
and then somehow
you use
many examples of what the output should look like
for a given input
然后通过对大量实例输入和期望输出的学习
to tweak and tune the values of those parameters
来调整和微调参数的值
to mimic this behavior
来模拟这种行为
for example maybe the
simplest form of machine learning is linear regression
例如，最直观的机器学习模型：线性回归
where your inputs and your outputs are each
single numbers
输入输出都是单个数字
something like the
square footage of a house and its price
如房子的面积和价格
and what you want is to find
a line of best fit through this data
你要做的就是找到一条能拟合这些数据的线
you know to predict future house prices
以次来预测将来的房价
that line is described by two continuous parameters
这条线由两个参数决定，即斜率和截距
say the slope and the y intercept
and the goal of linear regression
线性回归的目标是
is to
determine those parameters to closely match the data
needless to say
确定能够完美匹配这些数据的参数
deep learning models get much more complicated
不用说，深度学习模型更加复杂
gpt3 for example has not two but 175 billion parameters
比如，GPT3有1750亿个参数，而不仅仅是两个
but here's the thing
it's not a given that you can create
但值得注意的是
some giant model with a huge number of parameters
without it either
并不是简单地构建一个参数众多地庞大模型就能有效工作
grossly overfitting the training data
这样做可能会导致模型严重过拟合训练数据
or being completely intractable to train
或者训练起来极其困难
deep learning describes a class of models
深度学习涵盖了一系列具有出色扩展能力地模型类别
that in the last couple decades
have proven to scale remarkably well
what unifies them is that they all use the same
它们之所以能够成功，关键在于都采用了相同地训练方法
training algorithm
it's called back propagation
即反向传播
we talked about it in previous chapters
我们在前面章节已经介绍过了
and the context that I want you to have as we go in
is that in order for this
你需要理解的是
training algorithm to work well at scale
要让这种训练方法能在大规模应用中顺利进行
these models have to follow a certain specific format
模型必须遵循一种特定的结构
and if you know this format going in
it helps to explain many of the
如果你对这种结构有所了解
choices for how a transformer processes language
which
就能更好的理解Tranformer处理语言的方式及其背后的逻辑
otherwise run the risk of feeling kind of arbitrary
first whatever kind of model you're making
首先，不管你是构建哪种模型
the input has to be formatted as an array of real
输入必须是一个实数数组
numbers this could simply mean a list of numbers
it could be a two dimensional array
这可能知识一个数字列表，也可能是一个二维数组
or very often you deal with higher dimensional arrays
where the general term used is tensor
或者更常见的高维数组，我们通常称之为张量
you often think of that
input data as being progressively transformed
这些数据通常会被逐步流经多个不同的层
into many distinct layers
where again each layer is always structured

as some kind of array of real numbers
每一层的输出也可被组织成实数数组
until you get to a final
layer which you consider the output
直到最后一层，你可以视其为输出层
for example
the final layer in our text processing model
例如，我们的文本处理模型输出层最终输出一个数字列表
is a list of numbers
representing the probability distribution
for all possible next tokens
这些数字代表了所有可能的下一个词的概率分布
in deep learning
these model
parameters are almost always referred to as weights
在深度学习领域，这些模型的参数通常被称为权重
and this is because a key feature of these models
is that the only way
这样称呼的原因是，这些模型的一个核心特点是
these
parameters interact with the data being processed
这些参数与正在处理的数据之间的唯一交互方式
is through weighted sums
就是通过权重和
you also sprinkle some nonlinear functions throughout
but they won't depend on parameters
虽然模型中也会穿插一些非线性函数，但它们并不依赖这些参数
typically though

instead of seeing the weighted sums all
通常来说，我们不会直接看到这些权重零散的工作
naked and written out explicitly like this

you'll instead find them packaged together
而是被组织成矩阵向量乘积的形式
as various components in a matrix vector product
it amounts to saying the same thing
如果你了解矩阵向量乘法计算规则，你就会发现这其实是表达同一种概念
if you think back to how matrix vector multiplication
works each component in the output
输出的每一个元素就像是一个权重和
looks like a weighted sum
it's just often conceptually
cleaner for you and me to think about
更直观的方式是，将这些可调参数矩阵想象成对数据进行转换的工具
matrices that are filled with tunable parameters
that transform
vectors that are drawn from the data being processed
例如，GPT3中的1750亿个权重
for example those 175 billion weights in GPT3
are organized into just under 28 000 distinct matrices
those matrices in turn fall into
就被组织在大约28000个不同的矩阵中
eight different categories
这些矩阵又被划分为八个不同类型
and what you and I are gonna do
你和我将要做的就是逐一理解这些矩阵
is step through each one of those categories to
understand what that type does
了解没有类别的功能
as we go through
I think it's kind of fun
接下来的过程将会非常有趣
to reference the specific numbers from GPT3
我们将参考GPT3的具体数据
to count up exactly where those 175 billion come from
来统计这1750亿个参数是如何分配的
even if nowadays there are bigger and better models
this one has a certain charm as the first
即使现在有更大更好的模型，GPT3仍具有独特的魅力
large
language model to really capture the world's attention
作为第一个引发全球关注的大语言模型
outside of ML communities
影响力并不局限机器学习社区
also practically speaking
实际上
companies tend to keep much tighter lips around
对于更先进的模型，公司往往会对具体的数据保密
specific numbers for more modern networks
I just want to set the scene going in
这里我想说明的是
that as you peek under the hood to see what happens
inside a tool like ChatGPT
当你探索像ChatGPT这样工具的内部机制时，你会发现几乎所有计算过程都体现为矩阵和向量乘积
almost all of the actual computation
looks like matrix vector multiplication
there's a little bit of a risk getting lost in the sea
of billions of numbers
在这海量的数字中，很容易迷失方向
but
you should draw a very sharp distinction in your mind
但你需要清楚区分两个概念：模型的权重（用红色和蓝色表示）和正在处理的数据（用灰色表示）
between the weights of the model
which I'll always color in blue or red
and the data being processed
which I'll always color in gray

the weights are the actual brains
权重是模型的大脑
they are the things Learned during training
它们在训练过程中会被修改
and they determine how it behaves
它们决定了模型的行为模式
the data being processed
simply
encodes whatever specific input is fed into the model
而正在处理的数据仅仅编码了输入到模型中的具体输入
for a given run
like an example snippet of text
比如，一段文本示例
with all of that as foundation
理解了上述基础之后
let's dig into the first step of this text processing
让我们深度探讨文本处理的第一步
example which is to break up the input into little
将输入分割成小片段
chunks and turn those
chunks into vectors
将这些小片段转换成向量
I mentioned how those chunks are called tokens
我之前说过，这些小片段被称为token
which might be pieces of words or punctuation
它们可能是单词的一部分或者标点符号
but every now and then in this chapter
但在本章，特别是下一章
and especially in the next one
I'd like to just pretend that it's broken more cleanly
into words because we humans think in words
我将简化理解，假设它们按单词进行划分，因为人类就是按照单词进行思考
this will just make it much easier to reference little
examples and clarify
通过一个小例子来解释每一步就能使这个过程更容易理解了
each step
the model has a predefined

some list of all possible words say 50 000 of them
模型具有一个预设的词汇库，包含所有可能的单词，比如说有50000个
and the first matrix that we'll encounter
known as the embedding matrix
我们将首先遇到一个矩阵，叫做嵌入矩阵
has a single column for each one of these words
these columns are what determines what vector
它为每个单词都分配了一个独立的列，这些列决定了每个token会被转换成什么向量
each word turns into
in that first step
we label it
跟其它矩阵命名规则一样，我们称这个矩阵为Ve
we
and like all the matrices we see
its values begin random
but they're going to be Learned based on data
它的初始值是随机的，然后在训练过程中基于训练数据进行调整
turning words into vectors
was common practice in machine learning long before
transformers
在transformers之前，就已经有了这种单词转向量的方法
but it's a little weird if you've never seen it before
虽然对于初次接触的人来说可能觉得很奇怪
and it sets the foundation for everything that follows
so let's take a moment to get familiar with it
但它为接下来的一切建立了基础，因此，我们要花些时间来熟悉它
we often call this embedding a word
which invites you to think of these vectors
我们通常称这种转换为词嵌入，这种表述可以让你从几何的角度其理解这些向量
very geometrically
as points in some high dimensional space
visualizing a list of three
把它们想象成高维空间中的点
numbers as coordinates for points in 3d space
将三个数字看作三维空间中的坐标点很简单
would be no problem
but word embeddings tend to be much
much higher dimensional
但词嵌入的维度远超过这个范畴
in GPT3 they have 12
288 dimensions
GPT3的词嵌入维度为12288
and as you'll see
it matters to work in a space
如你所见
that has a lot of distinct directions
in the same way that you could take a 2 dimensional
选择一个高维空间是很重要的，就像你可以在3维空间选择一个2维切片
slice through a 3d space
and project all the points onto that slice
然后将所有点投影到这个切片上
for the sake of animating word embeddings
that a simple model is giving me
为了动态展示词嵌入向量
I'm gonna do an analogous thing
by choosing a 3 dimensional
slice through this very high dimensional space
我采用了相同的方法，从高维空间选择一个三维切片
and projecting the word vectors down onto that
and displaying the results
然后将高维向量投影到这个三维空间进行展示
the big idea here
is that as a model tweaks and
tunes its weights to determine how
这里的关键思想是，模型在训练过程中微调权重
exactly words get embedded as vectors during training
以确定词如何被嵌入为向量
it tends to settle on a set of embeddings where
directions in the space
它会倾向于找到一组嵌入，使得这个空间中的方向含有特定语义含义
have a kind of semantic meaning
for the simple word to vector model I'm running here
if I run
对于目前我运行的这个简单模型来说
a search for all the words whose embeddings are
closest to that of tower
通过搜索，找到所有与tower相近似的词向量
you'll notice how they all seem to give very similar
towerish vibes你会发现这些词都有着近似的tower元素
and if you want to pull up some
Python and play along at home

this is the specific model that I'm using
to make the animations
这些就是我用python来制作动画的模型
it's not a transformer

but it's enough to illustrate the idea that
虽然不是transformer，但足以说明一个观点
directions in the space can carry semantic meaning
空间中的方向能够表达语义信息
a very classic example of this
一个经典的例子是
is how if you take the difference
between the vectors for woman and man
计算woman和man向量之间的差值
something you would visualize as a little
vector in the space
它们的差也可以被可视化为一个向量
connecting the tip of one to the tip of the other
向量起点和终点分别是这两个词的顶点
it's very similar to the difference between

king and queen
这个差异和queen和king的差异很接近
so let's say you didn't know the word for a female
monarch you could find it by taking king
所以，假设你不知道表示woman queen的向量，
adding this woman man direction
你可以通过把king的向量加上这个差异向量，并搜索最接近这个点的向量找到它
and searching for the embeddings closest to that point
at least kind of
despite this being a classic example
至少理论上是这样
for the model I'm playing with
the true embedding of Queen
虽然这是我正在使用的模型的一个经典例子
is actually a little farther
off than this would suggest
但实际上，真正queen的嵌入实际上比这种方法预想的要远一些
presumably
because the way the queen is used in training data
这可能在训练数据中，女王并不仅仅是king的女性版本
is not merely a feminine version of king
when I played around
family relations seem to illustrate the idea
深入挖掘时，我发现通过家族关系来解释这种现象更为合理
much better
the point is it looks like during training
关键在训练过程中，模型发现采用这种嵌入方式更有利
the model found it advantageous to choose in bettings
such that one
direction in this space encodes gender information
即这个空间中的一个方向能编码性别信息
don't just ride the index
seek to outperform it with Fidelity Active etfs
learn more at Fidelity Com
Active
another
example is that if you take the embedding of Italy
另一个例子是，如果你从意大利的向量表示中减去德国的向量表示
and you subtract the embedding of Germany
and then you add that to the embedding of Hitler
you get something
再加上希特勒的向量表示
very close to the embedding of Mussolini
结果非常接近墨索里尼的向量表示
it's as if the model Learned to associate some
directions with Italianness
这就好像模型学会了将某些方向与意大利的特性相关联
and others with World War

II axis leaders
而将其它方向与二战轴心国的领导人相关联
maybe my favorite example in this vein
我个人最喜欢的一个例子是，在某些模型中
is how in some models
if you take the difference between Germany and Japan
如果你计算德国和日本的向量差值
and you add it to sushi
you end up very close to Broadworst
然后加上寿司的向量，你得到的结果非常接近德国香肠
also in playing this game of finding nearest neighbors
此外，在寻找最近邻的过程中，我还惊喜的发现猫和野兽和怪物距离都很近
I was very pleased to see how close
cat was to both beast and monster

one bit of mathematical intuition
有一个有用的数学概念
that's helpful to have in mind
especially for the next chapter
尤其对接下来的章节非常重要
is how the dot product of two
那就是两个向量的点积
vectors can be thought of as a way to measure
how well they align
可以视为一种衡量它们是否对齐的方法
computationally
dot products involve
从计算的角度看
multiplying all the corresponding
点积涉及逐一对应元素相乘，然后进行求和
components and then adding the results
which is good
这很好，因为我们的很多计算看起来就像是权重求和
since
so much of our computation has to look like weighted
sums geometrically
从几何角度看
the dot product is positive
when vectors point in similar directions
如果两个向量指向相似方向，点积为正
it's zero if they're perpendicular
如果相互垂直，点积为0
and it's negative whenever they point in
opposite directions
如果指向相反方向，点积为负数
for example let's say you were playing with this model
假设你正在使用某个模型
and you hypothesize

that the embedding of cats
cat might represent a sort of plurality
从cats的向量表示减去cat的向量表示，可能会在这个空间中找到复数概念的方向
direction in this space
to test this I'm going to take this vector
and compute its dot product
为了验证这个观点，我将计算某个向量与一些特定单数名词嵌入的点积
against the embeddings of certain
singular nouns

and compare it to the dot
products with the corresponding plural nouns
并将其与相应的复数名词点积进行比较
if you play around with this
you'll notice that the plural ones do indeed
如果你试一试，你会发现复数名词的点积值通常比单数的值更高，表明它们在某种方向上的对齐更为紧密
seem to consistently give higher values than the
singular ones
indicating that they align more with this direction
it's also fun how
if you take this dot product with the
更有趣的是，如果你将这个点积操作应用到一二三这些词汇上时，
embeddings of the words 1
2 3 and so on
they give increasing values
你会发现结果是逐渐增加的
so it's as if we can quantitatively measure
how plural the model finds a given word
就像我们能够量化的衡量模型认为一个词的复数程度
again the specifics for how words get
embedded is Learned using data
再次声明，单词的嵌入方式是通过数据学习得到的
this embedding matrix
whose columns tell us what happens to each word
这个词嵌入矩阵表明了词汇的转换过程
is the first pile of weights in our model
and using the GPT3 numbers
它也是模型的第一批权重，根据GPT3的数据
the vocabulary size specifically is 50
057 and again
词汇量是50257，但要注意
technically this consists not of words per se
but of tokens
实际上它指的不是单词本身，而是tokens
and the embedding dimension is 12 288
multiplying those tells us this consists of about 617
million weights

嵌入的维度是12288，将两者相乘，将得到6.17亿个权重


let's go ahead and add this to a running tally
remembering that by the end
we should count up to 175 billion
将这个数字加入到累计计数中，最后我们应该得到1750亿个权重
the case of transformers
you really want to think of the vectors in this
embedding space
在谈到transformers时，你会想到这些嵌入空间中的向量
as not merely representing individual words
不仅仅代表着单个词汇
for one thing
they also encode information
about the position of that word
还携带了词汇位置信息
which we'll talk about later
这一点稍后会说明
but more importantly
但更关键的是
you should think of them as having the capacity
to soak in context
这些向量能够吸纳并反应语境
a vector
that started its life as the embedding of the word king
for example
举个例子，一个最初代表国王的向量
might progressively get tugged and pulled by various
blocks in this network
在网络的各个环节的作用下，可能会逐渐变化
so that by the end
因此到最后，它指的方向会更具体更微妙
it points in a much more specific and nuanced direction
that somehow encodes that it was a king who
lived in Scotland
以某种方式编码一位生活在苏格兰的国王
and who had achieved his post
after murdering the previous king
在杀死前任国王后取得其职位的国王
and who's being described in shakespearean language
think about your own understanding of a given word
此人的描绘方式充满了莎士比亚式的语言
想想你对某个词汇的理解通常是怎么形成的
the meaning of that word is
clearly informed by the surroundings
那个词的意义很大程度上是由其所处的环境决定的
and sometimes this includes context from a long
distance away
有时甚至包含很远的上下文
so in putting together
a model that has the ability to predict
what word comes next
因此，在构建一个能预测下一个单词的模型时
the goal is to somehow
empower it to incorporate context efficiently
关键目标就是让它能够高效的融合上下文信息
to be clear in that very first step
明确一点，在第一步
when you create
the array of vectors based on the input text
当我们根据输入文本创建向量数组时
each one of those is simply plucked out of the
embedding matrix
每个向量都是直接从嵌入矩阵选取的
so initially
each one can only encode the meaning of a single word
without any input from its surroundings
这意味着，起初，每个向量仅能代表一个单词的含义
but you should
think of the primary goal
of this network that it flows through
而不涉及周边环境的信息
as being to enable
each one of those
vectors to soak up a meaning that's much more
但我们的目标就是让这些向量通过网络的传递
rich and specific
than what mere individual words could represent
使得每一个向量都能获取比单个词更丰富更具体的含义
the network can only process a fixed number of
vectors at a time
这个网络每次只能处理一定数量的向量
known as its context size
for GPT3 it was trained with a context size of 2048
这就是所谓的上下文大小，GPT3的上下文大小是2048
so the data flowing through the network
always looks like this
array of 2048 columns
意味着数据在网络流动时，总是看起来像一串2048列的数组
each of which has 12 000 dimensions
每一列都有12288个维度
this context size
limits how much text the transformer can incorporate
这个上下文大小限制了transformer在预测下一个词时可以纳入的文本量
when it's making a prediction of the next word
this is why long conversations with certain chatbots
这就解释了，为什么如果和一些聊天机器人
like the early versions of ChatGPT
比如，早期的ChatGPT版本
often gave the feeling of the bot kind of
losing the
进行长对话时，你可能会觉得机器人在对话中迷失了方向
thread of conversation as you continued too long
尤其是对话持续过长时
we'll go into the details of attention in due time
but skipping ahead
我们会在适当的时候详细讨论注意力机制的细节
I want to talk
for a minute about what happens at the very end
remember
先让我们简单了解下最终阶段的处理过程
the desired output is a probability distribution
over all tokens that might come next
请记住，最终的目标是产生一个概率分布，预测下一个可能出现的token
for example if the very last word is Professor
and the context includes words like Harry Potter
举个例子，如果最后一个词是教授，上下文中包含了哈利波特等词语
and immediately preceding we see
least favorite teacher
紧接着出现的是最不喜欢的老师
and also if you give me some leeway
如果允许我稍微发挥一下
by letting me pretend that token simply look like
full words then
假设tokens就是完整的单词
a well trained network
that had built up knowledge of Harry Potter
那么，一个经过良好训练，并对哈利波特世界有所了解的模型
would presumably assign a high number
to the word Snape
可能会给斯内普这个词一个较高的权重
this involves two different steps
the first one is to use another matrix
此过程涉及两个不同步骤，首先，使用另一个矩阵
that maps the very last vector in that context
to a list of 50
将上下文中的最后一个向量映射到一个包含50000个值的列表
000 values one for each token in the vocabulary
then there's a function that normalizes this
词表中的每个token都对应一个值
into a probability distribution
it's called Softmax
接着，通过一个函数把这些值转换成概率分布
and we'll talk more about it in just a second
这个函数叫softmax，我们将稍后讨论
but before that
it might seem a little bit weird to only use this
但在此之前，你可能会觉得
last embedding
to make a prediction
仅仅基于最后一个嵌入最决策似乎有些奇怪
when after all in that last step
毕竟在最后一层中，还有其它向量
there are thousands of other vectors in the layer
just sitting there
with their own context rich meanings
每个向量都蕴含着丰富的上下文意义
this has to do with the fact that in the
training process
这时因为在训练过程中
it turns out to be much more efficient
如果利用最后一层的每一个向量来同时预测其后可能出现的内容，被证明是更高效的方法
if you use each one of those vectors in the final layer
to simultaneously
make a prediction for what would come
immediately after it
there's a lot more to be said about training later on
关于训练的更多细节我们稍后还会提到
but I just want to call that out right now
this matrix is called the Unembedding Matrix
现在我先简单指出一点，这个矩阵被称为unembedding矩阵，我们用Wu来表示
and we give it the label Wu
again
like all the weight matrices we see
its entries begin at random
就像我们见过的所有权重矩阵一样，这个矩阵的初始值也是随机的
but they are Learned during the training process
但在训练过程中这些值会被更新
keeping score on our total parameter count
关于参数总数的统计，这个unembedding矩阵为词汇表中的每个词都分配了一行
this unembedding matrix has one row

for each word in the vocabulary
and each row has the same number of elements as the
embedding dimension
每一行的维度与嵌入维度一样，12288
it's very similar to the embedding matrix
这与嵌入矩阵相似，只不过是其逆过程
just with the order swapped
so it adds another 617
因此，他为网络增加了另外7.17亿参数
million parameters to the network
meaning our count so far is a little over a billion
到目前为止，参数总数已经超过10亿
a small but not wholly insignificant fraction
虽然是总参数量的一小部分，但确很重要
of 175 billion that we'll end up with in total
as the very last mini lesson for this chapter
在这章的最后一部分
I want to talk more about the Softmax function
since it makes
我想介绍一下softmax
another appearance for us once we
dive into the attention blocks
因为它在注意力机制中会再次成为焦点
the idea
is that if you want a sequence of numbers to act
as a probability distribution
如果你想让一串数字成为概率分布
say a distribution over all possible next words
例如，预测下一个可能出现的词的概率
then each value has to be between 0 and 1
and you also need all of them to add up to 1
这些数字需要在0-1之间，且总和为1
however if you're playing the deep learning game
where everything you do looks like matrix
但是，如果你正在探索深度学习，你所看到的每一步操作可能看起来都是矩阵与向量的乘法
vector multiplication
the outputs that you get by default
don't abide by this at all
那么你得到的结果可能并不符合这个条件
the values are often negative or much bigger than 1
这些值可能是负数，也可能大于1
and they almost certainly don't add up to one
而且加起来和几乎可以确定不为1

Softmax is the standard way
to turn an arbitrary list of numbers
softmax就是一种标准方法可以把任意一组数字转换为一个概率分布
into a valid distribution
in such a way
that the largest values end up closest to 1
使得最大的值可能接近1
and the smaller values end up very close to 0
最小的值接近0
that's all you really need to know
but if you're curious
了解这一点就够了，但如果你感到好奇
the way that it works
is to first raise
具体的工作原理是：首先对每个数字进行e的指数运算
e to the power of each of the numbers
which means you now have a list of positive values
and then you can take the sum
这样就得到了一组正数，然后取所有正数的和，然后每个数字除以总和
of all those positive values
and divide each term by that sum
which normalizes it into a list that adds up to one
这样就能把它们标准化为一个和为1的概率列表
you'll notice that if one of the numbers in the input
is meaningfully bigger than the rest
你会注意到，如果输入中的某个数值明显大于其它数值
then in the output
the corresponding term dominates the distribution
那么在输出中这个值对应的项在概率分布中起主导作用
so if you were sampling from it
you'd almost certainly just be picking the
maximizing input
几乎可以确定，你在采样时会选取这个最大的输入值
but it's softer than just picking the max
in the sense that when
other values are similarly large
但通常会选择比直接选取最大值更细腻的方法
they also get meaningful weight in the distribution
因为当其它值也接近最大值时，它们在分布中也能获取重要的比重
and everything changes continuously
as you continuously vary the inputs
而且随着你不断地改变输入，一切都会连续地变化
in some situations
like when Chatchipt
在一些情况下，比如，像ChatGPT利用这个分布选择下一个词时
is using this distribution to create a next word
there's room for a little bit of extra fun
可能会为这个函数增加一些额外的趣味
by adding a little extra spice into this function
with a constant t thrown into the denominator of those
exponents we call it the temperature
可以通过在指数的分母里添加一个常量t来实现，我们称之为温度
since it vaguely resembles the role of temperature in
certain thermodynamic equations
因为在某种程度上，与热力学方程中的温度作用相似
and the effect is that when t is larger
you give more weight to the lower values
它的效果是，当t较大时，会使较小的数值获得更多的权重
meaning the distribution is a little bit more uniform
使得分布稍微均匀一些
and if t is smaller
而t很小时
then the bigger values will dominate more aggressively
较大的值会占据主导地位，极端情况下，如果把t设置为0，所有权重都会集中到最大值上
wherein the extreme setting t equal to 0
means all of the weight goes to that maximum value
for example
例如，用GPT-3生成一个故事
I'll have gpt3 generate a story with the C to text
once upon a time there was a
but I'm going to use different temperatures
提示是：“once upon a time there was a
”我会在每次测试中使用不同的温度
in each case temperature zero
means that it
always goes with the most predictable word
温度为0，意味着他总是选择最可能的词
and what you get ends up being
kind of a trite derivative of Goldilocks
而你所得到的结果就变成了一个陈词滥调的金发姑娘的故事
a higher temperature
gives it a chance to choose less likely words
较高的温度给它提供了选择不太可能出现的词机会
but it comes with a risk
in this case the story starts out a bit more originally
但这也伴随着风险，在这个例子中，故事开始较为原创
about a young web artist from South Korea
讲述的是一位韩国年轻艺术家
but it quickly degenerates into nonsense
但很快就变得毫无意义
technically speaking
the API doesn't actually let you pick a temperature
bigger than 2
严格地说，API并不允许你选择大于2的温度
there is no mathematical reason for this
这个限制并没有数学上的根据
it's just an arbitrary constraint imposed
只是一个经验值
I suppose to keep their tool from being seen
generating things that are too nonsensical
目的是防止产生过于荒诞的结果
so if you're curious
the way this animation is actually working
is I'm taking the 20 most
所以如果你感到好奇，这个动画的原理是这样的，我选择了GPT-3生成的可能性最高的前20个词
probable next tokens that GPT3 generates
which seems to be the maximum they'll give me
and then I tweak the probabilities based on an
这看起来是它们能给我们最大的数量
exponent of 15th
as another bit of jargon
in the same way that you might call the components of
the output of this function
probabilities
people often refer to the inputs as logits
or some people say logits some people say logits
I'm going to say logits
so for instance
when you feed in some text
you have all these word
embeddings flow through the network
and you do this final
multiplication with the unembedding matrix
machine learning people
would refer to the components in that raw
unnormalized output
as the logits for the next word prediction
a lot of the goal with this chapter
was to lay the foundations for
understanding the attention mechanism
Karate Kid wax on wax off style
you see
if you have a strong intuition for word embeddings
for Softmax for how dot products measure similarity
and also the underlying premise that
most of the calculations have to look like
matrix multiplication
with matrices full of tunable parameters
then understanding the attention mechanism
this cornerstone piece in the whole modern boom in AI
should be relatively smooth
for that come join me in the next chapter
as I'm publishing this
a draft of that next chapter is available for review
by Patreon supporters
a final version should be up in public in a week or two
it usually depends on
how much I end up changing based on that review
in the meantime
if you want to dive into attention
and if you want to help the channel out a little bit
it's there waiting


斯坦福2024 AI指数报告
https://aiindex.stanford.edu/report/?sf187707917=1
https://drive.google.com/drive/folders/1_9oLjgrgZlRdAWOY1fhGNlPv9nSfDDwN
https://aiindex.stanford.edu/wp-content/uploads/2024/04/HAI_AI-Index-Report-2024.pdf



**20240417**
大模型当道，是Chat？还是Agent?


大模型当道，是Chat？还是Agent?
要想弄明白这个问题，还得先了解Agent和Chat是什么。

Chat（聊天），纯粹的Chat，像是一个主要由“大脑和嘴”构成的智能体，专注于信息处理和语言交流。比如ChatGPT这样的系统，它能够理解用户的查询，给出有用和连贯的回答，但它本身不直接执行任务。

Agent（代理）：像一个具有“手、脚”的智能体，它能够进行思考、决策，并且能执行具体的任务。

为了更好地理解Agent，让我举一个生动的案例。

比如你是一个服装厂的老板，可能你会经常考虑当前应该生产什么衣服？

但如果你直接去问大语言模型，那大概率会得到一堆官样文章。

对于这种知识密集型或者受多方因素影响的决策任务，通过大模型的一次简单问答，几乎不可能得到有价值的结果。

因此我们不妨先来观察一下人类在解决这种问题时候的行为，首先我们可能会按照经验，思考出一些影响衣服销量的主要因素，比如天气情况，当前流行的趋势，原料成本，自身的产能等等。

于是根据这些思考，我们会自然而然的将这个大的目标拆分为一些具体的，更容易执行的小目标，比如去看看未来一个月的天气情况，去看看大众最近关于衣服潮流的观点，或者看看最近友商的哪些款式卖的最好等等，这种目标拆分可谓是人类解决问题的惯用手法了。

然后就是采取具体的动作，去实现这些小目标。对于第一个我们会打开天气软件得到未来一个月的天气数据，比如，发现未来一个月会出现一次大的升温，这时候我们可能会利用这个结果对前面拆分的小目标进行一些调整和细化，比如把后续的研究聚焦到夏季服装上，继续执行小目标，我们可能会去各大网络社交平台，查看和夏季服装相关的讨论内容，比如，发现大家最近对跨凉背心热情高涨
，如此，我们可能会利用这次的动作结果继续调整小目标，让后续的研究聚焦到跨凉背，然后我们打开电商网站搜索跨凉背心，发现销量高的是一些价格较低但透汗效果很好的产品，如此，我们可能就得到了一个初步的结论：接下来制定的小目标可能就是围绕材料产能和成本展开了，于是，我们去查询自己登记的采购原料中能达到同样透汗效果的布料信息，可能还会拿出计算器算一下用这种布料制作一件背心的成本几何，最终能不能也卖到这么低或者比大家都低的价格，如果能做到，我们可能就得到了最终的答案，而如果不能，我们可能就会去看看除了透汗以外，背心还有哪些其他受欢迎的特点，如此这般，你会发现，人类在解决复杂问题的时候，往往都会陷入一种碎碎念的状态，或者说在内心不断的嘀嘀咕咕，当然还有一个更漂亮的说法：叫思维链。

在这个嘀咕的过程中我们总会情不自禁地将一个复杂的大目标拆分为许多个更好解决的小目标，然后一步一步地去逐个击破，并且我们每次还会根据动作产生的结果，对小目标的分解产生一些调整，然后继续不断的重复这个过程，直到越来越接近或者说收敛到初始目标的答案，而Agent正是人们基于大语言模型试图对人类解决问题行为模仿的一种尝试。


总结一下，Chat强调的是“说”，Agent强调的是“做”。


要你你选哪个？这么讲吧，但凡人类对于躺平的美好追求还在，“让机器替我干活”的念头就不可能熄灭。自ChatGPT发布后，从plugin的推出，到Function Calling再到Assistant API的面世，OpenAI这一系列动作就充分表明，有这么强大的LLM作为基本盘的情况下，人们就不可能仅仅满足于让它“嘚啵嘚”。历史总是惊人的相似。从2014年亚马逊开创性推出Amazon Echo开始，智能音箱横空出世。一开始的智能音箱，也只是有个“嘴”，只能实现播放音乐、查询信息、设置提醒等功能。而且“脑子”还不太灵光。但是随着阿里、百度、小米等科技巨头的纷纷加入，智能音箱在竞争中卷出了新高度。打通支付、和智能汽车、智能家居互通，智能音箱不断地突破和扩展功能边界，逐步坐到了智能家居生态的“大总管”位置上。随着应用场景的持续拓展，智能音箱又延展到儿童教育、养老关怀等领域，深刻影响了人们的日常生活。相信有一天，智能音箱会强大和多样化到一个程度，以至于“智能音箱”这个名字不再适合这个品类，那将是新一轮故事的开始。同样惊人相似的，还有从单纯的AI智能语音助手、智能客服（只会说）到以AI+RPA为核心技术的的AI数字员工（会说又会做）的发展史。。。这些，都是人工智能走向多元化和融合化的一个个缩影。因此，随着技术水平的不断进步和场景化落地的不断挖掘，Chat和Agent的界限必定会越来越模糊，生成式 AI 会融合Chat和Agent的特点，形成既能进行高质量、高人格化对话，又能高效执行复杂任务的 AI 自动化系统，为人们提供融合、互补、多样化的解决方案。AI Agent的崛起不仅仅是技术上的突破，更是对软件开发理念的一次深刻变革。在传统的软件开发中，程序员需要预先定义所有的逻辑和规则，然后进行代码实现。而AI Agent的出现，要求我们对软件进行充分地“放权”：它由一颗大脑（LLM）来进行自主支配运行，并在运行时自动学习、适应和调优。这种前所未有的开发范式的转变，让程序员不得不重新思考软件开发的本质，也重新思考软件开发的未来。

典型的AI agent分为Memory（记忆）、Tools（外部工具） 、Planning（计划） 和Action（行动）四个模块。

当前学习AI Agent基本上分作两条路径：

基于OPenAI技术路线，以及基于开源技术路线。建议每个技术人员，都选择一条路，亲自趟一趟。



大模型爆发之后，AI Agent的发展也可谓是一日千里，各种项目层出不穷。

AutoGPT项目地址：https://github.com/Significant-Gravitas/AutoGPT可以根据你设置的目标，将实现这个目标的任务进行拆解，再采用搜索、浏览网站、执行脚本等方式一条条去执行任务，帮你完成目标。JARVIS项目网址：https://github.com/microsoft/JARVIS一个非常有意思的“模型选择”Agent。它将用户要求拆解成子任务，再到Huggingface上选择合适的专家小模型执行任务，最后对结果进行处理和返回给用户。



由于JARVIS可以调用其它模型工具，因此它可以执行多模态任务。

MetaGPT项目网址：https://github.com/geekan/MetaGPTMetaGPT是另一个开源人工智能体框架，试图模仿传统软件公司的结构。与ChatDev类似，Agent被分配产品经理、项目经理和工程师的角色，并且他们在用户定义的编码任务上进行协作。工具、平台、社区的不断成熟，为个体开发者提供了一个全新的舞台。程序员与人工智能之间的距离从未如此之近。AI Agent的崛起，让有想法、有技术的人能够以前所未有的方式释放自己的创造力，打造出各种有趣、实用的AI原生应用。那么，你准备好加入这场革命了吗？

图像编码器成为累赘
一开始的图像编码器，要么在有监督数据上预训练一个目标检测模型，或者干脆直接使用ResNet这种骨干网络提取特征，但无论哪一种都有问题：效率和推理速度。

BLIP ：
统一视觉语言理解和生成

理解：图文检索，视觉问答，VR VE
生成：Image Captioning

现存模型不足：
Encoder only：没有解码器，没办法直接应用到文本生成的任务中

encoder-decoder：虽然能生成文本，但没有一个统一的框架能把生成和理解统一在一起



一开始就只需要一个图像编码器和文本编码器
如何对齐？
训练过程：
对比学习方法
对比学习损失函数

推理过程：
VQA：闭集
分类
检索

ALBEF：MLM，掩码
BLIP：LM，自回归

**start**
多模态的定义

多模态任务类型
理解，生成

模型构成：图像编码器，文本编码器，多模态融合，文本解码器

图像编码器：CNN，ViT
文本编码器：Transformer Encoder
多模态融合：Transformer Encoder
文本解码器：Transformer Decoder

从最简单开始

CLIP
图像编码器：ResNet或者ViT
对齐学习
对比学习损失函数

能完成的任务
分类，检索，闭集VQA

优缺点：速度快,事先计算好向量
善于检索和分类，不善于复杂的生成任务，因为融合模块只是简单的向量点积

融合升级时代
从经验看图像编码器应该比文本编码器大，并且需要一个复杂的多模态融合模块

ALBEF
采用Transformer Encoder作为多模态融合模块
增加两个损失函数：NLM，BERT编码器，掩码语言模型损失函数
ITM：BERT编码器有个CLS Token，是否图文是否匹配

VilT
图像编码器采用预训练目标检测模型，速度慢，效率低
图像编码器和文本编码器统一采用Transfomrer架构

大一统时代

模型架构的统一以及效率

VLMO
统一模型，混合专家模型


BLIP
前面适合理解，不适合生成任务
这时候需要加一个Decoder模块

模型参数共享
图片经历一次前向过程，文本经历三次

CoCa
与AlBEF像
一个图像编码器
一个Transformer解码器分两半

只使用Decoder
只使用ITC和LM损失
解决训练效率问题

BeiT v3

以前图像编码器：Vit
文本：Transformer
大一统，把图像也看做是一个语言

不要ITM ITC，只要MLM损失



大模型加持时代
大语言模型+视觉预训练模型：通过Adapter实现模态之间的对齐

Flamingo

BLIP-2

LLaVA

对于数据的操作：
数据集越来越大，数据集越来越不nosy
BLIP：cap filter
ALBEF：伪标签
硬编码


未来一定是多模态的
双塔，CLIP
encoder-decoder BLIP CoCa
只用encoder：ALBEF VLMO


**end**



Transformer中的自注意力机制，QKV

我们看文章，看视频时，如果不带着问题，或者问题少，看完之后收获不是很大，很快就忘了

你携带的问题就是Q，多头注意力，就是多个问题，问题越多，收获越大


**20240418**

!wget https://huggingface.co/visheratin/segment-anything-vit-b/resolve/main/encoder-quant.onnx
!wget https://huggingface.co/visheratin/segment-anything-vit-b/resolve/main/decoder-quant.onnx
!wget https://clip-as-service.s3.us-east-2.amazonaws.com/models/onnx/ViT-B-32/visual.onnx
!wget https://clip-as-service.s3.us-east-2.amazonaws.com/models/onnx/ViT-B-32/textual.onnx




GPT是generative pre-trained transformer首字母缩写
第一个词很明显地指出了它是用来生成新文本的。
pre-trained表明模型需要经历在大量数据上预训练过程，这个词也暗示了模型后续在某个特定任务上继续进一步微调的可能。

最后一个词才是最重要的部分，transformer是一种特定类型的神经网络，一种机器学习模型，它是现今AI高速发展的核心创新。

本文将通过可视化的方式探索Transformer内部原理，我们将逐步跟随数据流经模型的整个过程。

你可以基于transformers构建多种类型的模型，有的模型接受音频输入并生成文本

也可以是一个逆过程，根据文本生成语音

在2022年风靡全球的绘画工具DALL-E和Midjourney，能够根据文本生成图像，也是基于transformer的，即使有时我对它生成的内容不是很理解，但我对此仍感到惊讶。

最初的transformer是谷歌在2017年提出的。

起初主要用于将一种语言翻译成另一种语言

但是像其它一些变体，例如ChatGPT，是一个接受一段文本（可能也会伴随图像和声音）的模型，然后预测文章接下来的内容。

预测接下来可能出现的单词的概率分布

乍一看，预测下一个词与生成新文本是不同的

但一旦你有了这样的模型，你可以试着让它生成一段长文本

方法就是给它一个初始文本片段，然后随机从刚才生成的概率分布中选取一个样本

然后将这个样本追加到文本中

接下来重复这个过程去继续生成新的预测

新的预测要基于所有的文本，包括刚刚添加的

我不知道你怎么看，但我觉得这种方法的效果可能并不理想

举个例子

我在我的电脑中运行GPT2

然后尝试基于一段文本生成一段故事，让它不断地去生成下一个词。

结果，这个故事基本没什么逻辑可言

但是，如果我换成GPT3，这是同样的模型，只是规模更大，

突然间就像变魔法一样，不仅故事逻辑合理

甚至在故事中暗示出一个Π生物可能居住在一个充满数学和计算的世界里

这个过程就是通过重复的预测和选取来生成文本
ChatGPT和其它大模型也是一样，逐字地去生成文本，后面我们将可视化这个概率分布，看到每个词时的概率。

在此之前，让我们先从宏观上看数据是如何在transformer中流转的

然后，我们将详细探讨解释每一个步骤，并对其进行扩展

大体上讲，当生成某个词时，下面就是它底层运行机制

首先，输入文本会被拆分成小片段，这些小片段被称为tokens

对于文本来说，这些tokens通常是单词或者单词的一部分或者是其它常见的字符组合

如果包含图片或者声音，tokens可能是图像被划分为一个个小patch，或者是声音片段

然后，每个token都对应到一个向量上，也就是一串数字

这串数字的意义是从某种方式来表达这个片段的含义

如果你把这些向量看作高维空间中的坐标

那么，含义相似的词汇倾向彼此接近的向量上

这些向量序列接下来会经过一个称为“注意力块”的处理过程

使得向量之间能够相互交流

并根据彼此信息更新自身的值

例如，model这个词，在machine learning model中的意思和a fashion model中意思是不同的

注意力块的作用就是确定上下文中的哪些词对更新其它词的意义有关

以及该如何准确的更新其含义

每当我说道“含义”这个词时，完全是通过向量中的数字来表达的。

之后，这些向量会经过另一种处理

这个过程可能被称作多层感知机或者前馈神经网络

这个阶段向量之间不再产生交流

而是并行的经历同一处理

虽然这个步骤比较难理解

但我们会在后面讨论，这个步骤有点像对每个向量提出一系列问题

然后根据这些问题的答案更新向量

这两个处理阶段的操作本质上都是大量的矩阵乘法

我们主要学习的是：如何解读这些背后的矩阵

在讲解中，我省略了一些中间步骤的归一化细节，因为这毕竟只是宏观概览

接下来的过程就是重复的了

就是在注意力模块和多层感知机模块之间不断切换

直到最后，我们期望通过某种方式，文章的核心意义已经被完全融入到序列的最后一个向量中

然后对最后一个向量进行特定操作

产生一个覆盖所有可能token的概率分布

这些token代表接下来可能出现的小段文本

就像我说的

一旦你有了这样的工具，根据一段文本预测下一个token

你可以给它输入一段初始文本，让它不断地预测下一个token

然后从概率分布中取样，追加到现有文本中

然后不断重复这个过程

了解这一点的人可能还记得，早在ChatGPT出现以前，GPT3是这样工作的

根据一段起始文本自动补全故事和文章

讲这样的工具转化为机器人的一个简单方法是，准备一段文本，设定一个场景，也就是所谓的系统提示，

然后将用户的初始问题或者提示词作为对话的开头。

接着让AI开始预测，这个有用的AI助手会如何回应

为了使这个过程运行得更好，还需要额外的训练步骤

不过总的来说，这就是基本思路

在这一章，我们将详细探讨网络开始和网络结束发生的事情

同时，我也会花大量的时间回顾一些重要的背景知识

这些知识对于熟悉Transformer的工程师来说都是基础知识

如果你对背景知识比较熟悉，而且迫不及待了解更多

你可以直接跳到下一章节

这一章将会关注Transformer的核心部分，即注意力模块

在这之后，我还会详细讨论多层感知机模块，训练过程以及一些被忽略的细节

同时，这些视频也是深度学习系列视频的补充

你不一定非得按照顺序来看

但是在深入研究Transformer之前，我认为有必要确保对深度学习的基本概念和模型有一些了解

这里需要明确的是，机器学习是一种方法论

它涉及到使用数据来指导模型行为的模式

具体来说

你可能需要一个函数，输入一张图片

输出对应的标签描述

或者预测文本片段的下一个词

或者其它需要直觉和模式识别的任务

虽然我们现在已经习以为常

但是机器学习的核心思想在于，我们不再尝试去编写固定的程序来完成这些任务

这是AI在最早阶段人们会做的事情

而是构建一个具有可调节参数的灵活结构

就像一系列旋钮和调节器

然后通过对大量实例输入和期望输出的学习

来调整和微调参数的值

来模拟这种行为

例如，最直观的机器学习模型：线性回归

输入输出都是单个数字

如房子的面积和价格

你要做的就是找到一条能拟合这些数据的线

以次来预测将来的房价

这条线由两个参数决定，即斜率和截距

线性回归的目标是

确定能够完美匹配这些数据的参数

不用说，深度学习模型更加复杂
gpt3 for example has not two but 175 billion parameters
比如，GPT3有1750亿个参数，而不仅仅是两个
but here's the thing
it's not a given that you can create
但值得注意的是
some giant model with a huge number of parameters
without it either
并不是简单地构建一个参数众多地庞大模型就能有效工作
grossly overfitting the training data
这样做可能会导致模型严重过拟合训练数据
or being completely intractable to train
或者训练起来极其困难
deep learning describes a class of models
深度学习涵盖了一系列具有出色扩展能力地模型类别
that in the last couple decades
have proven to scale remarkably well
what unifies them is that they all use the same
它们之所以能够成功，关键在于都采用了相同地训练方法
training algorithm
it's called back propagation
即反向传播
we talked about it in previous chapters
我们在前面章节已经介绍过了
and the context that I want you to have as we go in
is that in order for this
你需要理解的是
training algorithm to work well at scale
要让这种训练方法能在大规模应用中顺利进行
these models have to follow a certain specific format
模型必须遵循一种特定的结构
and if you know this format going in
it helps to explain many of the
如果你对这种结构有所了解
choices for how a transformer processes language
which
就能更好的理解Tranformer处理语言的方式及其背后的逻辑
otherwise run the risk of feeling kind of arbitrary
first whatever kind of model you're making
首先，不管你是构建哪种模型
the input has to be formatted as an array of real
输入必须是一个实数数组
numbers this could simply mean a list of numbers
it could be a two dimensional array
这可能知识一个数字列表，也可能是一个二维数组
or very often you deal with higher dimensional arrays
where the general term used is tensor
或者更常见的高维数组，我们通常称之为张量
you often think of that
input data as being progressively transformed
这些数据通常会被逐步流经多个不同的层
into many distinct layers
where again each layer is always structured

as some kind of array of real numbers
每一层的输出也可被组织成实数数组
until you get to a final
layer which you consider the output
直到最后一层，你可以视其为输出层
for example
the final layer in our text processing model
例如，我们的文本处理模型输出层最终输出一个数字列表
is a list of numbers
representing the probability distribution
for all possible next tokens
这些数字代表了所有可能的下一个词的概率分布
in deep learning
these model
parameters are almost always referred to as weights
在深度学习领域，这些模型的参数通常被称为权重
and this is because a key feature of these models
is that the only way
这样称呼的原因是，这些模型的一个核心特点是
these
parameters interact with the data being processed
这些参数与正在处理的数据之间的唯一交互方式
is through weighted sums
就是通过权重和
you also sprinkle some nonlinear functions throughout
but they won't depend on parameters
虽然模型中也会穿插一些非线性函数，但它们并不依赖这些参数
typically though

instead of seeing the weighted sums all
通常来说，我们不会直接看到这些权重零散的工作
naked and written out explicitly like this

you'll instead find them packaged together
而是被组织成矩阵向量乘积的形式
as various components in a matrix vector product
it amounts to saying the same thing
如果你了解矩阵向量乘法计算规则，你就会发现这其实是表达同一种概念
if you think back to how matrix vector multiplication
works each component in the output
输出的每一个元素就像是一个权重和
looks like a weighted sum
it's just often conceptually
cleaner for you and me to think about
更直观的方式是，将这些可调参数矩阵想象成对数据进行转换的工具
matrices that are filled with tunable parameters
that transform
vectors that are drawn from the data being processed
例如，GPT3中的1750亿个权重
for example those 175 billion weights in GPT3
are organized into just under 28 000 distinct matrices
those matrices in turn fall into
就被组织在大约28000个不同的矩阵中
eight different categories
这些矩阵又被划分为八个不同类型
and what you and I are gonna do
你和我将要做的就是逐一理解这些矩阵
is step through each one of those categories to
understand what that type does
了解没有类别的功能
as we go through
I think it's kind of fun
接下来的过程将会非常有趣
to reference the specific numbers from GPT3
我们将参考GPT3的具体数据
to count up exactly where those 175 billion come from
来统计这1750亿个参数是如何分配的
even if nowadays there are bigger and better models
this one has a certain charm as the first
即使现在有更大更好的模型，GPT3仍具有独特的魅力
large
language model to really capture the world's attention
作为第一个引发全球关注的大语言模型
outside of ML communities
影响力并不局限机器学习社区
also practically speaking
实际上
companies tend to keep much tighter lips around
对于更先进的模型，公司往往会对具体的数据保密
specific numbers for more modern networks
I just want to set the scene going in
这里我想说明的是
that as you peek under the hood to see what happens
inside a tool like ChatGPT
当你探索像ChatGPT这样工具的内部机制时，你会发现几乎所有计算过程都体现为矩阵和向量乘积
almost all of the actual computation
looks like matrix vector multiplication
there's a little bit of a risk getting lost in the sea
of billions of numbers
在这海量的数字中，很容易迷失方向
but
you should draw a very sharp distinction in your mind
但你需要清楚区分两个概念：模型的权重（用红色和蓝色表示）和正在处理的数据（用灰色表示）
between the weights of the model
which I'll always color in blue or red
and the data being processed
which I'll always color in gray

the weights are the actual brains
权重是模型的大脑
they are the things Learned during training
它们在训练过程中会被修改
and they determine how it behaves
它们决定了模型的行为模式
the data being processed
simply
encodes whatever specific input is fed into the model
而正在处理的数据仅仅编码了输入到模型中的具体输入
for a given run
like an example snippet of text
比如，一段文本示例
with all of that as foundation
理解了上述基础之后
let's dig into the first step of this text processing
让我们深度探讨文本处理的第一步
example which is to break up the input into little
将输入分割成小片段
chunks and turn those
chunks into vectors
将这些小片段转换成向量
I mentioned how those chunks are called tokens
我之前说过，这些小片段被称为token
which might be pieces of words or punctuation
它们可能是单词的一部分或者标点符号
but every now and then in this chapter
但在本章，特别是下一章
and especially in the next one
I'd like to just pretend that it's broken more cleanly
into words because we humans think in words
我将简化理解，假设它们按单词进行划分，因为人类就是按照单词进行思考
this will just make it much easier to reference little
examples and clarify
通过一个小例子来解释每一步就能使这个过程更容易理解了
each step
the model has a predefined

some list of all possible words say 50 000 of them
模型具有一个预设的词汇库，包含所有可能的单词，比如说有50000个
and the first matrix that we'll encounter
known as the embedding matrix
我们将首先遇到一个矩阵，叫做嵌入矩阵
has a single column for each one of these words
these columns are what determines what vector
它为每个单词都分配了一个独立的列，这些列决定了每个token会被转换成什么向量
each word turns into
in that first step
we label it
跟其它矩阵命名规则一样，我们称这个矩阵为Ve
we
and like all the matrices we see
its values begin random
but they're going to be Learned based on data
它的初始值是随机的，然后在训练过程中基于训练数据进行调整
turning words into vectors
was common practice in machine learning long before
transformers
在transformers之前，就已经有了这种单词转向量的方法
but it's a little weird if you've never seen it before
虽然对于初次接触的人来说可能觉得很奇怪
and it sets the foundation for everything that follows
so let's take a moment to get familiar with it
但它为接下来的一切建立了基础，因此，我们要花些时间来熟悉它
we often call this embedding a word
which invites you to think of these vectors
我们通常称这种转换为词嵌入，这种表述可以让你从几何的角度其理解这些向量
very geometrically
as points in some high dimensional space
visualizing a list of three
把它们想象成高维空间中的点
numbers as coordinates for points in 3d space
将三个数字看作三维空间中的坐标点很简单
would be no problem
but word embeddings tend to be much
much higher dimensional
但词嵌入的维度远超过这个范畴
in GPT3 they have 12
288 dimensions
GPT3的词嵌入维度为12288
and as you'll see
it matters to work in a space
如你所见
that has a lot of distinct directions
in the same way that you could take a 2 dimensional
选择一个高维空间是很重要的，就像你可以在3维空间选择一个2维切片
slice through a 3d space
and project all the points onto that slice
然后将所有点投影到这个切片上
for the sake of animating word embeddings
that a simple model is giving me
为了动态展示词嵌入向量
I'm gonna do an analogous thing
by choosing a 3 dimensional
slice through this very high dimensional space
我采用了相同的方法，从高维空间选择一个三维切片
and projecting the word vectors down onto that
and displaying the results
然后将高维向量投影到这个三维空间进行展示
the big idea here
is that as a model tweaks and
tunes its weights to determine how
这里的关键思想是，模型在训练过程中微调权重
exactly words get embedded as vectors during training
以确定词如何被嵌入为向量
it tends to settle on a set of embeddings where
directions in the space
它会倾向于找到一组嵌入，使得这个空间中的方向含有特定语义含义
have a kind of semantic meaning
for the simple word to vector model I'm running here
if I run
对于目前我运行的这个简单模型来说
a search for all the words whose embeddings are
closest to that of tower
通过搜索，找到所有与tower相近似的词向量
you'll notice how they all seem to give very similar
towerish vibes你会发现这些词都有着近似的tower元素
and if you want to pull up some
Python and play along at home

this is the specific model that I'm using
to make the animations
这些就是我用python来制作动画的模型
it's not a transformer

but it's enough to illustrate the idea that
虽然不是transformer，但足以说明一个观点
directions in the space can carry semantic meaning
空间中的方向能够表达语义信息
a very classic example of this
一个经典的例子是
is how if you take the difference
between the vectors for woman and man
计算woman和man向量之间的差值
something you would visualize as a little
vector in the space
它们的差也可以被可视化为一个向量
connecting the tip of one to the tip of the other
向量起点和终点分别是这两个词的顶点
it's very similar to the difference between

king and queen
这个差异和queen和king的差异很接近
so let's say you didn't know the word for a female
monarch you could find it by taking king
所以，假设你不知道表示woman queen的向量，
adding this woman man direction
你可以通过把king的向量加上这个差异向量，并搜索最接近这个点的向量找到它
and searching for the embeddings closest to that point
at least kind of
despite this being a classic example
至少理论上是这样
for the model I'm playing with
the true embedding of Queen
虽然这是我正在使用的模型的一个经典例子
is actually a little farther
off than this would suggest
但实际上，真正queen的嵌入实际上比这种方法预想的要远一些
presumably
because the way the queen is used in training data
这可能在训练数据中，女王并不仅仅是king的女性版本
is not merely a feminine version of king
when I played around
family relations seem to illustrate the idea
深入挖掘时，我发现通过家族关系来解释这种现象更为合理
much better
the point is it looks like during training
关键在训练过程中，模型发现采用这种嵌入方式更有利
the model found it advantageous to choose in bettings
such that one
direction in this space encodes gender information
即这个空间中的一个方向能编码性别信息
don't just ride the index
seek to outperform it with Fidelity Active etfs
learn more at Fidelity Com
Active
another
example is that if you take the embedding of Italy
另一个例子是，如果你从意大利的向量表示中减去德国的向量表示
and you subtract the embedding of Germany
and then you add that to the embedding of Hitler
you get something
再加上希特勒的向量表示
very close to the embedding of Mussolini
结果非常接近墨索里尼的向量表示
it's as if the model Learned to associate some
directions with Italianness
这就好像模型学会了将某些方向与意大利的特性相关联
and others with World War

II axis leaders
而将其它方向与二战轴心国的领导人相关联
maybe my favorite example in this vein
我个人最喜欢的一个例子是，在某些模型中
is how in some models
if you take the difference between Germany and Japan
如果你计算德国和日本的向量差值
and you add it to sushi
you end up very close to Broadworst
然后加上寿司的向量，你得到的结果非常接近德国香肠
also in playing this game of finding nearest neighbors
此外，在寻找最近邻的过程中，我还惊喜的发现猫和野兽和怪物距离都很近
I was very pleased to see how close
cat was to both beast and monster

one bit of mathematical intuition
有一个有用的数学概念
that's helpful to have in mind
especially for the next chapter
尤其对接下来的章节非常重要
is how the dot product of two
那就是两个向量的点积
vectors can be thought of as a way to measure
how well they align
可以视为一种衡量它们是否对齐的方法
computationally
dot products involve
从计算的角度看
multiplying all the corresponding
点积涉及逐一对应元素相乘，然后进行求和
components and then adding the results
which is good
这很好，因为我们的很多计算看起来就像是权重求和
since
so much of our computation has to look like weighted
sums geometrically
从几何角度看
the dot product is positive
when vectors point in similar directions
如果两个向量指向相似方向，点积为正
it's zero if they're perpendicular
如果相互垂直，点积为0
and it's negative whenever they point in
opposite directions
如果指向相反方向，点积为负数
for example let's say you were playing with this model
假设你正在使用某个模型
and you hypothesize

that the embedding of cats
cat might represent a sort of plurality
从cats的向量表示减去cat的向量表示，可能会在这个空间中找到复数概念的方向
direction in this space
to test this I'm going to take this vector
and compute its dot product
为了验证这个观点，我将计算某个向量与一些特定单数名词嵌入的点积
against the embeddings of certain
singular nouns

and compare it to the dot
products with the corresponding plural nouns
并将其与相应的复数名词点积进行比较
if you play around with this
you'll notice that the plural ones do indeed
如果你试一试，你会发现复数名词的点积值通常比单数的值更高，表明它们在某种方向上的对齐更为紧密
seem to consistently give higher values than the
singular ones
indicating that they align more with this direction
it's also fun how
if you take this dot product with the
更有趣的是，如果你将这个点积操作应用到一二三这些词汇上时，
embeddings of the words 1
2 3 and so on
they give increasing values
你会发现结果是逐渐增加的
so it's as if we can quantitatively measure
how plural the model finds a given word
就像我们能够量化的衡量模型认为一个词的复数程度
again the specifics for how words get
embedded is Learned using data
再次声明，单词的嵌入方式是通过数据学习得到的
this embedding matrix
whose columns tell us what happens to each word
这个词嵌入矩阵表明了词汇的转换过程
is the first pile of weights in our model
and using the GPT3 numbers
它也是模型的第一批权重，根据GPT3的数据
the vocabulary size specifically is 50
057 and again
词汇量是50257，但要注意
technically this consists not of words per se
but of tokens
实际上它指的不是单词本身，而是tokens
and the embedding dimension is 12 288
multiplying those tells us this consists of about 617
million weights

嵌入的维度是12288，将两者相乘，将得到6.17亿个权重


let's go ahead and add this to a running tally
remembering that by the end
we should count up to 175 billion
将这个数字加入到累计计数中，最后我们应该得到1750亿个权重
the case of transformers
you really want to think of the vectors in this
embedding space
在谈到transformers时，你会想到这些嵌入空间中的向量
as not merely representing individual words
不仅仅代表着单个词汇
for one thing
they also encode information
about the position of that word
还携带了词汇位置信息
which we'll talk about later
这一点稍后会说明
but more importantly
但更关键的是
you should think of them as having the capacity
to soak in context
这些向量能够吸纳并反应语境
a vector
that started its life as the embedding of the word king
for example
举个例子，一个最初代表国王的向量
might progressively get tugged and pulled by various
blocks in this network
在网络的各个环节的作用下，可能会逐渐变化
so that by the end
因此到最后，它指的方向会更具体更微妙
it points in a much more specific and nuanced direction
that somehow encodes that it was a king who
lived in Scotland
以某种方式编码一位生活在苏格兰的国王
and who had achieved his post
after murdering the previous king
在杀死前任国王后取得其职位的国王
and who's being described in shakespearean language
think about your own understanding of a given word
此人的描绘方式充满了莎士比亚式的语言
想想你对某个词汇的理解通常是怎么形成的
the meaning of that word is
clearly informed by the surroundings
那个词的意义很大程度上是由其所处的环境决定的
and sometimes this includes context from a long
distance away
有时甚至包含很远的上下文
so in putting together
a model that has the ability to predict
what word comes next
因此，在构建一个能预测下一个单词的模型时
the goal is to somehow
empower it to incorporate context efficiently
关键目标就是让它能够高效的融合上下文信息
to be clear in that very first step
明确一点，在第一步
when you create
the array of vectors based on the input text
当我们根据输入文本创建向量数组时
each one of those is simply plucked out of the
embedding matrix
每个向量都是直接从嵌入矩阵选取的
so initially
each one can only encode the meaning of a single word
without any input from its surroundings
这意味着，起初，每个向量仅能代表一个单词的含义
but you should
think of the primary goal
of this network that it flows through
而不涉及周边环境的信息
as being to enable
each one of those
vectors to soak up a meaning that's much more
但我们的目标就是让这些向量通过网络的传递
rich and specific
than what mere individual words could represent
使得每一个向量都能获取比单个词更丰富更具体的含义
the network can only process a fixed number of
vectors at a time
这个网络每次只能处理一定数量的向量
known as its context size
for GPT3 it was trained with a context size of 2048
这就是所谓的上下文大小，GPT3的上下文大小是2048
so the data flowing through the network
always looks like this
array of 2048 columns
意味着数据在网络流动时，总是看起来像一串2048列的数组
each of which has 12 000 dimensions
每一列都有12288个维度
this context size
limits how much text the transformer can incorporate
这个上下文大小限制了transformer在预测下一个词时可以纳入的文本量
when it's making a prediction of the next word
this is why long conversations with certain chatbots
这就解释了，为什么如果和一些聊天机器人
like the early versions of ChatGPT
比如，早期的ChatGPT版本
often gave the feeling of the bot kind of
losing the
进行长对话时，你可能会觉得机器人在对话中迷失了方向
thread of conversation as you continued too long
尤其是对话持续过长时
we'll go into the details of attention in due time
but skipping ahead
我们会在适当的时候详细讨论注意力机制的细节
I want to talk
for a minute about what happens at the very end
remember
先让我们简单了解下最终阶段的处理过程
the desired output is a probability distribution
over all tokens that might come next
请记住，最终的目标是产生一个概率分布，预测下一个可能出现的token
for example if the very last word is Professor
and the context includes words like Harry Potter
举个例子，如果最后一个词是教授，上下文中包含了哈利波特等词语
and immediately preceding we see
least favorite teacher
紧接着出现的是最不喜欢的老师
and also if you give me some leeway
如果允许我稍微发挥一下
by letting me pretend that token simply look like
full words then
假设tokens就是完整的单词
a well trained network
that had built up knowledge of Harry Potter
那么，一个经过良好训练，并对哈利波特世界有所了解的模型
would presumably assign a high number
to the word Snape
可能会给斯内普这个词一个较高的权重
this involves two different steps
the first one is to use another matrix
此过程涉及两个不同步骤，首先，使用另一个矩阵
that maps the very last vector in that context
to a list of 50
将上下文中的最后一个向量映射到一个包含50000个值的列表
000 values one for each token in the vocabulary
then there's a function that normalizes this
词表中的每个token都对应一个值
into a probability distribution
it's called Softmax
接着，通过一个函数把这些值转换成概率分布
and we'll talk more about it in just a second
这个函数叫softmax，我们将稍后讨论
but before that
it might seem a little bit weird to only use this
但在此之前，你可能会觉得
last embedding
to make a prediction
仅仅基于最后一个嵌入最决策似乎有些奇怪
when after all in that last step
毕竟在最后一层中，还有其它向量
there are thousands of other vectors in the layer
just sitting there
with their own context rich meanings
每个向量都蕴含着丰富的上下文意义
this has to do with the fact that in the
training process
这时因为在训练过程中
it turns out to be much more efficient
如果利用最后一层的每一个向量来同时预测其后可能出现的内容，被证明是更高效的方法
if you use each one of those vectors in the final layer
to simultaneously
make a prediction for what would come
immediately after it
there's a lot more to be said about training later on
关于训练的更多细节我们稍后还会提到
but I just want to call that out right now
this matrix is called the Unembedding Matrix
现在我先简单指出一点，这个矩阵被称为unembedding矩阵，我们用Wu来表示
and we give it the label Wu
again
like all the weight matrices we see
its entries begin at random
就像我们见过的所有权重矩阵一样，这个矩阵的初始值也是随机的
but they are Learned during the training process
但在训练过程中这些值会被更新
keeping score on our total parameter count
关于参数总数的统计，这个unembedding矩阵为词汇表中的每个词都分配了一行
this unembedding matrix has one row

for each word in the vocabulary
and each row has the same number of elements as the
embedding dimension
每一行的维度与嵌入维度一样，12288
it's very similar to the embedding matrix
这与嵌入矩阵相似，只不过是其逆过程
just with the order swapped
so it adds another 617
因此，他为网络增加了另外7.17亿参数
million parameters to the network
meaning our count so far is a little over a billion
到目前为止，参数总数已经超过10亿
a small but not wholly insignificant fraction
虽然是总参数量的一小部分，但确很重要
of 175 billion that we'll end up with in total
as the very last mini lesson for this chapter
在这章的最后一部分
I want to talk more about the Softmax function
since it makes
我想介绍一下softmax
another appearance for us once we
dive into the attention blocks
因为它在注意力机制中会再次成为焦点
the idea
is that if you want a sequence of numbers to act
as a probability distribution
如果你想让一串数字成为概率分布
say a distribution over all possible next words
例如，预测下一个可能出现的词的概率
then each value has to be between 0 and 1
and you also need all of them to add up to 1
这些数字需要在0-1之间，且总和为1
however if you're playing the deep learning game
where everything you do looks like matrix
但是，如果你正在探索深度学习，你所看到的每一步操作可能看起来都是矩阵与向量的乘法
vector multiplication
the outputs that you get by default
don't abide by this at all
那么你得到的结果可能并不符合这个条件
the values are often negative or much bigger than 1
这些值可能是负数，也可能大于1
and they almost certainly don't add up to one
而且加起来和几乎可以确定不为1

Softmax is the standard way
to turn an arbitrary list of numbers
softmax就是一种标准方法可以把任意一组数字转换为一个概率分布
into a valid distribution
in such a way
that the largest values end up closest to 1
使得最大的值可能接近1
and the smaller values end up very close to 0
最小的值接近0
that's all you really need to know
but if you're curious
了解这一点就够了，但如果你感到好奇
the way that it works
is to first raise
具体的工作原理是：首先对每个数字进行e的指数运算
e to the power of each of the numbers
which means you now have a list of positive values
and then you can take the sum
这样就得到了一组正数，然后取所有正数的和，然后每个数字除以总和
of all those positive values
and divide each term by that sum
which normalizes it into a list that adds up to one
这样就能把它们标准化为一个和为1的概率列表
you'll notice that if one of the numbers in the input
is meaningfully bigger than the rest
你会注意到，如果输入中的某个数值明显大于其它数值
then in the output
the corresponding term dominates the distribution
那么在输出中这个值对应的项在概率分布中起主导作用
so if you were sampling from it
you'd almost certainly just be picking the
maximizing input
几乎可以确定，你在采样时会选取这个最大的输入值
but it's softer than just picking the max
in the sense that when
other values are similarly large
但通常会选择比直接选取最大值更细腻的方法
they also get meaningful weight in the distribution
因为当其它值也接近最大值时，它们在分布中也能获取重要的比重
and everything changes continuously
as you continuously vary the inputs
而且随着你不断地改变输入，一切都会连续地变化
in some situations
like when Chatchipt
在一些情况下，比如，像ChatGPT利用这个分布选择下一个词时
is using this distribution to create a next word
there's room for a little bit of extra fun
可能会为这个函数增加一些额外的趣味
by adding a little extra spice into this function
with a constant t thrown into the denominator of those
exponents we call it the temperature
可以通过在指数的分母里添加一个常量t来实现，我们称之为温度
since it vaguely resembles the role of temperature in
certain thermodynamic equations
因为在某种程度上，与热力学方程中的温度作用相似
and the effect is that when t is larger
you give more weight to the lower values
它的效果是，当t较大时，会使较小的数值获得更多的权重
meaning the distribution is a little bit more uniform
使得分布稍微均匀一些
and if t is smaller
而t很小时
then the bigger values will dominate more aggressively
较大的值会占据主导地位，极端情况下，如果把t设置为0，所有权重都会集中到最大值上
wherein the extreme setting t equal to 0
means all of the weight goes to that maximum value
for example
例如，用GPT-3生成一个故事
I'll have gpt3 generate a story with the C to text
once upon a time there was a
but I'm going to use different temperatures
提示是：“once upon a time there was a
”我会在每次测试中使用不同的温度
in each case temperature zero
means that it
always goes with the most predictable word
温度为0，意味着他总是选择最可能的词
and what you get ends up being
kind of a trite derivative of Goldilocks
而你所得到的结果就变成了一个陈词滥调的金发姑娘的故事
a higher temperature
gives it a chance to choose less likely words
较高的温度给它提供了选择不太可能出现的词机会
but it comes with a risk
in this case the story starts out a bit more originally
但这也伴随着风险，在这个例子中，故事开始较为原创
about a young web artist from South Korea
讲述的是一位韩国年轻艺术家
but it quickly degenerates into nonsense
但很快就变得毫无意义
technically speaking
the API doesn't actually let you pick a temperature
bigger than 2
严格地说，API并不允许你选择大于2的温度
there is no mathematical reason for this
这个限制并没有数学上的根据
it's just an arbitrary constraint imposed
只是一个经验值
I suppose to keep their tool from being seen
generating things that are too nonsensical
目的是防止产生过于荒诞的结果
so if you're curious
the way this animation is actually working
is I'm taking the 20 most
所以如果你感到好奇，这个动画的原理是这样的，我选择了GPT-3生成的可能性最高的前20个词
probable next tokens that GPT3 generates
which seems to be the maximum they'll give me
and then I tweak the probabilities based on an
这看起来是它们能给我们最大的数量
exponent of 15th
as another bit of jargon
in the same way that you might call the components of
the output of this function
probabilities
people often refer to the inputs as logits
or some people say logits some people say logits
I'm going to say logits
so for instance
when you feed in some text
you have all these word
embeddings flow through the network
and you do this final
multiplication with the unembedding matrix
machine learning people
would refer to the components in that raw
unnormalized output
as the logits for the next word prediction
a lot of the goal with this chapter
was to lay the foundations for
understanding the attention mechanism
Karate Kid wax on wax off style
you see
if you have a strong intuition for word embeddings
for Softmax for how dot products measure similarity
and also the underlying premise that
most of the calculations have to look like
matrix multiplication
with matrices full of tunable parameters
then understanding the attention mechanism
this cornerstone piece in the whole modern boom in AI
should be relatively smooth
for that come join me in the next chapter
as I'm publishing this
a draft of that next chapter is available for review
by Patreon supporters
a final version should be up in public in a week or two
it usually depends on
how much I end up changing based on that review
in the meantime
if you want to dive into attention
and if you want to help the channel out a little bit
it's there waiting


国外有个神秘组织叫3blue1brown，起初因为用动画讲解线性代数火遍地球，这次，它们把手伸进了大模型领域，就在刚刚发布了动画讲解GPT的系列视频，本想趁热乎翻译过来分享给大家还能蹭点热度，但仅仅一个下午国内网站就陆续出现了译版，有的甚至没来得及翻译，除了反应内容质量，还可见国内同行是真卷啊！

既然视频抢占不了先机，那就转投文章。

它这个系列的视频主要是通过动画讲解GPT大模型，GPT背后的框架是Tranformer，Transformer的核心是注意力和MLP。
所以它也是先从宏观概览Transformer的大致执行流程，然后掰开去详解每一部分。



**20240419**

单头注意力机制
  ​Query：查询向量
  ​key：键向量
  并行训练
  掩码
  ​​Value：值向量

多头注意力机制

├── 摘要
│   ├── 最新进展介绍
│   ├── 模型架构和训练流程讨论
│   └── 性能评估和未来方向
├── 引言
│   ├── 多模态预训练研究进展
│   └── 利用LLMs增强多模态任务
├── 模型架构
│   ├── 模态编码器
│   ├── 输入投影器
│   ├── LLM骨干
│   ├── 输出投影器
│   └── 模态生成器
├── 训练流程
│   ├── 多模态预训练 (MM PT)
│   └── 多模态指令调优 (MM IT)
├── 现状MM-LLMs
│   ├── 122个模型分类总结
│   └── 发展趋势分析
├── 性能评估
│   ├── 基准测试性能比较
│   └── 训练方法提供
├── 未来方向
│   └── MM-LLMs研究前景方向
├── 结论
│   └── 总结和持续进步强调
└── 限制
    ├── 研究局限性讨论
    └── 改进方向展望


    **20240422**
    https://twitter.com/hey_madni/status/1781718226923290767
    ScreenAI：AI大模型带来人机界面交互新方式

    打孔卡
    命令行
    操作界面UI
    AR/VR/MR
    AI

先简单了解下人机交互发展历程，这个“机”指的是手机或者电脑。

打孔卡

命令行（CLI）

图形用户界面（GUI）

AR/VR/MR

AI
可以通过自然语言去控制UI。
像Siri，小爱同学等语音助手也能通过自然语言交互，但它们一般不能帮你识别UI中的元素，只能打开其它应用，或者搜索互联网；与今天的ScreenAI不仅能够理解UI上的元素，还具有推理能力，并且能根据用户指令与UI进行交互。

ScreenAI是一个基于PALI的视觉语言模型。由一个多模态编码器和一个自回归解码器组成，编码器用于编码视觉和文本输入，解码器则能够输出文本。

根据用户指令不同，输出可以是：
导航指令
总结
标注信息
问答



**20240422**
金三银四面试季:让AI帮你写简历
https://airesume.com/
又到了一年一度的面试季，在准备面试的过程中，写简历是一个必须且痛苦的过程。
个人能力虽然重要，但简历是成功的敲门砖。

今天介绍如何用AI帮你写简历。

第一印象很重要，所以需要选择一个好的模板。确定格式和布局。








微调模型最简单的方法仍然是使用像Huggingface这样的框架


https://www.pecan.ai/
大语言模型擅长自然语言理解与生成，机器学习模型擅长从数据中发掘潜在模式。

第一步，会有一个大模型助手与你交谈，了解你的需求，目前pecan中支持二分类，多分类，线性回归，pecan会根据你的需求自动选择分类模型还是回归模型。

第二步，pecan要求你上传数据，然后基于上一步的需求描述，确定模型的输入输出变量。

第三步，pecan基于前面的需求描述和数据，生成一个notebook，里面是对数据进行预处理的SQL语言，会对数据进行补全，清理，相关性分析等等，最终生成训练数据。

第四步，获得最终预测，训练多个模型，然后选择性能最好的。

这个过程中数据预处理，特征工程，模型选择都是几乎是通过自然语言完成的，而不需要过多的数据科学和机器学习经验。

当然不是为了介绍Pecan这款软件，而是让大家了解大模型和传统机器学习融合的新范式。
另外，也是给大家传递一个信息，大模型真的在重塑各行各业。





**20240425**
Tokenization可视化
https://huggingface.co/spaces/Xenova/the-tokenizer-playground



**20240428**
Github Trending
OpenVoice



**20240429**
https://aistudio.google.com/
体验Gemini 1.5 Pro
可视化微调Gemini 1.0


**20240430**
人类简史说，国家，金钱，法律都是人类想象力的产物，通过语言去表达，通过其它手段去维护。

想象和实际存在的区别，标致汽车，公司

语言，会讲故事

大模型，讲的更好


**20240506**

医生越老越吃香，医生需要经验，经验不是单纯的背书，更多的是从长年累月病例中的积累。

这就要求模型不仅要从海量的医疗数据中学习医疗知识，还要有结合患者信息进行推理的能力。还能获取最新的知识。

在医疗领域讲究望闻问切，例如，能引起头痛的原因很多，感冒，鼻炎，肿瘤等等。这就需要医生结合其它一些辅助检查。

这需要模型有多模态能力。

而在医生问诊过程中，往往需要与患者进行多轮交互，这就需要模型具备长生下文。

Gemini 1.0​本身就已经具备多模态能力，Gemini 1.5在1.0的基础上扩充了上下文长度，Med Gemini就是在Gemini的基础上强化医疗能力。

Med Gemini目前还没有开放接口，但其论文中的测试数据表明，在推理，多模态，长上下文方面，在14个医疗数据性能测试上，10个达到最佳，所有测试均超越GPT4-V。

接下来看看Med Gemini是如何从Gemini 1.5进化成医疗小能手的。


**通过自训练和集成网络搜索来提高推理能力**


临床推理是一个迭代过程，最初结合临床知识和患者信息形成初始案例表示，该案例表示指导获取其它信息，直到可以做出决策。

为了保证时效性，需要结合搜索内容RAG


指令微调能提升推理能力

MedQA缺少推理过程中专家推演步骤->“Chain-of-Thoughts”CoTs





在AI开发过程中，框架确实为我们做了很多，但这有时也成了一种幸福的烦恼。

卷积是如何实现的？
计算是如何并行化的？
主机和GPU是如何传输数据的？
反向传播怎么计算梯度？

我们对这些细节知之甚少。


AlexNet，是神经网络开山之作。

作为第一个吃螃蟹的人，它可没那么幸运，在2012年，没有任何成熟的深度学习框架可用于训练或者推理。

一切只能从零开始，论文的作者之一Alex使用CUDA/C++手撸全部代码。

后续Pytorch，Tensorflow也是大致如此。

这为我们学习背后的原理提供了一个捷径。


https://code.google.com/archive/p/cuda-convnet/

https://github.com/ulrichstern/cuda-convnet


项目主要由Python和C++代码组成，通过Python/C API实现C代码操作Python对象

主要功能是C/C++实现的，C/C++代码中调用CUDA接口实现并行计算。Python代码组织数据结构



通过配置文件构造网络






AlexNet赢得2012年ImageNet挑战赛后，辛顿和AlexNet另两位作者立刻成立了DNNResearch，并将其以4400万美元的价格拍卖给谷歌。

深度学习和神经网络也开始受到关注。

全世界公司开始加入到这场科技竞赛中。




**20240507**
假若我用强化学习训练一个游戏代理，谁可与我一战？



**20240508**
youtube视频下载
https://ssyoutube.com/en173vB/youtube-video-downloader

百度数字人
https://qianfanmarket.baidu.com/product/detail/bd88c402-b217-4950-90eb-be5bb628eb54?track=cloudclouddaohang


**20240509**
Google DeepMind releases AlphaFold 3 

AlphaFold 3 predicts the structure and interactions of all of life’s molecules


for (int i = 0; i < infiles.size(); i++)
{
		//step1 解析sur文件，读取图像区域大小
		std::ifstream isurreader;
		isurreader.open(infiles[i], std::ios_base::binary);
		if (!isurreader.is_open())
		{
			isurreader.close();
			return false;
		}

		isurreader.read(header.get(), headersize);
		int width = *((int*)header.get());//图像宽
		int height = *((int*)(header.get() + 4));//图像高
		fovx = *((float*)(header.get() + 0x1BC));//FOVX
		fovy = *((float*)(header.get() + 0x1C0));//FOVY

		heights[i] = height;
		widths[i] = width;
		sizes[i] = Size(width, height);
		std::cout << "Size:" << sizes[i].width << " " << sizes[i].height << std::endl;
		pixeltommY = fovy / height;
		pixeltommX = fovx / width;
		//step2 获取图像数据区
		size_t img_length = (size_t)width * height * sizeof(unsigned short);
		std::shared_ptr<char> img(new char[img_length]);
		isurreader.seekg(headersize);
		isurreader.read(img.get(), img_length);
		unsigned short* selImg = (unsigned short*)img.get();

		float min = (2 * wls[i] - wws[i]) / 2.0f + 0.5f;
		float max = (2 * wls[i] + wws[i]) / 2.0f + 0.5f;
		float r = 4095.0f / (max - min);

		for (int i = 0; i < width * height; i++)
		{
			float pixel_float = ((float)selImg[i] - min) * r;
			if (pixel_float < 0) pixel_float = 0;
			if (pixel_float > 4095) pixel_float = 4095;

			unsigned short pixel_ushort = (unsigned short)pixel_float;
			selImg[i] = pixel_ushort;
		}

		cv::Mat orgmat(height, width, CV_16SC1, selImg);

		cv::Mat Channels[3] = { orgmat,orgmat,orgmat };
		cv::UMat color = UMat::zeros(Size(width, height), CV_16SC3);
		cv::merge(Channels, 3, color);
		mats[i] = color;//第i幅融合输入图像

		corners[i] = Point(leftups[2*i], leftups[2*i+1]);//第i幅融合输入图像左上角坐标
		std::cout << "Corner:" << corners[i].x << " " << corners[i].y << std::endl;

		cv::Size size = orgmat.size();
		masks[i] = cv::UMat(size, CV_8UC1, cv::Scalar::all(255));
		isurreader.close();
	}

	//判断图像大小是否一致
	std::set<int> withset(widths.begin(), widths.end());
	std::set<int> heightset(heights.begin(), heights.end());
	//需要对图像进行缩放
	if (withset.size() != 1 || heightset.size() != 1)
	{
		std::cout << "need to resize image"  << std::endl;

		//找到最大的图像
		auto max_itr = std::max_element(widths.begin(), widths.end());
		int max_index = std::distance(widths.begin(), max_itr);
		int max_width = widths[max_index];
		int max_height = heights[max_index];

		for (int i = 0; i < infiles.size(); i++)
		{
			if (widths[i] == max_width)
				continue;

			//目标图像宽高
			int new_width = max_width;
			int new_height = max_height;
			std::cout << "new_width:" << new_width << std::endl;
			std::cout << "new_height:" << new_height<< std::endl;

			resize(mats[i], mats[i], Size(new_width, new_height), INTER_LINEAR);
			resize(masks[i], masks[i], Size(new_width, new_height), INTER_LINEAR);

			widths[i] = new_width;
			heights[i] = new_height;

			sizes[i] = Size(new_width, new_height);
			pixeltommY = fovy / new_height;
			pixeltommX = fovx / new_width;

		}

	}



  **2240511**
  how neuralink works
  https://www.youtube.com/watch?v=K9rXjAfdrcU


  AI一键换衣
  https://idm-vton.github.io
  https://huggingface.co/spaces/yisol/IDM-VTON
  ​https://github.com/yisol/IDM-VTON



  https://twitter.com/cocktailpeanut/status/1788936103744946564

  造血过程
  https://www.youtube.com/watch?v=_zJzTMuemBI


根据代码画类图
https://www.smartdraw.com/


**20240514**

GPT2组织训练数据的详细细节
https://github.com/AIDajiangtang/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/ch02.ipynb


**20240514**
Model Expoler
https://research.google/blog/model-explorer/


前后模型可能并不具有这么强的因果关系，我们只是想强调，模型正朝着一个大方向发展


今天推荐的资料是由DataBricks发布的电子书：The Path to Deploying Production-Quality GenAI Applications

它为学习LLM的人提供了阶梯式的通关策略，无论你是初学者还是有一定经验的，都能找到自己的Stage。


DataBricks是一家专业处理数据的公司，可以看作是Spark的商业版，在数据处理方面具有优势，能够为训练大模型提供高质量的数据，而数据是Scale Law三大要素之一。

Stage 0：
作为一个初学者，为了避免被复杂的技术原理劝退，可以先用起来，第一步是要选择一个好的模型，一个好的模型具有强大的推理能力，能够跟随用户指令，能够举一反三。

作为一家商业性质的公司，在书中免不了要介绍自家的产品，没错，这就是DBRX，一个开源的大语言模型。



Stage 1：
尝鲜过后就会进入到一个平淡期，如果想重燃激情，就需要一种动力，这种动力来源于通过你的努力带来模型的性能提升。但此时你还没掌握太多的技术原理，也没有资源去做模型微调，如何去获取这种满足感？答案是Prompt Engineering，这是一种开箱即用也不需要付出资源就能提升模型性能的方法。

Prompt作为人类与LLM交互的唯一方式，通过设计合理的Prompt，不仅能避免模型的幻觉问题，还能充分挖掘LLM的涌现能力。


Stage 2：
前面我们学会了如何使用LLM，也学会了如何用好LLM，但你会发现LLM的回答并没有与时俱进，这是因为它的训练数据是有保质期的。最近发布的GPT-4o训练数据也是截止到2023年底。这就需要与传统搜索引擎结合使用，先用搜索引擎检索出网页，然后LLM都其进行进一步加工。

当然，RAG不仅仅能解决时效性问题，还能解决数据私有化问题，而LLM与外部组件相结合的方式也引申出一种新模型：AI Agent。

如果通过不断地累算力，数据，模型来提升模型能这条道路走不通的话，Agent也许就是另一条可行道路。


Stage 3：
RAG能让模型回答的更准确，更专业，但本质上讲，RAG也是一种Prompt Engineering，就是将检索的内容填充Prompt使其作为上下文传递给LLM，再利用LLM的总结，推理能力获得专业的回答。

那如何让一个学富五车的LLM变成一个领域专家？答案是微调。也就是通过使用领域数据对LLM进行微调。也就是通过反向传播算法修改模型参数使其拟合新数据。

还有一种比较重要的微调是指令微调，在使用LLM时，你让LLM翻译它就能翻译，你让它总结就能总结，这都是通过指令微调使其就被指令跟随能力，而在传统深度学习模型时代，翻译和总结就得单独训练两个模型了。

其实微调有很多种，总结来说，有的需要修改模型全部或者部分参数，而有的则是修改Prompt。


Stage 4：
微调一般都是基于一个开源的模型，这个模型一般在大规模数据集上进行自监督预训练，通过有监督数据进行指令微调，再通过强化学习让其向人类对齐。

所以一个组织在开源模型时，都会发布两个模型文件，一个只经过预训练过程，另一个则经过微调具备Chat功能。

https://www.bilibili.com/video/BV1Su4y1n7cw/?spm_id_from=333.337.search-card.all.click&vd_source=14011b7fd9822163ee46211772e57565
最近看了何凯明教授去年获得未来科技大奖的视频，对其中问答环节的内容进行了一个总结。

1.选择让你充满好奇和激情的研究方向
2.发论文不是最终目的，而是研究的起点
3.模型可解释性差又如何，性能好照样堪当大用
4.要想让大模型达到科学家水平，光靠累模型和数据还不够，还得让它有动手实验的能力




how to talk to LLM?

https://www.daily.co/blog/how-to-talk-to-an-llm-with-your-voice/


数据可视化工具：
https://github.com/apache/superset?tab=readme-ov-file



**20240517**

Meta Transformer
文字是人造的，图像是自然的


人脑可以同时处理多种模态输入，并且各种模态起到辅助另一种模态理解作用


数据模态不同，按理讲需要用到不同的模型架构。

在多模态大模型中，统一架构指的是一种设计思想，它允许模型使用单一的、一致的架构来处理和学习多种不同类型的数据模态。


架构能够将不同模态原始输入映射到一个**共享表示空间**

并且是一个**端到端**


**Transformer 架构** -》ViT

UnPaired Data


CLIP,ALIGN and Florence collect large collections of image and text pairs and train models to embed image and
language inputs in a joint space using contrastive learning。

CoCa adds an image captioning objective on top of the contrastive loss for improved performance.


**BeiT v3**
For example, the dual-encoder
architecture is used for efficient retrieval, encoder-decoder networks for generation
tasks, and the fusion-encoder architecture for image-text encoding 


**CLIP**
The dual-encoder architecture is effective for retrieval tasks, especially for masses of images and text. Feature vectors of images and text can be pre-computed and stored. the shallow interaction between images and text is not enough to handle complex VL classification tasks. 


The fusion-encoder architecture achieves superior performance on VL classification tasks. But it requires to jointly encode all possible image-text pairs to compute similarity scores for retrieval tasks. The quadratic time complexity leads to a much slower inference speed than the dual-encoder models whose time complexity is linear.


**MOME**
MOME Transformer consists of three modality experts, namely vision expert for image encoding, language expert for text encoding, and vision-language expert for image-text fusion.

GROK is also based on MOME,but these experts is cateoried by 领域。如，math，code，and so on.

The second category models the interaction of images and text using a deep fusion encoder with cross-modal attention.Image-text matching,masked language modeling, word-region/patch alignment, masked region classification and feature regression are widely used to train fusion-encoder-based models. 

Image-Text Contrast Given a batch of N image-text pairs, image-text contrastive learning aims
to predict the matched pairs from N × N possible image-text pairs. There are N2 − N negative
image-text pairs within a training batch.


前面仅用于视觉语言理解，后面开始能够用于生成任务


**BLIP**
However, encoder-based models are less straightforward to directly transfer to text generation tasks (e.g.image captioning), whereas encoder-decoder models have not been successfully adopted for image-text retrieval tasks.


how to handle Noisy Data?

a unified vision-language model which can operate in one of the three functionalities:
功能上的统一，而不是模型上的统一





腾讯混元大模型：
https://hunyuan.tencent.com/bot/chat
混元文生图：
https://huggingface.co/Tencent-Hunyuan/HunyuanDiT



**20240520**
剪映视频翻译功能

剪映破解版 5.5 夸克网盘链接：https://pan.quark.cn/s/24d0b44cb109


**20240521**
但是在ViT之前，CV中使用的是另一种更常用的预训练方法，就是先在ImageNet这种大规模数据上预训练，让模型学习到一些提取基本特征的能力，然后在专用数据上进行微调。


ALBEF
VLMO
BeiT v3
BLIP
CoCa
Flamingo
BLIP-2
IMAGEBIND
Meta-Transformer

为了有效学习文本中统计关联并压缩这些信息，神经网络实际上学到的是产生这些文本的有效表示,这些文本是现实世界的投影


LLM不是简单的预测下一个token


**20240522**

生成模型和判别模型 与图像生成有关系？
猫狗分类有两种方法


Unconditional generation ->p(y)  输入随机噪声
Conditional image generation  ->p(y|x)
text to image
image to image

文本条件，位置条件

更多类型的条件来生成图像:不光是文本，风格修改，局部修改等等,骨架，轮廓

图像修复和图像扩充,内容编辑,添加文字


跟图生文本一样，不是单一模型，而是由多个子模型构成

AE AutoEncoder:压缩，原始数据压缩到隐空间，为什么叫隐空间，高度压缩，人不易理解


how far are we from AGI?

为了有效学习文本中统计关联并压缩这些信息，神经网络实际上学到的是产生这些文本的有效表示,这些文本是现实世界的投影 -illy

LLM不是简单的预测下一个token - hinton

LLM的训练数据是由人类生成的，且质量参差不齐，

仅通过数据量和模型很难实现AGI，而是要为LLM加一个代码解释器，将结果作为反馈，Agent？何凯明

Scale law表明，只要增加数据量和模型尺寸，LLM的能力就能一直提升，具备涌现能力


是否有人真的希望AGI到来？


原有的格局，阶层可能会被打破重建，财富会被重新分配


硅基生命是否会抢占地球？





多模态大模型带来人机交互新方式，之前介绍过谷歌的ScreenAI，AI操作系统怎能少得了微软，今天介绍微软的UFO，此UFO非外星人。A UI-Focused Agent for Windows OS Interaction
思路基本上与ScreenAI一致。
通过自然语言向Agent下达命令
HostAgent根据指令制定规划，切换到指定应用程序
AppAgent在应用程序上执行操作

需要借助GPT4-V的视觉能力以及OCR识别UI元素。
https://arxiv.org/pdf/2402.07939v4
https://github.com/microsoft/UFO




600百万年前，一个母猩猩，生下两个女儿，一个成为了人类的祖先，另一个成为了所有黑猩猩的祖先

在很长一段时间内，智人并不是处于食物链顶端，直到某种基因突变导致其具有想象力

但人不能靠意念传递信息吧，所以就有了语言，语言是想象力的表达方式

微软：UFO


**20240523**
MicroSoft：Copilot+PC
 Easily find and remember what you have seen in your PC with Recall
 generate and refine AI images in near real-time directly on the device using Cocreator
 and bridge language barriers with Live Captions, translating audio from 40+ languages into English

 with AI at the center,
 We have completely reimagined the entirety of the PC

AI PC时代到来了！
微软Surface： Copilot+ PCs
搭载CPU+GPU+Neural Processing Unit (NPU) 集成芯片
云端大模型+本地小模型协同工作
借助多模态大模型视觉理解能力记住过往所有桌面UI，通过语言进行唤醒
还有更加强大的图像生成和语音识别等功能

https://blogs.microsoft.com/blog/2024/05/20/introducing-copilot-pcs/


相信AGI时代能够到来的人，相信语言的力量


不相信AGI时代能够到来的人，则认为训练数据是由人类创造的，且质量参差不齐。


opencv findContours 




**20240524**
技巧上升到指导行动的理论


https://arxiv.org/pdf/2402.13236 
Towards audio language modeling - an overview 
Neural audio codec models  converting continuous audio into discrete
codes, which can be employed to develop audio language models
(LMs). 


GPT-4o背后的语音技术猜测


文字本来就是语音的某种压缩


**20240527**
FeatureBlender：通过设置权重值实现重合部分线性过度



图像直方图匹配
#include <opencv2/opencv.hpp>
#include <iostream>
 
using namespace cv; //opencv的命名空间
using namespace std;
 
void drawHist(Mat& hist, int type, string name);//归一化并绘制直方图函数
//主函数
int main()
{
	Mat img11 = imread("E:/opencv/opencv-4.6.0-vc14_vc15/opencv/123.jpg");
	Mat img22 = imread("E:/opencv/opencv-4.6.0-vc14_vc15/opencv/lenac.png");
 
	Mat img1, img2;
	cvtColor(img11, img1, COLOR_BGR2GRAY);
	cvtColor(img22, img2, COLOR_BGR2GRAY);
 
	if (img1.empty() || img2.empty())
	{
		cout << "请确认图像文件名称是否正确" << endl;
		return -1;
	}
 
	Mat hist1,hist2;//用于存放两张图像的直方图
	//计算两图像直方图
	const int channels[1] = { 0 };
	float inRanges[2] = { 0,255 };
	const float* ranges[1] = { inRanges };
	const int bins[1] = { 256 };
	calcHist(&img1, 1, channels, Mat(), hist1, 1, bins, ranges);//计算原图像直方图
	calcHist(&img2, 1, channels, Mat(), hist2, 1, bins, ranges);//计算均衡化后图像直方图
	//归一化两张图象的直方图
	drawHist(hist1, NORM_INF, "hist1");
	drawHist(hist2, NORM_INF, "hist2");
 
	//计算两张图像直方图的累积概率
	float hist1_cdf[256] = { hist1.at<float>(0) };
	float hist2_cdf[256] = { hist2.at<float>(0) };
	for (int i = 1; i < 256; i++)
	{
		hist1_cdf[i] = hist1_cdf[i - 1] + hist1.at<float>(i);
		hist2_cdf[i] = hist2_cdf[i - 1] + hist2.at<float>(i);
	}
 
	//构建累计误差矩阵
	float diff_cdf[256][256];
	for (int i = 0; i < 256; i++)
	{
		for (int j = 0; j < 256; j++)
		{
			diff_cdf[i][j] = fabs(hist1_cdf[i] - hist2_cdf[j]);
		}
	}
 
	//生成LUT映射表
	Mat lut(1, 256, CV_8U);
	for (int i = 0; i < 256; i++)
	{
		//查找原灰度级为i的映射灰度
		//和i的累积概率差值最小的规定化灰度
		float min = diff_cdf[i][0];
		int index = 0;
		//寻找累积概率误差矩阵中每一行的最小值
		for (int j = 1; j < 256; j++)
		{
			if (min > diff_cdf[i][j])
			{
				min = diff_cdf[i][j];
				index = j;
			}
		}
		lut.at<uchar>(i) = (uchar)index;
	}
	
	Mat result, hist3;
	LUT(img1, lut, result);
	imshow("待匹配图像", img1);
	imshow("匹配的模板图像", img2);
	imshow("直方图匹配结果", result);
 
	calcHist(&result, 1, channels, Mat(), hist3, 1, bins, ranges); //统计直方图
	drawHist(hist3, NORM_INF, "hist3"); //绘制匹配后的图像直方图
 
 
	waitKey(0);//等待函数用于显示图像
	return 0;
 
}
 
void drawHist(Mat& hist, int type, string name)//归一化并绘制直方图函数(3个参数分别是：直方图统计结果，需要进行归一化的类型，以及显示图像直方图窗口名称）
{
	int hist_w = 512;
	int hist_h = 400;
	int width = 2;
	Mat histImage = Mat::zeros(hist_h, hist_w, CV_8UC3);
	normalize(hist, hist, 1, 0, type, -1, Mat());
	for (int i = 1; i <= hist.rows; i++)
	{
		rectangle(histImage, Point(width * (i - 1), hist_h - 1),
			Point(width * i - 1, hist_h - cvRound(hist_h * hist.at<float>(i - 1)) - 1),
			Scalar(255, 255, 255), -1);
	}
	imshow(name, histImage);
}
 
 

 **20240528**
BlocksCompensator:要求8UC3


前面的图生文模型不再是单一的模型，而是多个模型的组合

**生成式模型和判别式模型**



**自编码器们**

Encoder-decoder和Autoencoders是不同的。

Encoder-decoder：encoder network extracts key features,decoder network takes that extracted feature data as its input, are used in a variety of deep learning models,CNN(分割) RNN(Seq2Seq),输入和输出是不同的，UNet,输入图像，输出像素标签。有标签监督学习。


Autoencoders可以看作是一种特殊的Encoder-decoder架构。通过无监督学习去恢复，重建输入图像。
发现输入数据的隐变量，学习概率分布？

但它与无监督学习还是有点区别的，因为它是将输入作为标签，子监督学习。



在统计学、数据分析和机器学习中，"隐变量"（Latent Variables）指的是那些未被直接观察到，但可以解释可观测数据变量之间关系或模式的变量。

发现隐变量的一些常用技术包括：

因子分析：用于识别影响多个观测变量的潜在因子。
主成分分析（PCA）：一种降维技术，用于发现数据中的主要成分。
隐马尔可夫模型（HMM）：用于时间序列数据中，识别隐藏的状态序列。
潜在语义分析（LSA）：一种文本分析技术，用于发现文本数据中的潜在语义结构。
自编码器：一种神经网络，用于学习数据的压缩表示，可以揭示隐变量。
生成对抗网络（GAN）：在生成模型中，隐变量空间可以被用来生成新的数据实例。





自编码器变体为了适应特定任务，可能会稍有不同，但都有如下关键组件：

组成：
编码器：压缩，提取能够让解码器完美重构原始输入的信息
编码：编码器的输出，解码器的输入，输入的压缩表示,维度越小越好
解码器：解压缩

编码器和解码器可以采用不同的架构，例如，CNN，RNN，ANN；编码器和解码器是对偶的

隐藏层节点数量要小于输入输出维度，这样可以防止模型过拟合，防止模型直接学习到一个identity function，将输入拷贝到输出


降维
有的时候解码器会在训练完后被弃之不用,它的唯一目的是为了训练编码器，这个和GAN中的判别器作用类似。
训练完成后编码器可以作为另一个网络的一部分，CLIP中的图像编码器？但有的时候也会用于生成新样本，例如，VAEs


自编码器的变体
去噪自编码器
编码器的输入是在原始输入上加上高斯噪声，解码器的输出和原始输入作对比

变分自编码器
是一种生成式编码器，它学习数据压缩表示的概率分布，它学习的是一个数据的连续隐空间模型，其它是判别式编码器，它们学习的是离散的隐空间模型。

它将数据压缩表示看作是一种概率分布？然后去生成新样本

The fundamental difference between VAEs and other types of autoencoders is that while most autoencoders learn discrete latent space models, VAEs learn continuous latent variable models. Rather than a single encoding vector for latent space, VAEs model two different vectors: a vector of means, “μ,” and a vector of standard deviations, “σ.” Because these vectors capture latent attributes as a probability distribution—that is, they learn a stochastic encoding rather than a deterministic encoding—VAEs allow for interpolation and random sampling, greatly expanding their capabilities and use cases. This means that VAEs are generative AI models.




To generate a new sample, the VAE samples a random latent vector (ε) from within the unit Gaussian

https://www.ibm.com/topics/autoencoder



**GAN**

openAI的DALL-E使用了GAN