---
published: false
layout: post
title: "CLIP论文精读"
categories: 我的AI新书
date: 2023-09-21 00:00:00 +0800
excerpt: "多模态篇-CLIP"
---


**CLIP论文精读**
https://www.bilibili.com/video/BV1SL4y1s7LQ/?spm_id_from=333.999.0.0&vd_source=14011b7fd9822163ee46211772e57565

CLIP没有使用明确的标签，实际上采用了一个“句子-图片”配对的方式为模型提供了一定的监督。这解决了需要绝对标签进行训练的限制，增强了模型的泛化性能。

是不是可以理解为一条句子是图片标签的升级版，除了对图中主体的标识外，提供了一个更加完整的语义信息。模型也确实达到了非常好的性能，说明语言作为指导视觉数据训练的想法是非常好的，但是收集制作一个如此规模的高质量地语言图片配对数据集是不是比简单的标签标注更加的费力呢？

文章的本意之一是想达到像NLP领域中，可以随便拿来语言，不用过多处理和标注就能训练的目的，但是收集到大量的、高质量的句子和图片配对的数据这个事情这是不是与上述的出发点相反呢？

Answer：是的，一个句子可以理解成单个标签的升级版。至于label cost，比起直接让人工去label（比如让人看图说话），直接从网上爬还是轻松很多的。但如果为了保证高质量，每个图片文本对都有人去校对的话，那这个cost还真有可能比简单的分类标注要费力。至于CLIP的出发点，他还是想卖图片与文本结合的威力，比如强大的zero shot transfer，并不是卖无监督，所以可能不算相反吧。

好的，我来给你举个具体的例子，假设我们有这样的一批数据：

图像	文本
🐶	这是一只可爱的小狗
🐱	这是一只漂亮的小猫
🐭	这是一只聪明的小老鼠
🐰	这是一只温柔的小兔子
我们用image_encoder和text_encoder分别对图像和文本进行编码，得到它们的特征表示，然后用W_i和W_t分别对它们进行投影，得到它们的嵌入表示，假设嵌入空间的维度是2，那么我们可以得到这样的结果：

图像	图像嵌入	文本	文本嵌入
🐶	[0.8, 0.6]	这是一只可爱的小狗	[0.7, 0.7]
🐱	[0.6, 0.8]	这是一只漂亮的小猫	[0.7, 0.7]
🐭	[-0.8, -0.6]	这是一只聪明的小老鼠	[-0.7, -0.7]
🐰	[-0.6, -0.8]	这是一只温柔的小兔子	[-0.7, -0.7]
我们可以看到，每个图像和对应的文本的嵌入表示都比较接近，而与其他的嵌入表示都比较远，这说明模型已经学会了将相似的图像和文本映射到相近的空间位置。

接下来，我们用温度参数t对嵌入表示进行缩放，然后计算它们之间的余弦相似度，得到一个[n, n]的矩阵，假设t=1，那么我们可以得到这样的结果：

logits	🐶	🐱	🐭	🐰
🐶	1.0	0.96	-0.96	-1.0
🐱	0.96	1.0	-1.0	-0.96
🐭	-0.96	-1.0	1.0	0.96
🐰	-1.0	-0.96	0.96	1.0
我们可以看到，每个图像和对应的文本的相似度都是最高的，而与其他的相似度都比较低，这说明模型已经学会了区分不同的图像和文本。

最后，我们用对称的损失函数，优化模型的参数，具体来说，就是对每一行和每一列的logits进行softmax，然后与labels进行比较，计算交叉熵损失，然后求平均，得到图像和文本的损失，再求平均，得到对称的损失，例如：

labels	🐶	🐱	🐭	🐰
🐶	1	0	0	0
🐱	0	1	0	0
🐭	0	0	1	0
🐰  0	0	0	1