---
published: false
layout: post
title: "预告"
categories: 我的AI新书
date: 2023-09-21 00:00:00 +0800
excerpt: "预告"
---


ResNet为什么能训练出1000层的模型?



如何避免梯度消失？



将乘法运算变成加法运算（ResNet就是这么做的，特别是残差连接（Residual Connection））




Residual如何处理梯度消失？



假设有一个预测模型：y = f(x)

x：输入
f：表示神经网络模型
y：输出





w：权重



蓝色部分：

表示原有模型某一层的 w 的更新计算（输出 y 中省略了损失函数）
η：学习率
y 对 w 的梯度不能太小，如果太小的话，η 无论多大都不会起作用，并且也会影响数值的稳定性


紫色部分：

y‘ = f(x) + g( f(x) ) 表示使用堆叠的方式对原有的模型进行加深之后的模型

后面的部分表示 y' 对w的梯度,，经过链式法则展开之后：第二项 y‘ 关于 w 的梯度和之前蓝色部分的结果是一样的，没有任何变化；第一项 g(y) 关于 y 的梯度是新加的层的输出对输入的导数，它和预测值与真实值之间的差别有关系，假设预测的值和真实值之间的差别比较小的话，第一项的值就会变得特别小（假设所加的层的拟合能力比较强，第一项就会变得特别小，在这种情况下，和第二项相乘之后，乘积的值就会变得特别小，也就是梯度就会变得特别小，就只能增大学习率，但可能增大也不是很有用，因为这是靠近底部数据层的更新，如果增加得太大，很有可能新加的层中的w就已经很大了，这样的话可能会导致数值不稳定）
正是因为乘法的存在，所以如果中间有一项比较小的话，可能就会导致整个式子的乘积比较小，越到底层的话乘积就越小


绿色部分：

y‘' = f(x) + g( f(x) ) 表示使用残差连接的方式对原有的模型进行加深之后的模型输出
使用加法的求导对模型表达式进行展开得到两项，第一项和前面所说的一样，就是蓝色的部分
对于这两项来说，就算第二项的值比较小，但还是有第一项的值进行补充（大数加上一个小数还是一个大数，但是大数乘以一个小数就可能变成小数），正是由于跨层数据通路的存在，模型底层的权重相比于模型加深之前不会有大幅度的缩小


靠近数据端的权重 w 难以训练，但是由于加入了跨层数据通路，所以在计算梯度的时候，上层的loss可以通过跨层连接通路直接快速地传递给下层，所以在一开始，下面的层也能够拿到比较大的梯度



从梯度大小的角度来解释，residual connection 使得靠近数据的层的权重 w 也能够获得比较大的梯度，因此，不管网络有多深，下面的层都是可以拿到足够大的梯度，使得网络能够比较高效地更新









Q&A



1、学习率可不可以使靠近输出的小一点，靠近输入的大一点，这样会不会就可以解决梯度消失的问题？﻿
QA P2 - 00:00
﻿


2、为什么深层的网络，底层比较难训练？是因为它拿到的梯度一般比较小吗？﻿
QA P2 - 01:15
﻿








----end---- 作者：如果我是泡橘子 https://www.bilibili.com/read/cv15472735/?from=readlist&jump_opus=1 出处：bilibili