---
published: false
layout: post
title: "字节对编码（Byte-Pair Encoding，BPE）：基于子词的分词算法"
categories: 我的AI新书
date: 2023-09-21 00:00:00 +0800
excerpt: "字节对编码（Byte-Pair Encoding，BPE）：基于子词的分词算法"
---



**像ChatGPT这种大语言模型的训练资料相当于人类多少阅读量？**

这是一个很有趣的问题，但是没有一个确定的答案。不同的大语言模型（LLM）可能使用不同的数据集来训练，而且数据集的大小和质量也会影响模型的性能。一般来说，LLM的训练资料是由海量的文本组成的，比如网页、新闻、书籍、社交媒体等。这些文本的总量可能达到数百亿或数万亿个字，相当于人类阅读数千本或数十万本书籍的量级。1

ChatGPT是基于GPT-3.5和GPT-4这两个LLM的聊天机器人，它们分别有1750亿和5300亿个参数。2 参数的数量可以反映模型的复杂度和容量，也可以间接反映模型的训练资料的规模。据估计，GPT-3.5使用了45TB的文本数据来训练，相当于人类阅读30万本书籍的量级。3 GPT-4的训练数据规模可能更大，但具体的数字还没有公开。

当然，LLM的训练资料的量级并不完全等同于人类的阅读量，因为LLM和人类的学习方式和目标是不同的。LLM是通过统计和机器学习的方法来学习文本中的语法、语义、知识和逻辑的，而人类是通过理解、思考、记忆和创造的过程来学习语言和世界的。LLM可能需要更多的数据来捕捉文本中的细微和隐含的信息，而人类可能可以通过更少的数据来获得更深刻和更广泛的理解。


**谷歌大模型进度为什么会落后**

大公司内部无法形成统一认识，相互掣肘
用户对初创公司和大公司的容忍度不一样
传统搜索和大模型相互冲突