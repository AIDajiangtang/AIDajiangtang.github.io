---
published: false
layout: post
title: "对比学习常见问题"
categories: 我的AI新书
date: 2023-09-21 00:00:00 +0800
excerpt: "对比学习常见问题"
---
GBDT预备知识
0.参考文献和思维导图
1.预备知识
1.1 决策树CART算法
1.1.1 与C4.5等的算法区别
1.1.2 基尼指数
1.1.3 基尼指数例题
1.1.4 分类树
1.1.5 回归树
1.1.6 CART的剪枝
0.参考文献和思维导图
参考连接：GBDTt 作者：老弓的学习日记
思维导图：https://github.com/QianJoe/Ensemble-Learning
1.预备知识
1.1 决策树CART算法
CART算法包括：选择特征、生成决策树、剪枝。

1.1.1 与C4.5等的算法区别
区别1：C4.5等算法一般都是多叉树，而CART是二叉树。

如图C4.5生成的决策树：

多叉树怎么转换为二叉树：

例如：对于纹理这个特征而言，可以分为清晰、稍模糊、模糊三叉树，但是如果把这个三叉树改成二叉树，就可以写成清晰和不清晰，接着在「不清晰」中，再分为模糊和稍模糊。

区别2：选择特征的依据不同
要想生成一棵决策树，首先应该选择最优特征。在CART算法中，是通过基尼指数来选择最优特征的，ID3和C4.5是使用信息增益和信息增益比。

1.1.2 基尼指数
假设有K KK个类，样本点属于第k kk类的概率为p k p_kp 
k
​
 ，概率分布的基尼指数定义为：
G i n i ( p ) = ∑ k = 1 K p k ( 1 − p k ) = 1 − ∑ k = 1 K p k 2 Gini(p)=\sum_{k=1}^Kp_k(1-p_k)=1-\sum_{k=1}^Kp_k^2
Gini(p)= 
k=1
∑
K
​
 p 
k
​
 (1−p 
k
​
 )=1− 
k=1
∑
K
​
 p 
k
2
​
 

显然，这就是样本点被错分的概率期望。如果整个样本集只有一个类别，那么基尼指数就是0，表示样本集纯度达到最高值。反正总共就一个类，那么任意抽取一个样本，自然就知道它的归属类别。

对于二类分类问题
如果样本点属于第一类的概率是p pp，不属于的概率就是1 − p 1-p1−p，代入到这个公式里就是：
G i n i ( p ) = p ( 1 − p ) + ( 1 − p ) ( 1 − ( 1 − p ) ) = 2 p ( 1 − p )
Gini(p)
​
  
=p(1−p)+(1−p)(1−(1−p))
=2p(1−p)
​
 

然而实际生活中概率p pp无法知道，只能用估计值，所以有以下：

如果对给定的样本集合D DD，可以分为两个子集C 1 C_1C 
1
​
 和C 2 C_2C 
2
​
 ：
G i n i ( p ) = 1 − ∑ k = 1 2 ( ∣ C k ∣ ∣ D ∣ ) 2 Gini(p)=1-\sum_{k=1}^2\left(\frac{|C_k|}{|D|}\right)^2
Gini(p)=1− 
k=1
∑
2
​
 ( 
∣D∣
∣C 
k
​
 ∣
​
 ) 
2
 
，其中∣ C k ∣ ∣ D ∣ \frac{|C_k|}{|D|} 
∣D∣
∣C 
k
​
 ∣
​
 就是p pp的经验值。

之所以单独把二分类的情况列出来，是因为在提出基尼指数的CART算法中用的就是这个，毕竟CART算法生成的是二叉决策树。但其实基尼指数完全可以用到多分类问题中。

如：对于特征A条件下，样本集D的基尼指数为

G i n i ( D , A ) = ∣ D 1 ∣ ∣ D ∣ G i n i ( D 1 ) + ∣ D 2 ∣ ∣ D ∣ G i n i ( D 2 ) Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)
Gini(D,A)= 
∣D∣
∣D 
1
​
 ∣
​
 Gini(D 
1
​
 )+ 
∣D∣
∣D 
2
​
 ∣
​
 Gini(D 
2
​
 )

这里就是选定了特征A，并且将数据集中按照特征分成了两个数据集，再分别求它们对应的基尼指数。

1.1.3 基尼指数例题
拿水蜜桃来举个例子。一共10 1010个桃子，其中5 55个好吃，5 55个不好吃。
那么可以计算出整个数据集的基尼指数：
G i n i ( D ) = 2 p ( 1 − p ) = 2 ∗ 1 2 ∗ 1 2 = 0.5 Gini(D)=2p(1-p)=2*\frac{1}{2}*\frac{1}{2}=0.5
Gini(D)=2p(1−p)=2∗ 
2
1
​
 ∗ 
2
1
​
 =0.5
，分类：好吃和不好吃，两种。

第一个特征，选择甜度特征
按照甜度阈值ϵ = 0.2 \epsilon=0.2ϵ=0.2分成两组。
假设，甜度大于0.2 0.20.2的有6 66个桃子，其中5 55个好吃，1 11个不好吃，甜度小于等于0.2 0.20.2的有4 44个桃子，都不好吃，那么我们就可以列出这样一个二叉树。数据集就被分成了D 1 D_1D 
1
​
 和D 2 D_2D 
2
​
 两个。这里我们把甜度特征标记为A AA。
如图：

接着来计算甜度特征下的基尼指数：

计算D 1 D_1D 
1
​
 数据集的基尼指数：
G i n i ( D 1 ) = 2 ∗ 5 6 ∗ 1 6 = 10 36 Gini(D_1)=2*\frac{5}{6}*\frac{1}{6}=\frac{10}{36}
Gini(D 
1
​
 )=2∗ 
6
5
​
 ∗ 
6
1
​
 = 
36
10
​
 

接着计算D 1 D_1D 
1
​
 占比的权重为：
w 1 = 6 10 w_1=\frac{6}{10}
w 
1
​
 = 
10
6
​
 

计算D 2 D_2D 
2
​
 数据集的基尼指数：
G i n i ( D 2 ) = 2 ∗ 0 4 ∗ 4 4 = 0 Gini(D_2)=2*\frac{0}{4}*\frac{4}{4}=0
Gini(D 
2
​
 )=2∗ 
4
0
​
 ∗ 
4
4
​
 =0

接着计算D 2 D_2D 
2
​
 占比的权重为：
w 2 = 4 10 w_2=\frac{4}{10}
w 
2
​
 = 
10
4
​
 

计算甜度特征下的基尼指数
G i n i ( D , A ) = 6 10 ∗ 10 36 + 4 10 ∗ 0 = 0.17 Gini(D,A)=\frac{6}{10}*\frac{10}{36}+\frac{4}{10}*0=0.17
Gini(D,A)= 
10
6
​
 ∗ 
36
10
​
 + 
10
4
​
 ∗0=0.17

第二个特征，选择硬度特征
按照软硬分成两组。
假设，有5个硬桃子，其中2个好吃，3个不好吃，5个软桃子中，有3个好吃，2个不好吃。那么继续列出一个二叉树，这里我们把硬度特征标记为B BB。


计算D 1 D_1D 
1
​
 数据集的基尼指数：
G i n i ( D 1 ) = 2 ∗ 2 5 ∗ 3 5 = 12 25 Gini(D_1)=2*\frac{2}{5}*\frac{3}{5}=\frac{12}{25}
Gini(D 
1
​
 )=2∗ 
5
2
​
 ∗ 
5
3
​
 = 
25
12
​
 

接着计算D 1 D_1D 
1
​
 占比的权重为：
w 1 = 5 10 w_1=\frac{5}{10}
w 
1
​
 = 
10
5
​
 

计算D 2 D_2D 
2
​
 数据集的基尼指数：
G i n i ( D 2 ) = 2 ∗ 3 5 ∗ 2 5 = 12 25 Gini(D_2)=2*\frac{3}{5}*\frac{2}{5}=\frac{12}{25}
Gini(D 
2
​
 )=2∗ 
5
3
​
 ∗ 
5
2
​
 = 
25
12
​
 

接着计算D 2 D_2D 
2
​
 占比的权重为：
w 2 = 5 10 w_2=\frac{5}{10}
w 
2
​
 = 
10
5
​
 

计算甜度特征下的基尼指数：
G i n i ( D , B ) = 1 2 ∗ 12 25 + 1 2 ∗ 12 25 = 0.48 Gini(D,B)=\frac{1}{2}*\frac{12}{25}+\frac{1}{2}*\frac{12}{25}=0.48
Gini(D,B)= 
2
1
​
 ∗ 
25
12
​
 + 
2
1
​
 ∗ 
25
12
​
 =0.48

选择特征
通过比较可以看出：
G i n i ( D , A ) < G i n i ( D , B ) Gini(D,A)<Gini(D,B)
Gini(D,A)<Gini(D,B)

按照甜度分类时，分类的确定性更胜一筹，那么就可以用这个特征作为最优特征。
这就是用基尼指数来找到最优特征的方法，通过对数据集中不同特征进行基尼指数的遍历计算，就能得出最小时对应的特征，这就完成了CART算法中的第一步。

1.1.4 分类树
输入：数据集D DD，特征集A AA，停止条件阈值ϵ \epsilonϵ
输出：CART分类决策树

1.算法基本流程

第一步，从根节点出发，构建二叉树
第二步，计算现有特征下对数据集D DD基尼指数，选择最优特征假设特征集A AA中A 1 , A 2 , ⋯   , A n A_1,A_2,\cdots,A_nA 
1
​
 ,A 
2
​
 ,⋯,A 
n
​
 个特征，那么我们先选出A 1 A_1A 
1
​
 特征，假设这个特征里，有a 11 , a 12 , ⋯   , a 1 m 1 a_{11},a_{12},\cdots,a_{1m_1}a 
11
​
 ,a 
12
​
 ,⋯,a 
1m 
1
​
 
​
 个值，那么对数据集D DD按照每一个a 1 i a_{1i}a 
1i
​
 特征值来分成D 1 D_1D 
1
​
 和D 2 D_2D 
2
​
 两个数据集，并且计算一下对应的基尼指数，选择基尼指数最小的那个特征值a 1 i a_{1i}a 
1i
​
 作为最优切分点。
以此类推，得出每个特征下的最优切分点，也就是最优的特征值。接着比较在最优切分下每个特征的基尼指数，选择基尼指数最小的那个特征，就是最优特征。
第三步，根据最优特征和最优切分点，生成两个子节点，并将数据集分配到对应的子节点中。
按照最优切分点来分成二叉树。
第四步，分别对两个子节点继续递归调用上面的步骤，直到满足条件，即生成CART分类决策树。
这里的条件，一般就是阈值 ，当基尼指数小于这个阈值时，意味着样本基本上属于一类，或者就是没有更多的特征了，那么就完成了CART分类决策树的生成。

分类树的例题
训练集D DD，特征集分别是A 1 A_1A 
1
​
 年龄，A 2 A_2A 
2
​
 是否有工作，A 3 A_3A 
3
​
 是否有自己的房子，A 4 A_4A 
4
​
 信贷情况。
类别为y 1 = 是 y_1=是y 
1
​
 =是，y 2 = 否 y_2=否y 
2
​
 =否

贷款申请样本数据表：

ID	年龄	有工作	有自己的房子	信贷情况	类别
1	青年	否	否	一般	否
2	青年	否	否	好	否
3	青年	是	否	好	是
4	青年	是	是	一般	是
5	青年	否	否	一般	否
6	中年	否	否	一般	否
7	中年	否	否	好	否
8	中年	是	是	好	是
9	中年	否	是	非常好	是
10	中年	否	是	非常好	是
11	老年	否	是	非常好	是
12	老年	否	是	好	是
13	老年	是	否	好	是
14	老年	是	否	非常好	是
15	老年	否	否	一般	否
回顾一下公式：
对于特征A条件下，样本集D的基尼指数为：
G i n i ( D , A ) = ∣ D 1 ∣ ∣ D ∣ G i n i ( D 1 ) + ∣ D 2 ∣ ∣ D ∣ G i n i ( D 2 ) Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)
Gini(D,A)= 
∣D∣
∣D 
1
​
 ∣
​
 Gini(D 
1
​
 )+ 
∣D∣
∣D 
2
​
 ∣
​
 Gini(D 
2
​
 )

第一个 选择年龄A 1 A_1A 
1
​
 这个特征
年龄特征中有：青年A 11 A_{11}A 
11
​
 ，中年A 12 A_{12}A 
12
​
 和老年A 1 3 A_13A 
1
​
 3三个特征值。

D|A1	年龄	个数	是否同意贷款
否	是
D1	青年	5	3	2
D2	中年	5	2	3
D3	老年	5	1	4
1.以青年和非青年分类：

因为CART算法是二叉树，将数据集分为青年和非青年（也就是中年和老年）。
那么，青年D 1 D_1D 
1
​
 的基尼数和权重：
G i n i ( D 1 ) = 2 ∗ 2 5 ∗ 3 5 = 12 25 w 1 = 5 15 Gini(D_1)=2*\frac{2}{5}*\frac{3}{5}=\frac{12}{25}\\w_1=\frac{5}{15}
Gini(D 
1
​
 )=2∗ 
5
2
​
 ∗ 
5
3
​
 = 
25
12
​
 
w 
1
​
 = 
15
5
​
 

非青年D 2 D_2D 
2
​
 的基尼数和权重：
G i n i ( D 2 ) = 2 ∗ 7 10 ∗ 3 10 = 42 100 w 2 = 10 15 Gini(D_2)=2*\frac{7}{10}*\frac{3}{10}=\frac{42}{100}\\w_2=\frac{10}{15}
Gini(D 
2
​
 )=2∗ 
10
7
​
 ∗ 
10
3
​
 = 
100
42
​
 
w 
2
​
 = 
15
10
​
 

G i n i ( D , A 11 ) = 5 15 ∗ 12 25 + 10 15 ∗ 42 100 = 0.44 Gini(D,A_{11})=\frac{5}{15} *\frac{12}{25}+\frac{10}{15}*\frac{42}{100}=0.44
Gini(D,A 
11
​
 )= 
15
5
​
 ∗ 
25
12
​
 + 
15
10
​
 ∗ 
100
42
​
 =0.44

同理，还可以以中年、老年来作为分类点。
2.以中年和非中年分类：

G i n i ( D , A 12 ) = 5 15 ∗ 2 ∗ 3 5 ∗ 2 5 + 10 15 ∗ 2 ∗ 6 10 ∗ 4 10 = 0.48 Gini(D,A_{12})=\frac{5}{15} *2*\frac{3}{5}*\frac{2}{5}+\frac{10}{15}*2*\frac{6}{10}*\frac{4}{10}=0.48
Gini(D,A 
12
​
 )= 
15
5
​
 ∗2∗ 
5
3
​
 ∗ 
5
2
​
 + 
15
10
​
 ∗2∗ 
10
6
​
 ∗ 
10
4
​
 =0.48

3.以老年和非老年分类：

G i n i ( D , A 13 ) = 5 15 ∗ 2 ∗ 4 5 ∗ 1 5 + 10 15 ∗ 2 ∗ 5 10 ∗ 5 10 = 0.44 Gini(D,A_{13})=\frac{5}{15} *2*\frac{4}{5}*\frac{1}{5}+\frac{10}{15}*2*\frac{5}{10}*\frac{5}{10}=0.44
Gini(D,A 
13
​
 )= 
15
5
​
 ∗2∗ 
5
4
​
 ∗ 
5
1
​
 + 
15
10
​
 ∗2∗ 
10
5
​
 ∗ 
10
5
​
 =0.44

可以看出，青年和老年的基尼指数最小，都可以作为最优划分点。
第二个 选择工作A 2 A_2A 
2
​
 这个特征
工作特征中有：有工作A 21 A_{21}A 
21
​
 、无工作A 22 A_{22}A 
22
​
  2个特征值。

D|A2	有工作	个数	是否同意贷款
否	是
D1	是	5	0	5
D2	否	10	6	4
那么，有工作D 1 D_1D 
1
​
 的基尼数和权重：
G i n i ( D 1 ) = 2 ∗ 0 5 ∗ 5 5 = 0 w 1 = 5 15 Gini(D_1)=2*\frac{0}{5}*\frac{5}{5}=0 \\ w_1=\frac{5}{15}
Gini(D 
1
​
 )=2∗ 
5
0
​
 ∗ 
5
5
​
 =0
w 
1
​
 = 
15
5
​
 

有工作D 2 D_2D 
2
​
 的基尼数和权重：
G i n i ( D 2 ) = 2 ∗ 4 10 ∗ 6 10 = 48 100 w 2 = 10 15 Gini(D_2)=2*\frac{4}{10}*\frac{6}{10}=\frac{48}{100}\\w_2=\frac{10}{15}
Gini(D 
2
​
 )=2∗ 
10
4
​
 ∗ 
10
6
​
 = 
100
48
​
 
w 
2
​
 = 
15
10
​
 

G i n i ( D , A 2 ) = 5 15 ∗ 0 + 10 15 ∗ 48 100 = 0.32 Gini(D,A_{2})=\frac{5}{15} *0+\frac{10}{15}*\frac{48}{100}=0.32
Gini(D,A 
2
​
 )= 
15
5
​
 ∗0+ 
15
10
​
 ∗ 
100
48
​
 =0.32

第三个 选择房子A 3 A_3A 
3
​
 这个特征*
房子特征中有：有房子A 31 A_{31}A 
31
​
 、无房子A 32 A_{32}A 
32
​
  2个特征值。

D|A3	有自己的房子	个数	是否同意贷款
否	是
D1	是	6	0	6
D2	否	9	3	6
那么，有房子D 1 D_1D 
1
​
 的基尼数和权重：
G i n i ( D 1 ) = 2 ∗ 0 6 ∗ 6 6 = 0 w 1 = 6 15 Gini(D_1)=2*\frac{0}{6}*\frac{6}{6}=0 \\ w_1=\frac{6}{15}
Gini(D 
1
​
 )=2∗ 
6
0
​
 ∗ 
6
6
​
 =0
w 
1
​
 = 
15
6
​
 

有房子D 2 D_2D 
2
​
 的基尼数和权重：
G i n i ( D 2 ) = 2 ∗ 6 9 ∗ 3 9 = 36 81 w 2 = 9 15 Gini(D_2)=2*\frac{6}{9}*\frac{3}{9}=\frac{36}{81}\\w_2=\frac{9}{15}
Gini(D 
2
​
 )=2∗ 
9
6
​
 ∗ 
9
3
​
 = 
81
36
​
 
w 
2
​
 = 
15
9
​
 

G i n i ( D , A 2 ) = 6 15 ∗ 0 + 9 15 ∗ 36 81 = 0.27 Gini(D,A_{2})=\frac{6}{15} *0+\frac{9}{15}*\frac{36}{81}=0.27
Gini(D,A 
2
​
 )= 
15
6
​
 ∗0+ 
15
9
​
 ∗ 
81
36
​
 =0.27

第四个 选择信贷A 4 A_4A 
4
​
 这个特征
信贷特征中有：信贷非常好A 41 A_{41}A 
41
​
 、信贷好A 42 A_{42}A 
42
​
 、信贷一般A 43 A_{43}A 
43
​
  3个特征值。

D|A4	信贷情况	个数	是否同意贷款
否	是
D1	非常好	4	0	4
D2	好	6	2	4
D3	一般	5	4	4
1.以非常好和并不非常好分类：

那么，非常好D 1 D_1D 
1
​
 的基尼数和权重：
G i n i ( D 1 ) = 2 ∗ 0 4 ∗ 4 4 = 0 w 1 = 4 15 Gini(D_1)=2*\frac{0}{4}*\frac{4}{4}=0 \\ w_1=\frac{4}{15}
Gini(D 
1
​
 )=2∗ 
4
0
​
 ∗ 
4
4
​
 =0
w 
1
​
 = 
15
4
​
 

不非常好D 2 D_2D 
2
​
 的基尼数和权重：
G i n i ( D 2 ) = 2 ∗ 5 11 ∗ 6 11 = 48 100 w 2 = 11 15 Gini(D_2)=2*\frac{5}{11}*\frac{6}{11}=\frac{48}{100}\\w_2=\frac{11}{15}
Gini(D 
2
​
 )=2∗ 
11
5
​
 ∗ 
11
6
​
 = 
100
48
​
 
w 
2
​
 = 
15
11
​
 

G i n i ( D , A 41 ) = 4 15 ∗ 0 + 11 15 ∗ 60 121 = 0.36 Gini(D,A_{41})=\frac{4}{15} *0+\frac{11}{15}*\frac{60}{121}=0.36
Gini(D,A 
41
​
 )= 
15
4
​
 ∗0+ 
15
11
​
 ∗ 
121
60
​
 =0.36

2.以好和非好分类

G i n i ( D , A 42 ) = 6 15 ∗ 2 ∗ 4 6 ∗ 2 6 + 9 15 ∗ 2 ∗ 5 9 ∗ 4 9 = 0.47 Gini(D,A_{42})=\frac{6}{15} *2*\frac{4}{6}*\frac{2}{6}+\frac{9}{15}*2*\frac{5}{9}*\frac{4}{9}=0.47
Gini(D,A 
42
​
 )= 
15
6
​
 ∗2∗ 
6
4
​
 ∗ 
6
2
​
 + 
15
9
​
 ∗2∗ 
9
5
​
 ∗ 
9
4
​
 =0.47

3.以一般和非一般分类

G i n i ( D , A 43 ) = 5 15 ∗ 2 ∗ 1 5 ∗ 4 5 + 10 15 ∗ 2 ∗ 8 10 ∗ 2 10 = 0.32 Gini(D,A_{43})=\frac{5}{15} *2*\frac{1}{5}*\frac{4}{5}+\frac{10}{15}*2*\frac{8}{10}*\frac{2}{10}=0.32
Gini(D,A 
43
​
 )= 
15
5
​
 ∗2∗ 
5
1
​
 ∗ 
5
4
​
 + 
15
10
​
 ∗2∗ 
10
8
​
 ∗ 
10
2
​
 =0.32

可以看出，特征值一般的基尼指数最小，作为最优划分点。

把4个特征得出的基尼指数进行比较：

特征值	基尼指数
年龄	0.44
工作	0.32
房子	0.27
信贷情况	0.32
可以看出，特征房子对应的基尼指数最小，那么就可以作为最优特征绘制二叉树。

可以看出，有房子的都是同意贷款，那么没房子这里该怎么继续划分，继续对无房子的数据集进行统计：

ID	年龄	有工作	有自己的房子	信贷情况	类别
1	青年	否	否	一般	否
2	青年	否	否	好	否
3	青年	是	否	好	是
5	青年	否	否	一般	否
6	中年	否	否	一般	否
7	中年	否	否	好	否
13	老年	是	否	好	是
14	老年	是	否	非常好	是
15	老年	否	否	一般	否
按照年龄、工作、信贷情况来分类，可以得出：

1.在无房子数据集内，以年龄特征分类

年龄	个数	不同意贷款	同意贷款
青年	4	3	1
中年	2	2	0
老年	3	1	2
2.在无房子数据集内，以工作特征分类

工作	个数	不同意贷款	同意贷款
有工作	3	0	3
无工作	6	6	0
3.在无房子数据集内，以信贷情况特征分类

信贷情况	个数	不同意贷款	同意贷款
非常好	1	0	1
好	4	2	2
一般	4	4	0
不用分别计算就可以看出工作这个特征对应的分类非常明显，因此对应的基尼指数肯定最小，那么就可以继续选这个特征进行二叉树的分类。
于是：

1.1.5 回归树
输出是连续的
1.划分连续数据
既然是决策树，那么输出的一定就是叶子结点，对于 连续变量而言，可以按照一定的要求将连续变量进行划分。
以之前桃子例子来说：
输入：用[ 0 , 0.5 ] [0,0.5][0,0.5]来表示由不甜到甜的程度
输出：用[ 1 , 10 ] [1,10][1,10]来表示由不好吃到好吃的程度

甜度	0.05	0.15	0.25	0.35	0.45
好吃程度	5.5	7.6	9.5	9.7	8.2
在对数据划分时，就可以从甜度这个输入量来进行划分，但是要注意由于CART算法是二叉树，所以每次划分只能划分成两类，比如：甜度≤ 0.1 \leq 0.1≤0.1和甜度> 0.1 > 0.1>0.1这样两类，然后可以再继续在甜度> 0.1 > 0.1>0.1 这个范围内在选择最优切分点继续划分。

右上角的角标数意味着分类的次数，右下角标数代表着所属的类，这就意味着，输出单元最终不只是 2个单元，也可以是多个单元。
2.如何生成回归树模型
假设将输入空间划分成M MM个单元R 1 , R 2 , ⋯   , R m R_1,R_2,\cdots,R_mR 
1
​
 ,R 
2
​
 ,⋯,R 
m
​
 ，并在每个单元R m R_mR 
m
​
 上有一个固定的输出值c m c_mc 
m
​
 ，回归树模型可以表示为：
f ( x ) = ∑ m = 1 M c m I ( x ∈ R m ) f(x)=\sum_{m=1}^Mc_mI(x\in R_m)
f(x)= 
m=1
∑
M
​
 c 
m
​
 I(x∈R 
m
​
 )

这里的f ( x ) f(x)f(x)就是CART回归模型，c m c_mc 
m
​
 代表输出的类，I ( x ∈ R m ) I(x\in R_m)I(x∈R 
m
​
 )是指示性函数。
假设输入和输出变量如下：

输入	R 1 R_1R 
1
​
 	R 2 R_2R 
2
​
 	…	R m R_mR 
m
​
 
输出	c 1 c_1c 
1
​
 	c 2 c_2c 
2
​
 	…	c m c_mc 
m
​
 
I ( x ∈ R m ) I(x\in R_m)I(x∈R 
m
​
 )是指当x ∈ R m x\in R_mx∈R 
m
​
 取1 11，x ∉ R m x\notin R_mx∈
/
R 
m
​
 取0 00。
这样就意味着，对于某个输出单元也就是类c m c_mc 
m
​
 而言，当输入单元R m R_mR 
m
​
 和它一致时就存在，如果不一致时，就没有。这样把所有输入单元对应的类求和之后，便是最终的回归树模型。

3.平方误差和最优输出
怎么找切分点？这时就需要通过平方误差最小化来找到最优切分点了。
选择第x ( j ) x^{(j)}x 
(j)
 个变量和取s ss，分别作为切分变量和切分点，并定义两个区域：
R 1 ( j , s ) = x ∣ x ( j ) ≤ s R 2 ( j , s ) = x ∣ x ( j ) > s R_1(j,s)=x|x^{(j)}\leq s \\ R_2(j,s)=x|x^{(j)} > s
R 
1
​
 (j,s)=x∣x 
(j)
 ≤s
R 
2
​
 (j,s)=x∣x 
(j)
 >s

用平方误差最小化来寻找最优切分变量j jj和最优切分点s ss：
min ⁡ j , s [ m i n c 1 ∑ x i ∈ R 1 ( j , s ) ( y i − c 1 ) 2 + m i n c 2 ∑ x i ∈ R 2 ( j , s ) ( y i − c 2 ) 2 ] \min_{j,s}\left[min_{c_1}\sum_{x_i\in R_1(j,s)}(y_i-c_1)^2+min_{c_2}\sum_{x_i\in R_2(j,s)}(y_i-c_2)^2\right]
j,s
min
​
  
​
 min 
c 
1
​
 
​
  
x 
i
​
 ∈R 
1
​
 (j,s)
∑
​
 (y 
i
​
 −c 
1
​
 ) 
2
 +min 
c 
2
​
 
​
  
x 
i
​
 ∈R 
2
​
 (j,s)
∑
​
 (y 
i
​
 −c 
2
​
 ) 
2
  
​
 

这个公式意味着，将输出变量按照输入变量分为了两类，然后要求出来每次分类后的各个分类的平方误差最小值之和，也就意味着整体的最小平方误差，平方误差最小，意味着分类和实际最吻合。其中：
c ^ 1 = a v e ( y i ∣ x i ∈ R 1 ( j , s ) ) c ^ 2 = a v e ( y i ∣ x i ∈ R 2 ( j , s ) ) \hat c_1=ave (y_i|x_i\in R_1(j,s))\\ \hat c_2=ave (y_i|x_i\in R_2(j,s))
c
^
  
1
​
 =ave(y 
i
​
 ∣x 
i
​
 ∈R 
1
​
 (j,s))
c
^
  
2
​
 =ave(y 
i
​
 ∣x 
i
​
 ∈R 
2
​
 (j,s))

这里可以理解成，如果我们想要平方误差最小，那么就是将每次分类后的和设置为对应的每个区域内的输出变量的平均值。

为什么使用该节点所有样本平均值时得到的损失最小？
推导过程：

损失函数：
J = 1 n ∑ i = 1 n ( f ( x i ) − y i ) 2 = 1 n ∑ i = 1 n ( ∑ m = 1 M c m I ( x i ∈ R m ) − y i ) 2 , 按照样本的方式去遍历 = 1 n ∑ m = 1 M ∑ x i ∈ R m ( c m − y i ) 2 , 按照节点去遍历 
J
​
  
= 
n
1
​
  
i=1
∑
n
​
 (f(x 
i
​
 )−y 
i
​
 ) 
2
 
= 
n
1
​
  
i=1
∑
n
​
 ( 
m=1
∑
M
​
 c 
m
​
 I(x 
i
​
 ∈R 
m
​
 )−y 
i
​
 ) 
2
 ,按照样本的方式去遍历
= 
n
1
​
  
m=1
∑
M
​
  
x 
i
​
 ∈R 
m
​
 
∑
​
 (c 
m
​
 −y 
i
​
 ) 
2
 ,按照节点去遍历
​
 
其中，1 n ∑ m = 1 M \frac{1}{n}\sum_{m=1}^M 
n
1
​
 ∑ 
m=1
M
​
 是叶子节点数，∑ x i ∈ R m \sum_{x_i\in R_m}∑ 
x 
i
​
 ∈R 
m
​
 
​
 是叶子结点R m R_mR 
m
​
 中所包含的样本。
优化目标：c m ∗ = min ⁡ c m 1 n ∑ m = 1 M ∑ x i ∈ R m ( c m − y i ) 2 c_m^*=\min_{c_m}\frac{1}{n}\sum_{m=1}^M\sum_{x_i\in R_m}(c_m-y_i)^2c 
m
∗
​
 =min 
c 
m
​
 
​
  
n
1
​
 ∑ 
m=1
M
​
 ∑ 
x 
i
​
 ∈R 
m
​
 
​
 (c 
m
​
 −y 
i
​
 ) 
2
 
，此时损失函数只包含一个参数c m c_mc 
m
​
 ，可直接对J JJ求导并令导数等于0，求解c m ∗ c_m^*c 
m
∗
​
 ：
∂ J ∂ c m = 1 n ∑ m = 1 M ∑ x i ∈ R m ( c m − y i ) 2 ∂ c m = 1 n ∑ x i ∈ R m ( c m − y i ) 2 ∂ c m = 2 ∑ x i ∈ R m ( c m − y i ) = N m c m − ∑ x i ∈ R m y i , 其中 N m 是叶子节点 R m 包含样本的个数
∂c 
m
​
 
∂J
​
 
​
  
= 
∂c 
m
​
 
n
1
​
 ∑ 
m=1
M
​
 ∑ 
x 
i
​
 ∈R 
m
​
 
​
 (c 
m
​
 −y 
i
​
 ) 
2
 
​
 
= 
∂c 
m
​
 
n
1
​
 ∑ 
x 
i
​
 ∈R 
m
​
 
​
 (c 
m
​
 −y 
i
​
 ) 
2
 
​
 
=2 
x 
i
​
 ∈R 
m
​
 
∑
​
 (c 
m
​
 −y 
i
​
 )
=N 
m
​
 c 
m
​
 − 
x 
i
​
 ∈R 
m
​
 
∑
​
 y 
i
​
 ,其中N 
m
​
 是叶子节点R 
m
​
 包含样本的个数
​
 

令该导数等于0，有：
N m c m − ∑ x i ∈ R m y i = 0 N_mc_m-\sum_{x_i\in R_m}y_i=0
N 
m
​
 c 
m
​
 − 
x 
i
​
 ∈R 
m
​
 
∑
​
 y 
i
​
 =0
，解得：c m = ∑ x i ∈ R m y i N m c_m=\frac{\sum_{x_i\in R_m}y_i}{N_m}c 
m
​
 = 
N 
m
​
 
∑ 
x 
i
​
 ∈R 
m
​
 
​
 y 
i
​
 
​
 

即：当每个叶子节点的c m c_mc 
m
​
 的取值，为该节点所有样本y i y_iy 
i
​
 的平均值时，得到损失最小，即最优的回归树。

4.停止条件
可以是将输出变量分为两个类，也可以是直到没有多余的样本点。
输出的就是一棵CART二叉树。

5.例题：桃子例题

甜度	0.05	0.15	0.25	0.35	0.45
好吃程度	5.5	7.6	9.5	9.7	8.2
1.以甜度特征进行回归计算
第一 以甜度 s = 0.1 s=0.1s=0.1进行划分
可以将表格里的连续数据划分成R 1 R_1R 
1
​
 和R 2 R_2R 
2
​
 两类：
R 1 R_1R 
1
​
 类是：

甜度	0.05
好吃程度	5.5
R 2 R_2R 
2
​
 类是：

甜度	0.15	0.25	0.35	0.45
好吃程度	7.6	9.5	9.7	8.2
可以得出：
c ^ 1 = 5.5 c ^ 2 = 7.6 + 9.5 + 9.7 + 8.2 4 = 8.75 \hat c_1=5.5 \\ \hat c_2=\frac{7.6+9.5+9.7+8.2}{4}=8.75
c
^
  
1
​
 =5.5
c
^
  
2
​
 = 
4
7.6+9.5+9.7+8.2
​
 =8.75

接着代入平方误差公式中：

= ∑ x i ∈ R 1 ( j , s ) ( y i − c 1 ) 2 + ∑ x i ∈ R 2 ( j , s ) ( y i − c 2 ) 2 = 0 + ( 7.6 − 8.75 ) 2 + ( 9.5 − 8.75 ) 2 + ( 9.7 − 8.75 ) 2 + ( 8.2 − 8.75 ) 2 = 3.09
​
  
= 
x 
i
​
 ∈R 
1
​
 (j,s)
∑
​
 (y 
i
​
 −c 
1
​
 ) 
2
 + 
x 
i
​
 ∈R 
2
​
 (j,s)
∑
​
 (y 
i
​
 −c 
2
​
 ) 
2
 
=0+(7.6−8.75) 
2
 +(9.5−8.75) 
2
 +(9.7−8.75) 
2
 +(8.2−8.75) 
2
 
=3.09
​
 

第二步 以甜度s = 0.2 s=0.2s=0.2进行划分
计算出平方误差和结果为3.53 3.533.53。

第三步 以甜度s = 0.3 s=0.3s=0.3进行划分
计算出平方误差和结果为9.13 9.139.13。

第四步 以甜度s = 0.4 s=0.4s=0.4进行划分
计算出平方误差和结果为11.52 11.5211.52。

这样，从四个分类中，我们选取最小值，也就是当甜度s 1 = 0.1 s_1=0.1s 
1
​
 =0.1时，作为最优切分点，同时输出的CART回归树模型就是：
f ( x ) = { 5.5 ( s ≤ 0.1 ) 8.75 ( s > 0.1 ) f(x)=
f(x)={ 
5.5
8.75
​
  
(s≤0.1)
(s>0.1)
​
 

当然我们还可以对s > 0.1 s>0.1s>0.1区域进行回归划分，这就要取决于你的停止条件，如果说是继续分成三类，那么就可以按照相同的思路进行计算。
通过对连续变量进行划分，就可以转换为离散的变量来进行计算，那么就和之前的分类树模型也是相通的方法，这也就是为什么常见的都是CART分类树模型。

1.1.6 CART的剪枝
感兴趣自己看看，这里省略
————————————————
版权声明：本文为CSDN博主「joejoeqian」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/joejoeqian/article/details/129974453



GBDT
1.预备知识
2.提升树
2.1 一般步骤
2.1.1 模型
2.1.2 损失函数
2.1.3 优化方法
2.2 二分类问题的提升树
2.2.1 基学习器
2.2.2 损失函数
2.2.3 说明
2.2.4 原理
2.3 回归问题的提升树
2.3.1 基学习器
2.3.2 损失函数
2.3.3 前向分步算法
2.3.4 思路
3.一般决策问题梯度提升树GBDT
3.1 要解决的问题
3.2 基学习器
3.3 损失函数
3.4 前向分步算法+梯度提升
3.4.1 核心目标
3.4.2 将损失函数进行处理
3.4.3 梯度提升
3.5 算法流程
3.6 思路
3.6.1 个体学习器如何训练得到
3.6.2 如何将个体学习组合
3.6.3 目标
3.7 例题
3.8 GBDT的优缺点
3.8.1 GBDT主要的优点
3.8.2 GBDT的主要缺点
4.GBDT解决二分类问题
4.1 面临的问题
4.2 解决方案
4.2.1 逻辑回归做二分类
4.2.2 学传统逻辑回归的解决方案
4.3 模型
4.4 基学习器
4.4.1 前向分步算法+梯度提升
1.预备知识
集成学习之GBDT预备知识

2.提升树
被认为是统计学习中性能最好的方法之一

2.1 一般步骤
2.1.1 模型
使用的也是加法模型：
f M ( x ) = ∑ m = 1 M T ( x ; Θ m ) f_M(x)=\sum_{m=1}^MT(x;\Theta_m)
f 
M
​
 (x)= 
m=1
∑
M
​
 T(x;Θ 
m
​
 )
，其中M MM为树的个数，T ( x ; Θ m ) T(x;\Theta_m)T(x;Θ 
m
​
 )表示决策树，Θ m \Theta_mΘ 
m
​
 为决策树的参数。

2.1.2 损失函数
回归问题：平方误差损失函数
分类问题：
二分类问题：指数损失函数
多分类问题：softmax
一般决策问题：自定义损失函数
2.1.3 优化方法
前向分步算法：

如书上：
其中当前模型f m ( x ) f_m(x)f 
m
​
 (x)已知。

2.2 二分类问题的提升树
2.2.1 基学习器
CART决策树

2.2.2 损失函数
使用指数损失函数：
f ( x ) = ∑ m = 1 M β m b ( x ; γ m ) f M ( x ) = ∑ m = 1 M T ( x ; Θ m ) f(x)=\sum_{m=1}^M\beta_mb(x;\gamma _m)\\f_M(x)=\sum_{m=1}^MT(x;\Theta_m)
f(x)= 
m=1
∑
M
​
 β 
m
​
 b(x;γ 
m
​
 )
f 
M
​
 (x)= 
m=1
∑
M
​
 T(x;Θ 
m
​
 )

2.2.3 说明
提升树相当于Adaboost算法的特殊情况，

Adaboost的模型为：
f ( x ) = ∑ m = 1 M α m G m ( x ) = α 1 G 1 ( x ) + ⋯ + α m G m ( x ) + ⋯ + α M G M ( x )
f(x)=∑m=1MαmGm(x)=α1G1(x)+⋯+αmGm(x)+⋯+αMGM(x)
�
(
�
)
=
∑
�
=
1
�
�
�
�
�
(
�
)
=
�
1
�
1
(
�
)
+
⋯
+
�
�
�
�
(
�
)
+
⋯
+
�
�
�
�
(
�
)
f(x)
​
  
= 
m=1
∑
M
​
 α 
m
​
 G 
m
​
 (x)
=α 
1
​
 G 
1
​
 (x)+⋯+α 
m
​
 G 
m
​
 (x)+⋯+α 
M
​
 G 
M
​
 (x)
​
 
，其中α m \alpha_mα 
m
​
 由G m ( x ) G_m(x)G 
m
​
 (x)的"分类误差率"决定，训练样本G m ( x ) G_m(x)G 
m
​
 (x)：提高前一轮“错误分类”的样本的权值，降低前一轮“正确分类”的样本的权值。

它只是将：

1.基分类器G(x)限制为二分类树；
2.基分类器权重α m \alpha_mα 
m
​
 全部置为1。
如图：

2.2.4 原理
只要我们使用的是指数损失函数，就可以用指数损失函数来调整样本数据的权重，从而让每个基分类器学到不同的内容。

指数损失函数：
L ( y , f ( x ) ) = exp ⁡ [ − y f ( x ) ] L(y,f(x))=\exp[-yf(x)]
L(y,f(x))=exp[−yf(x)]

，其中当f ( x ) f(x)f(x)分类正确时，与y yy同号，L ( y , f ( x ) ) < = 1 L(y,f(x))<=1L(y,f(x))<=1。当f ( x ) f(x)f(x)分类错误时，与y yy异号，L ( y , f ( x ) ) > 1 L(y,f(x))>1L(y,f(x))>1。

2.3 回归问题的提升树
2.3.1 基学习器
CART回归树
T ( x ; Θ ) = ∑ j = 1 J c j I ( x ∈ R j ) T(x;\Theta)=\sum_{j=1}^Jc_jI(x\in R_j)T(x;Θ)=∑ 
j=1
J
​
 c 
j
​
 I(x∈R 
j
​
 )
f M ( x ) = ∑ m = 1 M T ( x ; Θ m ) f_M(x)=\sum_{m=1}^MT(x;\Theta_m)f 
M
​
 (x)=∑ 
m=1
M
​
 T(x;Θ 
m
​
 )

2.3.2 损失函数
使用平方误差损失：L ( y , f ( x ) ) = ( y − f ( x ) ) 2 L(y,f(x))=(y-f(x))^2L(y,f(x))=(y−f(x)) 
2
 
L ( y , f ( x ) ) = ( y − f ( x ) ) 2 = [ y − f m − 1 ( x ) − T ( x ; Θ m ) ] 2 = [ r − T ( x ; Θ m ) ] 2
L(y,f(x))=(y−f(x))2=[y−fm−1(x)−T(x;Θm)]2=[r−T(x;Θm)]2
�
(
�
,
�
(
�
)
)
=
(
�
−
�
(
�
)
)
2
=
[
�
−
�
�
−
1
(
�
)
−
�
(
�
;
Θ
�
)
]
2
=
[
�
−
�
(
�
;
Θ
�
)
]
2
L(y,f(x))
​
  
=(y−f(x)) 
2
 
=[y−f 
m−1
​
 (x)−T(x;Θ 
m
​
 )] 
2
 
=[r−T(x;Θ 
m
​
 )] 
2
 
​
 

，其中r rr是残差，目标就是拟合残差r rr。

2.3.3 前向分步算法
θ ^ m = arg ⁡ min ⁡ θ m ∑ i = 1 N L ( y ( i ) , f m − 1 ( x ( i ) ) + T ( x ( i ) , θ m ) ) = arg ⁡ min ⁡ θ m ∑ i = 1 N ( r m ( i ) − T ( x ( i ) , θ m ) ) 2
θ^m=argminθm∑i=1NL(y(i),fm−1(x(i))+T(x(i),θm))=argminθm∑i=1N(r(i)m−T(x(i),θm))2
�
^
�
=
arg
⁡
min
�
�
∑
�
=
1
�
�
(
�
(
�
)
,
�
�
−
1
(
�
(
�
)
)
+
�
(
�
(
�
)
,
�
�
)
)
=
arg
⁡
min
�
�
∑
�
=
1
�
(
�
�
(
�
)
−
�
(
�
(
�
)
,
�
�
)
)
2
θ
^
  
m
​
 
​
  
=arg 
θm
min
​
  
i=1
∑
N
​
 L(y 
(i)
 ,f 
m−1
​
 (x 
(i)
 )+T(x 
(i)
 ,θ 
m
​
 ))
=arg 
θm
min
​
  
i=1
∑
N
​
 (r 
m
(i)
​
 −T(x 
(i)
 ,θ 
m
​
 )) 
2
 
​
 

2.3.4 思路
1.个体学习器如何训练得到
如何改变训练数据的权值或概率分布如何改变？

用残差进行拟合，一步一步的将残差缩小。

2.如何将个体学习器组合
相加

3.目标

使得总体损失逐步减少

3.一般决策问题梯度提升树GBDT
GBDT也是迭代，使用了前向分布算法，但是弱学习器限定了只能使用CART回归树模型，同时迭代思路和Adaboost也有所不同。

GBDT的思想可以用一个通俗的例子解释，假如有个人30岁，我们首先用20岁去拟合，发现损失有10岁，这时我们用6岁去拟合剩下的损失，发现差距还有4岁，第三轮我们用3岁拟合剩下的差距，差距就只有一岁了。如果我们的迭代轮数还没有完，可以继续迭代下面，每一轮迭代，拟合的岁数误差都会减小。

从上面的例子看这个思想还是蛮简单的，但是有个问题是这个损失的拟合不好度量，损失函数各种各样，怎么找到一种通用的拟合方法呢？

3.1 要解决的问题


3.2 基学习器
回归树：

T ( x ; Θ ) = ∑ j = 1 J c j I ( x ∈ R j ) T(x;\Theta)=\sum_{j=1}^Jc_jI(x\in R_j)T(x;Θ)=∑ 
j=1
J
​
 c 
j
​
 I(x∈R 
j
​
 )
f M ( x ) = ∑ m = 1 M T ( x ; Θ m ) f_M(x)=\sum_{m=1}^MT(x;\Theta_m)f 
M
​
 (x)=∑ 
m=1
M
​
 T(x;Θ 
m
​
 )

3.3 损失函数
找到一般的损失函数：L ( y , f ( x ) ) L(y,f(x))L(y,f(x))

3.4 前向分步算法+梯度提升
3.4.1 核心目标
1.已知加法模型，一定会存在多个优化器，不断迭代优化；
2.我们要确保，每增加一个基学习器，都要使得总体损失越来越小，即第m步要比第m-1步的损失要小。
即：
L ( y ( i ) , f m ( x ( i ) ) ) < L ( y ( i ) , f m − 1 ( x ( i ) ) ) → L ( y ( i ) , f m − 1 ( x ( i ) ) ) − L ( y ( i ) , f m ( x ( i ) ) ) > 0
L(y(i),fm(x(i)))<L(y(i),fm−1(x(i)))→L(y(i),fm−1(x(i)))−L(y(i),fm(x(i)))>0
�
(
�
(
�
)
,
�
�
(
�
(
�
)
)
)
<
�
(
�
(
�
)
,
�
�
−
1
(
�
(
�
)
)
)
→
�
(
�
(
�
)
,
�
�
−
1
(
�
(
�
)
)
)
−
�
(
�
(
�
)
,
�
�
(
�
(
�
)
)
)
>
0
​
  
L(y 
(i)
 ,f 
m
​
 (x 
(i)
 ))<L(y 
(i)
 ,f 
m−1
​
 (x 
(i)
 ))→L(y 
(i)
 ,f 
m−1
​
 (x 
(i)
 ))−L(y 
(i)
 ,f 
m
​
 (x 
(i)
 ))>0
​
 

3.4.2 将损失函数进行处理
处理的原因就是：往我们的核心目标上靠。

L ( y ( i ) , f m ( x ( i ) ) ) < L ( y ( i ) , f m − 1 ( x ( i ) ) ) → L ( y ( i ) , f m − 1 ( x ( i ) ) ) − L ( y ( i ) , f m ( x ( i ) ) ) > 0
L(y(i),fm(x(i)))<L(y(i),fm−1(x(i)))→L(y(i),fm−1(x(i)))−L(y(i),fm(x(i)))>0
�
(
�
(
�
)
,
�
�
(
�
(
�
)
)
)
<
�
(
�
(
�
)
,
�
�
−
1
(
�
(
�
)
)
)
→
�
(
�
(
�
)
,
�
�
−
1
(
�
(
�
)
)
)
−
�
(
�
(
�
)
,
�
�
(
�
(
�
)
)
)
>
0
​
  
L(y 
(i)
 ,f 
m
​
 (x 
(i)
 ))<L(y 
(i)
 ,f 
m−1
​
 (x 
(i)
 ))→L(y 
(i)
 ,f 
m−1
​
 (x 
(i)
 ))−L(y 
(i)
 ,f 
m
​
 (x 
(i)
 ))>0
​
 

由泰勒公式：f ( x ) ≈ f ( x 0 ) + f ′ ( x 0 ) ( x − x 0 ) f(x)\approx f(x_0)+f^{'}(x_0)(x-x_0)f(x)≈f(x 
0
​
 )+f 
′
 
 (x 
0
​
 )(x−x 
0
​
 )和L ( y , f m ( x ) ) L(y,f_m(x))L(y,f 
m
​
 (x))中只有f m ( x ) f_m(x)f 
m
​
 (x)是未知量，且f m ( x ) = f m − 1 ( x ) + T ( x ; Θ m ) f_m(x)=f_{m-1}(x)+T(x;\Theta_m)f 
m
​
 (x)=f 
m−1
​
 (x)+T(x;Θ 
m
​
 )，得：

L ( y , f m ( x ) ) ≈ L ( y , f m − 1 ( x ) ) + ∂ L ( y , f m ( x ) ) ∂ f m ( x ) ∣ f m ( x ) = f m − 1 ( x ) ⋅ ( f m ( x ) − f m − 1 ( x ) ) = L ( y , f m − 1 ( x ) ) + ∂ L ( y , f m ( x ) ) ∂ f m ( x ) ∣ f m ( x ) = f m − 1 ( x ) ⋅ T ( x ; Θ m )
L(y,fm(x))≈L(y,fm−1(x))+∂L(y,fm(x))∂fm(x)|fm(x)=fm−1(x)⋅(fm(x)−fm−1(x))=L(y,fm−1(x))+∂L(y,fm(x))∂fm(x)|fm(x)=fm−1(x)⋅T(x;Θm)
�
(
�
,
�
�
(
�
)
)
≈
�
(
�
,
�
�
−
1
(
�
)
)
+
∂
�
(
�
,
�
�
(
�
)
)
∂
�
�
(
�
)
|
�
�
(
�
)
=
�
�
−
1
(
�
)
⋅
(
�
�
(
�
)
−
�
�
−
1
(
�
)
)
=
�
(
�
,
�
�
−
1
(
�
)
)
+
∂
�
(
�
,
�
�
(
�
)
)
∂
�
�
(
�
)
|
�
�
(
�
)
=
�
�
−
1
(
�
)
⋅
�
(
�
;
Θ
�
)
L(y,f 
m
​
 (x))
​
  
≈L(y,f 
m−1
​
 (x))+ 
∂f 
m
​
 (x)
∂L(y,f 
m
​
 (x))
​
 ∣ 
f 
m
​
 (x)=f 
m−1
​
 (x)
​
 ⋅(f 
m
​
 (x)−f 
m−1
​
 (x))
=L(y,f 
m−1
​
 (x))+ 
∂f 
m
​
 (x)
∂L(y,f 
m
​
 (x))
​
 ∣ 
f 
m
​
 (x)=f 
m−1
​
 (x)
​
 ⋅T(x;Θ 
m
​
 )
​
 

即有：

L ( y , f m − 1 ( x ) ) − L ( y , f m ( x ) ) ≈ − ∂ L ( y , f m ( x ) ) ∂ f m ( x ) ∣ f m ( x ) = f m − 1 ( x ) ⋅ T ( x ; Θ m )
L(y,fm−1(x))−L(y,fm(x))≈−∂L(y,fm(x))∂fm(x)|fm(x)=fm−1(x)⋅T(x;Θm)
�
(
�
,
�
�
−
1
(
�
)
)
−
�
(
�
,
�
�
(
�
)
)
≈
−
∂
�
(
�
,
�
�
(
�
)
)
∂
�
�
(
�
)
|
�
�
(
�
)
=
�
�
−
1
(
�
)
⋅
�
(
�
;
Θ
�
)
L(y,f 
m−1
​
 (x))−L(y,f 
m
​
 (x))
​
  
≈− 
∂f 
m
​
 (x)
∂L(y,f 
m
​
 (x))
​
 ∣ 
f 
m
​
 (x)=f 
m−1
​
 (x)
​
 ⋅T(x;Θ 
m
​
 )
​
 

当T ( x ; Θ m ) ≈ − ∂ L ( y , f m ( x ) ) ∂ f m ( x ) ∣ f m ( x ) = f m − 1 ( x ) T(x;\Theta_m)\approx -\frac{\partial L(y,f_m(x))}{\partial f_m(x)}|_{f_m(x)=f_{m-1}(x)}T(x;Θ 
m
​
 )≈− 
∂f 
m
​
 (x)
∂L(y,f 
m
​
 (x))
​
 ∣ 
f 
m
​
 (x)=f 
m−1
​
 (x)
​
 时，L ( y , f m − 1 ( x ) ) − L ( y , f m ( x ) ) ≥ 0 L(y,f_{m-1}(x))-L(y,f_m(x))\geq 0L(y,f 
m−1
​
 (x))−L(y,f 
m
​
 (x))≥0
，其中该式子一旦等于0就终止训练。

r m ( x , y ) = − ∂ L ( y , f m ( x ) ) ∂ f m ( x ) ∣ f m ( x ) = f m − 1 ( x ) r_m(x,y)=-\frac{\partial L(y,f_m(x))}{\partial f_m(x)}|_{f_m(x)=f_{m-1}(x)}r 
m
​
 (x,y)=− 
∂f 
m
​
 (x)
∂L(y,f 
m
​
 (x))
​
 ∣ 
f 
m
​
 (x)=f 
m−1
​
 (x)
​
 ，将( x i , y i ) (x_i,y_i)(x 
i
​
 ,y 
i
​
 )代入r m ( x , y ) r_m(x,y)r 
m
​
 (x,y)，即可得到r m r_mr 
m
​
 ，进而得到第m mm轮的训练数据集：T m = { ( x 1 , r m 1 ) , ( x 2 , r m 2 ) , ⋯   , ( x N , r m N ) } T_m=\{(x_1,r_{m1}),(x_2,r_{m2}),\cdots,(x_N,r_{mN})\}T 
m
​
 ={(x 
1
​
 ,r 
m1
​
 ),(x 
2
​
 ,r 
m2
​
 ),⋯,(x 
N
​
 ,r 
mN
​
 )}，其中r m r_mr 
m
​
 是被划分的输入空间，c m c_mc 
m
​
 空间r m r_mr 
m
​
 对应的输出值。

3.4.3 梯度提升
1.计算当前损失函数的负梯度表达式
r m ( x , y ) = − ∂ L ( y , f m ( x ) ) ∂ f m ( x ) ∣ f m ( x ) = f m − 1 ( x ) r_m(x,y)=-\frac{\partial L(y,f_m(x))}{\partial f_m(x)}|_{f_m(x)=f_{m-1}(x)}
r 
m
​
 (x,y)=− 
∂f 
m
​
 (x)
∂L(y,f 
m
​
 (x))
​
 ∣ 
f 
m
​
 (x)=f 
m−1
​
 (x)
​
 

2.构造新的训练样本
将( x i , y i ) (x_i,y_i)(x 
i
​
 ,y 
i
​
 )代入r m ( x , y ) r_m(x,y)r 
m
​
 (x,y)，即可得到r m r_mr 
m
​
 ，进而得到第m mm轮的训练数据集：T m = { ( x 1 , r m 1 ) , ( x 2 , r m 2 ) , ⋯   , ( x N , r m N ) } T_m=\{(x_1,r_{m1}),(x_2,r_{m2}),\cdots,(x_N,r_{mN})\}T 
m
​
 ={(x 
1
​
 ,r 
m1
​
 ),(x 
2
​
 ,r 
m2
​
 ),⋯,(x 
N
​
 ,r 
mN
​
 )}，其中r m r_mr 
m
​
 是被划分的输入空间，c m c_mc 
m
​
 空间r m r_mr 
m
​
 对应的输出值。

3.让当前的基学习器去拟合上述训练样本，得到T ( x ; Θ m ) T(x;\Theta_m)T(x;Θ 
m
​
 )

3.5 算法流程
输入是训练集样本T = { ( x , y 1 ) , ( x 2 , y 2 ) , . . . ( x m , y m ) } T=\{(x_,y_1),(x_2,y_2), ...(x_m,y_m)\}T={(x 
,
​
 y 
1
​
 ),(x 
2
​
 ,y 
2
​
 ),...(x 
m
​
 ,y 
m
​
 )}， 最大迭代次数T, 损失函数L。
输出是强学习器f ( x ) f(x)f(x)

初始化弱学习器
f 0 ( x ) = a r g    m i n ⏟ c ∑ i = 1 m L ( y i , c ) f_0(x) = \underbrace{arg\; min}_{c}\sum\limits_{i=1}^{m}L(y_i, c)
f 
0
​
 (x)= 
c
argmin
​
 
​
  
i=1
∑
m
​
 L(y 
i
​
 ,c)
对迭代轮数t=1,2,…T有：
a) 对样本i = 1 , 2 , ⋯   , m i=1,2,\cdots,mi=1,2,⋯,m，计算负梯度
r t i = − [ ∂ L ( y i , f ( x i ) ) ) ∂ f ( x i ) ] f ( x ) = f t − 1 ( x )
rti=−[[∂L(yi,f(xi)))∂f(xi)]]f(x)=ft−1(x)
�
�
�
=
−
[
∂
�
(
�
�
,
�
(
�
�
)
)
)
∂
�
(
�
�
)
]
�
(
�
)
=
�
�
−
1
(
�
)
r 
ti
​
 =−[ 
∂f(x 
i
​
 )
∂L(y 
i
​
 ,f(x 
i
​
 )))
​
 ] 
f(x)=f 
t−1
​
 (x)
​
 
​
 

b) 利用( x i , r t i ) ( i = 1 , 2 , . . m ) (x_i,r_{ti})(i=1,2,..m)(x 
i
​
 ,r 
ti
​
 )(i=1,2,..m), 拟合一棵CART回归树,得到第t颗回归树，其对应的叶子节点区域为R t j , j = 1 , 2 , . . . , J R_{tj}, j =1,2,..., JR 
tj
​
 ,j=1,2,...,J。其中J为回归树t的叶子节点的个数。
c) 对叶子区域j =1,2,…J,计算最佳拟合值
c t j = a r g    m i n ⏟ c ∑ x i ∈ R t j L ( y i , f t − 1 ( x i ) + c ) c_{tj} = \underbrace{arg\; min}_{c}\sum\limits_{x_i \in R_{tj}} L(y_i,f_{t-1}(x_i) +c)
c 
tj
​
 = 
c
argmin
​
 
​
  
x 
i
​
 ∈R 
tj
​
 
∑
​
 L(y 
i
​
 ,f 
t−1
​
 (x 
i
​
 )+c)

d) 更新强学习器
f t ( x ) = f t − 1 ( x ) + ∑ j = 1 J c t j I ( x ∈ R t j ) f_{t}(x) = f_{t-1}(x) + \sum\limits_{j=1}^{J}c_{tj}I(x \in R_{tj})
f 
t
​
 (x)=f 
t−1
​
 (x)+ 
j=1
∑
J
​
 c 
tj
​
 I(x∈R 
tj
​
 )
得到强学习器f(x)的表达式
f ( x ) = f T ( x ) = f 0 ( x ) + ∑ t = 1 T ∑ j = 1 J c t j I ( x ∈ R t j ) f(x) = f_T(x) =f_0(x) + \sum\limits_{t=1}^{T}\sum\limits_{j=1}^{J}c_{tj}I(x \in R_{tj})
f(x)=f 
T
​
 (x)=f 
0
​
 (x)+ 
t=1
∑
T
​
  
j=1
∑
J
​
 c 
tj
​
 I(x∈R 
tj
​
 )


对(1)初始化的说明：
f 0 ( x ) = arg ⁡ min ⁡ c ∑ i = 1 N L ( y i , c ) f_0(x)=\arg\min_c\sum_{i=1}^NL(y_i,c)f 
0
​
 (x)=argmin 
c
​
 ∑ 
i=1
N
​
 L(y 
i
​
 ,c)
假设L且MSE，对它求导：
∂ ∑ i = 1 N ( y i − c ) 2 ∂ c = ∑ i = 1 N − 2 ( y i − c ) = ∑ i = 1 N ( 2 c − 2 y i ) = 2 N ⋅ c − 2 ∑ i = 1 N y i
∂∑Ni=1(yi−c)2∂c=∑i=1N−2(yi−c)=∑i=1N(2c−2yi)=2N⋅c−2∑i=1Nyi
∂
∑
�
=
1
�
(
�
�
−
�
)
2
∂
�
=
∑
�
=
1
�
−
2
(
�
�
−
�
)
=
∑
�
=
1
�
(
2
�
−
2
�
�
)
=
2
�
⋅
�
−
2
∑
�
=
1
�
�
�
∂c
∂∑ 
i=1
N
​
 (y 
i
​
 −c) 
2
 
​
 
​
  
= 
i=1
∑
N
​
 −2(y 
i
​
 −c)
= 
i=1
∑
N
​
 (2c−2y 
i
​
 )
=2N⋅c−2 
i=1
∑
N
​
 y 
i
​
 
​
 
令2 N ⋅ c − 2 ∑ i = 1 N y i = 0 → c = 1 N ∑ i = 1 N y i 2N\cdot c-2\sum_{i=1}^Ny_i=0\rightarrow c=\frac{1}{N}\sum_{i=1}^Ny_i2N⋅c−2∑ 
i=1
N
​
 y 
i
​
 =0→c= 
N
1
​
 ∑ 
i=1
N
​
 y 
i
​
 。

3.6 思路
3.6.1 个体学习器如何训练得到
改变训练数据的权值或者概率分布，如何改变?

拟合负梯度

3.6.2 如何将个体学习组合
简单组合

3.6.3 目标
L ( y ( i ) , f m ( x ( i ) ) ) < L ( y ( i ) , f m − 1 ( x ( i ) ) ) → L ( y ( i ) , f m − 1 ( x ( i ) ) ) − L ( y ( i ) , f m ( x ( i ) ) ) > 0
L(y(i),fm(x(i)))<L(y(i),fm−1(x(i)))→L(y(i),fm−1(x(i)))−L(y(i),fm(x(i)))>0
�
(
�
(
�
)
,
�
�
(
�
(
�
)
)
)
<
�
(
�
(
�
)
,
�
�
−
1
(
�
(
�
)
)
)
→
�
(
�
(
�
)
,
�
�
−
1
(
�
(
�
)
)
)
−
�
(
�
(
�
)
,
�
�
(
�
(
�
)
)
)
>
0
​
  
L(y 
(i)
 ,f 
m
​
 (x 
(i)
 ))<L(y 
(i)
 ,f 
m−1
​
 (x 
(i)
 ))→L(y 
(i)
 ,f 
m−1
​
 (x 
(i)
 ))−L(y 
(i)
 ,f 
m
​
 (x 
(i)
 ))>0
​
 

使得总体损失逐步减小

3.7 例题
L = 1 2 ( y − f m ( x ) ) 2 L=\frac{1}{2}(y-f_m(x))^2L= 
2
1
​
 (y−f 
m
​
 (x)) 
2
 

求负梯度：− ∂ L ∂ f m ( x ) = y − f m ( x ) = r m -\frac{\partial L}{\partial f_m(x)}=y-f_m(x)=r_m− 
∂f 
m
​
 (x)
∂L
​
 =y−f 
m
​
 (x)=r 
m
​
 

θ ^ m = arg ⁡ min ⁡ θ m ∑ i = 1 N L ( y ( i ) , f m − 1 ( x ( i ) ) + T ( x ( i ) , θ m ) ) = arg ⁡ min ⁡ θ m ∑ i = 1 N ( r m ( i ) − T ( x ( i ) , θ m ) )
θ^m=argminθm∑i=1NL(y(i),fm−1(x(i))+T(x(i),θm))=argminθm∑i=1N(r(i)m−T(x(i),θm))
�
^
�
=
arg
⁡
min
�
�
∑
�
=
1
�
�
(
�
(
�
)
,
�
�
−
1
(
�
(
�
)
)
+
�
(
�
(
�
)
,
�
�
)
)
=
arg
⁡
min
�
�
∑
�
=
1
�
(
�
�
(
�
)
−
�
(
�
(
�
)
,
�
�
)
)
θ
^
  
m
​
 
​
  
=arg 
θm
min
​
  
i=1
∑
N
​
 L(y 
(i)
 ,f 
m−1
​
 (x 
(i)
 )+T(x 
(i)
 ,θ 
m
​
 ))
=arg 
θm
min
​
  
i=1
∑
N
​
 (r 
m
(i)
​
 −T(x 
(i)
 ,θ 
m
​
 ))
​
 

3.8 GBDT的优缺点
3.8.1 GBDT主要的优点
1.可以灵活处理各种类型的数据，包括连续值和离散值。

2.在相对少的调参时间情况下，预测的准确率也可以比较高。这个是相对SVM来说的。

3.使用一些健壮的损失函数，对异常值的鲁棒性非常强。比如 Huber损失函数和Quantile损失函数。

3.8.2 GBDT的主要缺点
1.由于弱学习器之间存在依赖关系，难以并行训练数据。不过可以通过自采样的SGBT来达到部分并行。
4.GBDT解决二分类问题
4.1 面临的问题
GBDT使用基分类器是回归树，其加法模型无法直接输出类别或者概率预估。

4.2 解决方案
4.2.1 逻辑回归做二分类
以往直接用逻辑回归做二分类问题时的解决方案就是用一个sigmoid函数，将输出映射到0-1的概率空间：

Z = w 1 x 1 + w 2 x 2 + ⋯ + w n x n + b = w x + b y ^ = 1 1 + e − Z J = − 1 m ∑ i = 1 m [ y ( i ) log ⁡ y ^ ( i ) + ( 1 − y ( i ) log ⁡ ( 1 − y ( i ) ) ] , 交叉熵 Z=w_1x_1+w_2x_2+\cdots+w_nx_n+b=wx+b\\\hat y = \frac{1}{1+e^{-Z}}\\J=-\frac{1}{m}\sum_{i=1}^m \left[y^{(i)}\log \hat y^{(i)}+(1-y^{(i)}\log(1-y^{(i)})\right],交叉熵
Z=w 
1
​
 x 
1
​
 +w 
2
​
 x 
2
​
 +⋯+w 
n
​
 x 
n
​
 +b=wx+b
y
^
​
 = 
1+e 
−Z
 
1
​
 
J=− 
m
1
​
  
i=1
∑
m
​
 [y 
(i)
 log 
y
^
​
  
(i)
 +(1−y 
(i)
 log(1−y 
(i)
 )],交叉熵

4.2.2 学传统逻辑回归的解决方案
通过sigmoid函数，将加法模型f(x)映射到0~1的概率空间：

模仿线性模型（交叉熵损失）：
f m ( x ) = ∑ m = 1 M T ( x ; Θ m ) y ^ = 1 1 + e − f m ( x ) J = − 1 m ∑ i = 1 m [ y ( i ) log ⁡ y ^ ( i ) + ( 1 − y ( i ) log ⁡ ( 1 − y ( i ) ) ] f_m(x)=\sum_{m=1}^M T(x;\Theta_m)\\ \hat y= \frac{1}{1+e^{-f_m(x)}}\\J=-\frac{1}{m}\sum_{i=1}^m \left[y^{(i)}\log \hat y^{(i)}+(1-y^{(i)}\log(1-y^{(i)})\right]
f 
m
​
 (x)= 
m=1
∑
M
​
 T(x;Θ 
m
​
 )
y
^
​
 = 
1+e 
−f 
m
​
 (x)
 
1
​
 
J=− 
m
1
​
  
i=1
∑
m
​
 [y 
(i)
 log 
y
^
​
  
(i)
 +(1−y 
(i)
 log(1−y 
(i)
 )]

4.3 模型
使用加法模型：
T ( x ; Θ ) = ∑ j = 1 J c j I ( x ∈ R j ) f M ( x ) = ∑ m = 1 M T ( x ; Θ m ) T(x;\Theta)=\sum_{j=1}^Jc_jI(x\in R_j)\\f_M(x)=\sum_{m=1}^MT(x;\Theta_m)
T(x;Θ)= 
j=1
∑
J
​
 c 
j
​
 I(x∈R 
j
​
 )
f 
M
​
 (x)= 
m=1
∑
M
​
 T(x;Θ 
m
​
 )

，其中T TT前面有时有一个学习率当系数，是为了解决过拟合。

4.4 基学习器
真实值y yy与整个加法模型的损失，所以要将交叉熵公式转化为y yy和f m ( x ) f_m(x)f 
m
​
 (x)，这也是因为残差是损失函数的负梯度。
最终的损失如下：
L ( y , f m ( x ) ) = log ⁡ ( 1 + e − f m ( x ) ) + ( 1 − y ) ⋅ f m ( x ) L(y,f_m(x))=\log(1+e^{-f_m(x)})+(1-y)\cdot f_m(x)
L(y,f 
m
​
 (x))=log(1+e 
−f 
m
​
 (x)
 )+(1−y)⋅f 
m
​
 (x)

推导过程：
y ^ = 1 1 + e − f m ( x ) \hat y= \frac{1}{1+e^{-f_m(x)}} 
y
^
​
 = 
1+e 
−f 
m
​
 (x)
 
1
​
 

L = − y log ⁡ y ^ − ( 1 − y ) log ⁡ ( 1 − y ^ ) = − y log ⁡ 1 1 + e − f m ( x ) − ( 1 − y ) log ⁡ ( 1 − 1 1 + e − f m ( x ) ) = y log ⁡ ( 1 + e − f m ( x ) ) − ( 1 − y ) [ log ⁡ e − f m ( x ) − log ⁡ ( 1 + e − f m ( x ) ) ] = y log ⁡ ( 1 + e − f m ( x ) ) − ( 1 − y ) [ − f m ( x ) − log ⁡ ( 1 + e − f m ( x ) ) ] = y log ⁡ ( 1 + e − f m ( x ) ) + ( 1 − y ) f m ( x ) + ( 1 − y ) log ⁡ ( 1 + e − f m ( x ) ) = log ⁡ ( 1 + e − f m ( x ) ) + ( 1 − y ) f m ( x )
L=−ylogy^−(1−y)log(1−y^)=−ylog11+e−fm(x)−(1−y)log(1−11+e−fm(x))=ylog(1+e−fm(x))−(1−y)[loge−fm(x)−log(1+e−fm(x))]=ylog(1+e−fm(x))−(1−y)[−fm(x)−log(1+e−fm(x))]=ylog(1+e−fm(x))+(1−y)fm(x)+(1−y)log(1+e−fm(x))=log(1+e−fm(x))+(1−y)fm(x)
�
=
−
�
log
⁡
�
^
−
(
1
−
�
)
log
⁡
(
1
−
�
^
)
=
−
�
log
⁡
1
1
+
�
−
�
�
(
�
)
−
(
1
−
�
)
log
⁡
(
1
−
1
1
+
�
−
�
�
(
�
)
)
=
�
log
⁡
(
1
+
�
−
�
�
(
�
)
)
−
(
1
−
�
)
[
log
⁡
�
−
�
�
(
�
)
−
log
⁡
(
1
+
�
−
�
�
(
�
)
)
]
=
�
log
⁡
(
1
+
�
−
�
�
(
�
)
)
−
(
1
−
�
)
[
−
�
�
(
�
)
−
log
⁡
(
1
+
�
−
�
�
(
�
)
)
]
=
�
log
⁡
(
1
+
�
−
�
�
(
�
)
)
+
(
1
−
�
)
�
�
(
�
)
+
(
1
−
�
)
log
⁡
(
1
+
�
−
�
�
(
�
)
)
=
log
⁡
(
1
+
�
−
�
�
(
�
)
)
+
(
1
−
�
)
�
�
(
�
)
L
​
  
=−ylog 
y
^
​
 −(1−y)log(1− 
y
^
​
 )
=−ylog 
1+e 
−f 
m
​
 (x)
 
1
​
 −(1−y)log(1− 
1+e 
−f 
m
​
 (x)
 
1
​
 )
=ylog(1+e 
−f 
m
​
 (x)
 )−(1−y)[loge 
−f 
m
​
 (x)
 −log(1+e 
−f 
m
​
 (x)
 )]
=ylog(1+e 
−f 
m
​
 (x)
 )−(1−y)[−f 
m
​
 (x)−log(1+e 
−f 
m
​
 (x)
 )]
=ylog(1+e 
−f 
m
​
 (x)
 )+(1−y)f 
m
​
 (x)+(1−y)log(1+e 
−f 
m
​
 (x)
 )
=log(1+e 
−f 
m
​
 (x)
 )+(1−y)f 
m
​
 (x)
​
 

4.4.1 前向分步算法+梯度提升
1.核心目标
L ( y ( i ) , f m ( x ( i ) ) ) < L ( y ( i ) , f m − 1 ( x ( i ) ) ) → L ( y ( i ) , f m − 1 ( x ( i ) ) ) − L ( y ( i ) , f m ( x ( i ) ) ) > 0
L(y(i),fm(x(i)))<L(y(i),fm−1(x(i)))→L(y(i),fm−1(x(i)))−L(y(i),fm(x(i)))>0
�
(
�
(
�
)
,
�
�
(
�
(
�
)
)
)
<
�
(
�
(
�
)
,
�
�
−
1
(
�
(
�
)
)
)
→
�
(
�
(
�
)
,
�
�
−
1
(
�
(
�
)
)
)
−
�
(
�
(
�
)
,
�
�
(
�
(
�
)
)
)
>
0
​
  
L(y 
(i)
 ,f 
m
​
 (x 
(i)
 ))<L(y 
(i)
 ,f 
m−1
​
 (x 
(i)
 ))→L(y 
(i)
 ,f 
m−1
​
 (x 
(i)
 ))−L(y 
(i)
 ,f 
m
​
 (x 
(i)
 ))>0
​
 

2.梯度提升
1.计算当前损失函数的负梯度表达式

L ( y , f m ( x ) ) = log ⁡ ( 1 + e − f m ( x ) ) + ( 1 − y ) ⋅ f m ( x ) L(y,f_m(x))=\log(1+e^{-f_m(x)})+(1-y)\cdot f_m(x)L(y,f 
m
​
 (x))=log(1+e 
−f 
m
​
 (x)
 )+(1−y)⋅f 
m
​
 (x)
∂ L ( y , f m ( x ) ) ∂ f m ( x ) = ∂ [ log ⁡ ( 1 + e − f m ( x ) ) + ( 1 − y ) ⋅ f m ( x ) ] ∂ f m ( x ) = ∂ log ⁡ ( 1 + e − f m ( x ) ) ∂ f m ( x ) + ∂ ( 1 − y ) ⋅ f m ( x ) ∂ f m ( x ) = − e − f m ( x ) 1 + e − f m ( x ) + 1 − y = − e − f m ( x ) + ( 1 − y ) ( 1 + e − f m ( x ) ) 1 + e − f m ( x ) = 1 − ( 1 + e − f m ( x ) ) y 1 + e − f m ( x ) = 1 1 + e − f m ( x ) − y
∂L(y,fm(x))∂fm(x)=∂[log(1+e−fm(x))+(1−y)⋅fm(x)]∂fm(x)=∂log(1+e−fm(x))∂fm(x)+∂(1−y)⋅fm(x)∂fm(x)=−e−fm(x)1+e−fm(x)+1−y=−e−fm(x)+(1−y)(1+e−fm(x))1+e−fm(x)=1−(1+e−fm(x))y1+e−fm(x)=11+e−fm(x)−y
∂
�
(
�
,
�
�
(
�
)
)
∂
�
�
(
�
)
=
∂
[
log
⁡
(
1
+
�
−
�
�
(
�
)
)
+
(
1
−
�
)
⋅
�
�
(
�
)
]
∂
�
�
(
�
)
=
∂
log
⁡
(
1
+
�
−
�
�
(
�
)
)
∂
�
�
(
�
)
+
∂
(
1
−
�
)
⋅
�
�
(
�
)
∂
�
�
(
�
)
=
−
�
−
�
�
(
�
)
1
+
�
−
�
�
(
�
)
+
1
−
�
=
−
�
−
�
�
(
�
)
+
(
1
−
�
)
(
1
+
�
−
�
�
(
�
)
)
1
+
�
−
�
�
(
�
)
=
1
−
(
1
+
�
−
�
�
(
�
)
)
�
1
+
�
−
�
�
(
�
)
=
1
1
+
�
−
�
�
(
�
)
−
�
∂f 
m
​
 (x)
∂L(y,f 
m
​
 (x))
​
 
​
  
= 
∂f 
m
​
 (x)
∂[log(1+e 
−f 
m
​
 (x)
 )+(1−y)⋅f 
m
​
 (x)]
​
 
= 
∂f 
m
​
 (x)
∂log(1+e 
−f 
m
​
 (x)
 )
​
 + 
∂f 
m
​
 (x)
∂(1−y)⋅f 
m
​
 (x)
​
 
=− 
1+e 
−f 
m
​
 (x)
 
e 
−f 
m
​
 (x)
 
​
 +1−y
= 
1+e 
−f 
m
​
 (x)
 
−e 
−f 
m
​
 (x)
 +(1−y)(1+e 
−f 
m
​
 (x)
 )
​
 
= 
1+e 
−f 
m
​
 (x)
 
1−(1+e 
−f 
m
​
 (x)
 )y
​
 
= 
1+e 
−f 
m
​
 (x)
 
1
​
 −y
​
 

2.构造新的训练样本

将( x i , y i ) (x_i,y_i)(x 
i
​
 ,y 
i
​
 )代入r m ( x , y ) r_m(x,y)r 
m
​
 (x,y)即可得到r m 1 r_{m1}r 
m1
​
 ，进而得到第m轮的训练数据集：
T m = { ( x 1 , r m 1 ) , ( x 2 , r m 2 ) , ⋯   , ( x N , r m N ) } T_m=\{(x_1,r_{m1}),(x_2,r_{m2}),\cdots,(x_N,r_{mN})\}
T 
m
​
 ={(x 
1
​
 ,r 
m1
​
 ),(x 
2
​
 ,r 
m2
​
 ),⋯,(x 
N
​
 ,r 
mN
​
 )}

r m ( x , y ) = − [ ∂ L ( y , f m ( x ) ) ∂ f m ( x ) ] f m ( x ) = f m − 1 ( x ) = − [ 1 1 + e − f m − 1 ( x ) − y ] = y − y ^ m − 1 r_m(x,y)=-\left[\frac{\partial L(y,f_m(x))}{\partial f_m(x)}\right]_{f_m(x)=f_{m-1}(x)}=-\left[\frac{1}{1+e^{-f_{m-1}(x)}}-y\right]=y-\hat y_{m-1}
r 
m
​
 (x,y)=−[ 
∂f 
m
​
 (x)
∂L(y,f 
m
​
 (x))
​
 ] 
f 
m
​
 (x)=f 
m−1
​
 (x)
​
 =−[ 
1+e 
−f 
m−1
​
 (x)
 
1
​
 −y]=y− 
y
^
​
  
m−1
​
 

即：r m i = y i − y ^ m − 1 , i r_{mi}=y_i-\hat y_{m-1,i}r 
mi
​
 =y 
i
​
 − 
y
^
​
  
m−1,i
​
 。

3.让当前的回归树拟合上述训练样本，得到T ( x ; Θ m ) T(x;\Theta_m)T(x;Θ 
m
​
 )。

3.面临问题
1.如何构造回归树T ( x ; Θ m ) T(x;\Theta_m)T(x;Θ 
m
​
 )

1.树的深度如何决定
2.划分节点如何选取
3.叶子节点代表的值c m c_mc 
m
​
 如何定
2.如何衡量T ( x ; Θ m ) T(x;\Theta_m)T(x;Θ 
m
​
 )对残差（负梯度）的拟合效果？
使用传统的损失函数无法达到最优效果，使用总体损失又缺乏闭式解。
划分方式→ \rightarrow→损失函数：
1 n ∑ i = 1 n ( f ( x i ) − r m i ) \frac{1}{n}\sum_{i=1}^n(f(x_i)-r_{mi})
n
1
​
  
i=1
∑
n
​
 (f(x 
i
​
 )−r 
mi
​
 )
，
c m j ∗ = 1 N m j ∑ x i ∈ R m j r m i c_{mj}^*=\frac{1}{N_{mj}}\sum_{x_i\in R_{mj}}r_{mi}
c 
mj
∗
​
 = 
N 
mj
​
 
1
​
  
x 
i
​
 ∈R 
mj
​
 
∑
​
 r 
mi
​
 

划分方式→ \rightarrow→ 负梯度拟合效果 ⇔ \Leftrightarrow⇔损失函数：总体损失
∑ x i ∈ R m j [ log ⁡ ( 1 + e − f m ( x i ) ) + ( 1 − y i ) ⋅ f m ( x i ) ] \sum_{x_i\in R_{mj}}\left[\log(1+e^{-f_m(x_i)})+(1-y_i)\cdot f_m(x_i)\right]
x 
i
​
 ∈R 
mj
​
 
∑
​
 [log(1+e 
−f 
m
​
 (x 
i
​
 )
 )+(1−y 
i
​
 )⋅f 
m
​
 (x 
i
​
 )]
，
c m j ∗ = arg ⁡ min ⁡ ∑ x i ∈ R m j L ( y i , f m − 1 ( x i ) + c m j ) c_{mj}^*=\arg\min\sum_{x_i\in R_{mj}}L(y_i,f_{m-1}(x_i)+c_{mj})
c 
mj
∗
​
 =argmin 
x 
i
​
 ∈R 
mj
​
 
∑
​
 L(y 
i
​
 ,f 
m−1
​
 (x 
i
​
 )+c 
mj
​
 )

为了得到闭式解：

使用总体损失的大小来衡量负梯度的拟合效果，是最好的但是该优化无法得到闭式解。通过泰勒二阶展开，得到闭式解：
C m j ∗ = ∑ x i ∈ R m j r m i ∑ x i ∈ R m j ( y i − r m i ) ( 1 − y i + r m i ) C_{mj}^*=\frac{\sum_{x_i\in R_{mj}}r_{mi}}{\sum_{x_i\in R_{mj}}(y_i-r_{mi})(1-y_i+r_{mi})}
C 
mj
∗
​
 = 
∑ 
x 
i
​
 ∈R 
mj
​
 
​
 (y 
i
​
 −r 
mi
​
 )(1−y 
i
​
 +r 
mi
​
 )
∑ 
x 
i
​
 ∈R 
mj
​
 
​
 r 
mi
​
 
​
 

推导过程：

二阶泰勒展开：
f ( x ) ≈ f ( x 0 ) + f ′ ( x 0 ) ( x − x 0 ) + 1 2 f " ( x 0 ) ( x − x 0 ) 2 f(x)\approx f(x_0)+f^{'}(x_0)(x-x_0)+\frac{1}{2}f^{"}(x_0)(x-x_0)^2
f(x)≈f(x 
0
​
 )+f 
′
 
 (x 
0
​
 )(x−x 
0
​
 )+ 
2
1
​
 f 
"
 (x 
0
​
 )(x−x 
0
​
 ) 
2
 

对L ( y i , f m − 1 ( x i ) + C m j ) L(y_i,f_{m-1}(x_i)+C_{mj})L(y 
i
​
 ,f 
m−1
​
 (x 
i
​
 )+C 
mj
​
 )进行二阶泰勒展开，其中x i ∈ R m j x_i\in R_{mj}x 
i
​
 ∈R 
mj
​
 
L ( y i , f m − 1 ( x i ) + C m j ) ≈ L ( y i , f m − 1 ( x i ) ) + ∂ L ( y i , f m ( x i ) ) ∂ f m ( x i ) ∣ f m ( x i ) = f m − 1 ( x i ) ⋅ C m j + ∂ 2 L ( y i , f m ( x i ) ) ∂ f m ( x i ) 2 ∣ f m ( x i ) = f m − 1 ( x i ) ⋅ C m j 2 = L ( y , f m − 1 ( x ) ) − + （ y ^ m − 1 , i − y i ） C m j + 1 2 y ^ m − 1 , i ( 1 − y ^ m − 1 , i ) C m j 2 , 其中 x i ∈ R m j
L(yi,fm−1(xi)+Cmj)≈L(yi,fm−1(xi))+∂L(yi,fm(xi))∂fm(xi)|fm(xi)=fm−1(xi)⋅Cmj+∂2L(yi,fm(xi))∂fm(xi)2|fm(xi)=fm−1(xi)⋅C2mj=L(y,fm−1(x))−+（y^m−1,i−yi）Cmj+12y^m−1,i(1−y^m−1,i)C2mj,其中xi∈Rmj
�
(
�
�
,
�
�
−
1
(
�
�
)
+
�
�
�
)
≈
�
(
�
�
,
�
�
−
1
(
�
�
)
)
+
∂
�
(
�
�
,
�
�
(
�
�
)
)
∂
�
�
(
�
�
)
|
�
�
(
�
�
)
=
�
�
−
1
(
�
�
)
⋅
�
�
�
+
∂
2
�
(
�
�
,
�
�
(
�
�
)
)
∂
�
�
(
�
�
)
2
|
�
�
(
�
�
)
=
�
�
−
1
(
�
�
)
⋅
�
�
�
2
=
�
(
�
,
�
�
−
1
(
�
)
)
−
+
（
�
^
�
−
1
,
�
−
�
�
）
�
�
�
+
1
2
�
^
�
−
1
,
�
(
1
−
�
^
�
−
1
,
�
)
�
�
�
2
,
其
中
�
�
∈
�
�
�
L(y 
i
​
 ,f 
m−1
​
 (x 
i
​
 )+C 
mj
​
 )
​
  
≈L(y 
i
​
 ,f 
m−1
​
 (x 
i
​
 ))+ 
∂f 
m
​
 (x 
i
​
 )
∂L(y 
i
​
 ,f 
m
​
 (x 
i
​
 ))
​
 ∣ 
f 
m
​
 (x 
i
​
 )=f 
m−1
​
 (x 
i
​
 )
​
 ⋅C 
mj
​
 + 
∂f 
m
​
 (x 
i
​
 ) 
2
 
∂ 
2
 L(y 
i
​
 ,f 
m
​
 (x 
i
​
 ))
​
 ∣ 
f 
m
​
 (x 
i
​
 )=f 
m−1
​
 (x 
i
​
 )
​
 ⋅C 
mj
2
​
 
=L(y,f 
m−1
​
 (x))−+（ 
y
^
​
  
m−1,i
​
 −y 
i
​
 ）C 
mj
​
 + 
2
1
​
  
y
^
​
  
m−1,i
​
 (1− 
y
^
​
  
m−1,i
​
 )C 
mj
2
​
 ,其中x 
i
​
 ∈R 
mj
​
 
​
 

因此有：
C m j ∗ = arg ⁡ min ⁡ ∑ x i ∈ R m j L ( y i , f m − 1 ( x i ) + C m j ) C_{mj}^*=\arg\min\sum_{x_i\in R_{mj}}L(y_i,f_{m-1}(x_i)+C_{mj})
C 
mj
∗
​
 =argmin 
x 
i
​
 ∈R 
mj
​
 
∑
​
 L(y 
i
​
 ,f 
m−1
​
 (x 
i
​
 )+C 
mj
​
 )

∑ x i ∈ R m j L ( y i , f m − 1 ( x i ) + C m j ) = ∑ x i ∈ R m j [ L ( y , f m − 1 ( x ) ) + （ y ^ m − 1 , i − y i ） C m j + 1 2 y ^ m − 1 , i ( 1 − y ^ m − 1 , i ) C m j 2 ] , 其中 x i ∈ R m j = ∑ x i ∈ R m j L ( y , f m − 1 ( x ) ) + N m j C m j ∑ x i ∈ R m j ( y ^ m − 1 , i − y i ) + 1 2 N m j C m j 2 ∑ x i ∈ R m j ( 1 − y ^ m − 1 , i ) y ^ m − 1 , i
∑xi∈RmjL(yi,fm−1(xi)+Cmj)=∑xi∈Rmj[L(y,fm−1(x))+（y^m−1,i−yi）Cmj+12y^m−1,i(1−y^m−1,i)C2mj],其中xi∈Rmj=∑xi∈RmjL(y,fm−1(x))+NmjCmj∑xi∈Rmj(y^m−1,i−yi)+12NmjC2mj∑xi∈Rmj(1−y^m−1,i)y^m−1,i
∑
�
�
∈
�
�
�
�
(
�
�
,
�
�
−
1
(
�
�
)
+
�
�
�
)
=
∑
�
�
∈
�
�
�
[
�
(
�
,
�
�
−
1
(
�
)
)
+
（
�
^
�
−
1
,
�
−
�
�
）
�
�
�
+
1
2
�
^
�
−
1
,
�
(
1
−
�
^
�
−
1
,
�
)
�
�
�
2
]
,
其
中
�
�
∈
�
�
�
=
∑
�
�
∈
�
�
�
�
(
�
,
�
�
−
1
(
�
)
)
+
�
�
�
�
�
�
∑
�
�
∈
�
�
�
(
�
^
�
−
1
,
�
−
�
�
)
+
1
2
�
�
�
�
�
�
2
∑
�
�
∈
�
�
�
(
1
−
�
^
�
−
1
,
�
)
�
^
�
−
1
,
�
x 
i
​
 ∈R 
mj
​
 
∑
​
 L(y 
i
​
 ,f 
m−1
​
 (x 
i
​
 )+C 
mj
​
 )
​
  
= 
x 
i
​
 ∈R 
mj
​
 
∑
​
 [L(y,f 
m−1
​
 (x))+（ 
y
^
​
  
m−1,i
​
 −y 
i
​
 ）C 
mj
​
 + 
2
1
​
  
y
^
​
  
m−1,i
​
 (1− 
y
^
​
  
m−1,i
​
 )C 
mj
2
​
 ],其中x 
i
​
 ∈R 
mj
​
 
= 
x 
i
​
 ∈R 
mj
​
 
∑
​
 L(y,f 
m−1
​
 (x))+N 
mj
​
 C 
mj
​
  
x 
i
​
 ∈R 
mj
​
 
∑
​
 ( 
y
^
​
  
m−1,i
​
 −y 
i
​
 )+ 
2
1
​
 N 
mj
​
 C 
mj
2
​
  
x 
i
​
 ∈R 
mj
​
 
∑
​
 (1− 
y
^
​
  
m−1,i
​
 ) 
y
^
​
  
m−1,i
​
 
​
 

式中只有C m j C_{mj}C 
mj
​
 为变量，且是一个一元二次方程，( 1 − y ^ m − 1 , i ) y ^ m − 1 , i > 0 (1-\hat y_{m-1,i})\hat y_{m-1,i}>0(1− 
y
^
​
  
m−1,i
​
 ) 
y
^
​
  
m−1,i
​
 >0，开口向上，有最小值当且仅当：
C m j ∗ = − b 2 a = − N m j ⋅ ∑ x i ∈ R m j ( y ^ m − 1 , i − y i ) N m j ⋅ ∑ x i ∈ R m j ( 1 − y ^ m − 1 , i ) y ^ m − 1 , i = − ∑ x i ∈ R m j ( y ^ m − 1 , i − y i ) ∑ x i ∈ R m j ( 1 − y ^ m − 1 , i ) y ^ m − 1 , i , 因为 r m i = y i − y ^ m − 1 , i = ∑ x i ∈ R m j r m i ∑ x i ∈ R m j ( y i − r m i ) ( 1 − y i + r m i )
C∗mj=−b2a=−Nmj⋅∑xi∈Rmj(y^m−1,i−yi)Nmj⋅∑xi∈Rmj(1−y^m−1,i)y^m−1,i=−∑xi∈Rmj(y^m−1,i−yi)∑xi∈Rmj(1−y^m−1,i)y^m−1,i,因为rmi=yi−y^m−1,i=∑xi∈Rmjrmi∑xi∈Rmj(yi−rmi)(1−yi+rmi)
�
�
�
∗
=
−
�
2
�
=
−
�
�
�
⋅
∑
�
�
∈
�
�
�
(
�
^
�
−
1
,
�
−
�
�
)
�
�
�
⋅
∑
�
�
∈
�
�
�
(
1
−
�
^
�
−
1
,
�
)
�
^
�
−
1
,
�
=
−
∑
�
�
∈
�
�
�
(
�
^
�
−
1
,
�
−
�
�
)
∑
�
�
∈
�
�
�
(
1
−
�
^
�
−
1
,
�
)
�
^
�
−
1
,
�
,
因
为
�
�
�
=
�
�
−
�
^
�
−
1
,
�
=
∑
�
�
∈
�
�
�
�
�
�
∑
�
�
∈
�
�
�
(
�
�
−
�
�
�
)
(
1
−
�
�
+
�
�
�
)
C 
mj
∗
​
 
​
  
=− 
2a
b
​
 =− 
N 
mj
​
 ⋅∑ 
x 
i
​
 ∈R 
mj
​
 
​
 (1− 
y
^
​
  
m−1,i
​
 ) 
y
^
​
  
m−1,i
​
 
N 
mj
​
 ⋅∑ 
x 
i
​
 ∈R 
mj
​
 
​
 ( 
y
^
​
  
m−1,i
​
 −y 
i
​
 )
​
 
=− 
∑ 
x 
i
​
 ∈R 
mj
​
 
​
 (1− 
y
^
​
  
m−1,i
​
 ) 
y
^
​
  
m−1,i
​
 
∑ 
x 
i
​
 ∈R 
mj
​
 
​
 ( 
y
^
​
  
m−1,i
​
 −y 
i
​
 )
​
 ,因为r 
mi
​
 =y 
i
​
 − 
y
^
​
  
m−1,i
​
 
= 
∑ 
x 
i
​
 ∈R 
mj
​
 
​
 (y 
i
​
 −r 
mi
​
 )(1−y 
i
​
 +r 
mi
​
 )
∑ 
x 
i
​
 ∈R 
mj
​
 
​
 r 
mi
​
 
​
 
​
 

4.结论
通过不同的划分方式得到多棵树，然后计算总体损失，最后选择损失最小的那颗树。

1.使用总体损失对回归树进行优化，计算过于复杂。
2.两种方法得出的划分方式（即树的结构）是一样的。
于是：
1.使用传统回归树构建好回归树的结构（就是用MSE计算损失）。
2.再使用总体损失中的方式计算树的叶子节点中的c m j c_{mj}c 
mj
​
 。
————————————————
版权声明：本文为CSDN博主「joejoeqian」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/joejoeqian/article/details/129960333