---
published: false
layout: post
title: "MAE论文精读"
categories: 我的AI新书
date: 2023-09-21 00:00:00 +0800
excerpt: "MAE论文精读"
---


MAE



masked autoencoders are scalable vision leanrners



2021年11月11日提交到arxiv





与之前读的文章的关系



transformer

它是一个纯基于注意力机制的编码器和解码器
在机器翻译任务上，它比基于RNN的架构要更好一些


BERT

它使用一个transformer编码器，拓展到了更一般的NLP任务上
它使用了完形填空的自监督的训练机制，这样就不需要使用标号，而是通过预测一个句子里面哪些词不见了，从而获取对文本特征抽取的能力
BERT极大地扩展了transformer的应用，可以在一个大规模的、没有标号的数据上训练出非常好的模型出来


ViT

这个模型可以认为就是将transformer用到CV上面
具体来讲，就是将整个图片分割成很多个16*16的小方块，然后每个方块做成一个次，然后放进transformer进行训练
ViT这篇文章证明：假设训练数据足够大的时候比如说用1000万或者1亿的训练样本，它相对于CNN的架构来说，transformer的架构在精度上可能会更高一点


MAE

可以认为是BERT的CV版
它是基于ViT这篇文章，但是它把整个训练拓展到没有标号的数据上面，也就是跟BERT一样通过完形填空来获取对于图片的理解
MAE并不是第一个将BERT拓展到CV上的工作，但是MAE很有可能是这一系列工作之中未来影响力最大的一篇（因为BERT这篇文章极大地加速了transformer这个架构在NLP里面的应用，所以MAE很有可能使得transformer在CV上的应用更加普及）








1、标题+作者



masked autoencoder are scalable vision learners



带掩码的自编码器是一个可拓展的视觉学习器

scalable：可拓展的
vision learner：这里没有写成classifier或者其他的东西，因为它能够用到的地方相对广一些，他是一个backbone模型
masked：在BERT的时候，masked language model就是一个带掩码的语言模型（完形填空），这里使用masked来源于BERT，每次挖掉一些东西然后去预测被挖掉的东西
autoencoder：在transformer和BERT中，使用的都是encoder，前面没有加上auto，这里的auto不是自动的意思，而是“自”的意思。在机器学习中有一类模型叫做auto自模型，比如自回归模型，这一大类模型的特点是，标号和样本（y和x）来自于同一个东西（比如说在语言模型中，每一次用前面的次去预测下一个词，每一个词、标号、或者y都是来自同样的句子里面的词所以叫做auto）。在NLP中，语言模型是它的一大类模型，所以说不说auto，大家都是可以理解的，但是在计算机视觉中，这样的任务比较少，因为图片的标号很少来自图片本身，标号很多时候是文本而图片是像素，但是编码器、解码器在图片里面用的非常多，所以作者在这里加上了auto，意在指出和计算机视觉中其他的encoder相比，这里的标号也就是图片本身，这样能跟之前的很多工作区分开来


在写论文的时候假设算法特别快的话，就把标题放在efficient，假设做的东西比较大的话，就叫做scalable，二选一



标题的模板



什么是一个好同志

zaiGPT系列中，三篇文章使用的都是同样的句式
这个句式是一个非常强有力的句式，它基本上就是将结论放在了title中，而且这种句式相对来讲比较客观，它将工作的结论浓缩成一句简单的话




作者



何凯明

ResNet的一作


他们都是来自facebook的AI研究院（FAIR）



*表示同样贡献



project lead：可以认为是项目的发起人或者是管理人，带领大家做什么样的想法，然后从公司拿到资源来做这个事情。这是比较少见的将作者标记为project lead，一般project lead可能是最后一个作者，大老板









2、摘要



这篇文章展示了masked autoencoder是一个scalable self-supervised learner for computer vision

这里是标题的拓展
同时写出了名字MAE


MAE的途径非常简单，随机地盖住图片中地一些块，然后再去重构这些被盖住的像素，这个思想来自于BERT中的带掩码的语言模型，但是不同之处在于patch是来自图片的一个小块，预测的是这个块中所有的像素



两个核心的设计

1、非对称的encoder-decoder架构

虽然是一个自编码器架构，但是实际上任何模型都有一个编码器和一个解码器，比如说在BERT中的解码器就是最后一个全连接输出层，因为BERT预测的东西相对来讲比较简单，所以它的解码器就是一个简单的全连接层就可以了，但是本文中可能相对复杂一点，因为需要预测一个块中所有的像素。
编码器之作用在可见的patch中，对于丢掉的patch，编码器不会对它进行编码，这样能够节省一定的计算时间
解码器是一个比较轻量的解码器，能够重构原始的图片
2、如果发现遮住了大量的块（比如说把75%的块全部遮住），则会得到一个非显然的而且有意义的自监督任务。如果只是遮住几块的话，只需要进行插值就可以出来了，模型可能学不到特别的东西，但是如果将一大半都遮住的话，就会迫使模型去学习一些更好的特征

将上述两个设计放在一起，可以更有效地训练大的模型

大是说构造一个比较有挑战性的任务，他不是去学习一个很显然的解
快是说，对于被遮住的块就不看了，假设盖住了不去看的话，那么训练量就是之前的1/4，所以能加速3倍或者以上


结果：只使用ImageNet-1K（100万张图片）的数据，在一个最简单的ViT-Huge的模型中（模型来自ViT这篇文章）能够得到87.8%的准确率

在ViT这篇文章的最后一段有提到过怎样做自监督学习，作者说效果并不是很好，所以没有展开，作者当时的结论是：用有标号的模型和更大的训练集可以得到很好的效果
这里的挑战在于，只使用小的数据集（ImageNet-1k，100万张图片），而且是通过自监督，就能够做到跟之前可能效果一样好的模型了


这个模型主要是用来做迁移学习，它证明了在迁移学习的其他任务上表现也非常好









3、关键图



对于计算机视觉的文章来说，最重要的一张图就是放在第一页右上角的那张




图一讲的是MAE整个模型的架构
输入一张图，首先将它切成一个一个的小块，图中涂成灰色的那些小块就是被盖住的部分，从没有盖住的部分可以看出，原图应该是一只红色的鸟
去掉那些被盖住的块，剩下的小块已经不太多了，因为3/4的地方都已经被盖住了
将剩下的小块放进encoder（也就是vit）中，得到每一个块对应的特征，然后将其拉长，再把被盖住的那些块重新放回到原来的位置，也就是说这个被拉长的东西其实就是原始图片拉成的一条向量（其中没有盖住的部分就填上经过vit之后输出的特征，被盖住的地方依然是灰色的，这里其实就是一个位置信息，没有其他的可以表示的）
将得到的长向量输入到解码器中，解码器会尝试将里面的像素信息全部重构回来，使得最后的target训练出来就是原始的没有被掩码盖住的图片
这里的编码器画的稍微比解码器要高一点，这里的意思是说模型主要的计算来自于编码器，因为最重要的就是对图片的像素进行编码（编码器只需要处理没有被盖住的图片，也就是说一张图片其实只需要看到1/4的像素就行了，这样可以降低计算量，transformer的模型计算量都特别大，如果能够达到几倍的加速，其实也是很重要的事情），这是在预训练的时候所干的事情
如果想用这个模型来处理一个计算机任务的话，就只需要编码器就够了，解码器是不需要的，图片进来之后不需要对它做掩码，直接切成一个一个的小块，然后放进vit，就能得到所有块的特征的表示，这就可以当作是输入图片的特征表达，然后用来做所要处理的任务



图2演示的是在ImageNet的验证集上通过MAE构造出来的图片，因为这些图片是没有参与训练的，所以只是测试结果
每张图片中，左边一列演示的是将图片80%的块去掉，中间列演示的是MAE构造出来的图片效果，最右边一列演示的是原始的真实的ImageNet'的图片
第一行第一张图片里的时钟上的指针基本上被遮住了，但是还原的时候还是基本上将两个指针整个还原出来了
第二行第一张图片中车的两头都没有，但是还原的效果跟真实的车的效果还是非常一致
第三行第一张图中的狗也是基本上看不出来狗在什么地方，但是还是能够大概的还原出来
这里的重构虽然在细节上比较模糊，因为原始图片的尺寸也不是很大，但是对图片内容的重构效果还是非常惊人的
MAE不一定对所有图片的重构都有很好的效果，这里可能只是挑选出来了一些比较好的样例做了展示。如果想要将任意打码的图片还原出来，后续可能还需要很多工作进行改进



图三是在COCO数据集上，跟之前的处理完全一样，只是数据集不同，但是效果上来说也是非常惊人



图四演示的是对同一张图片遮盖不同比例的区域时的结果
最后一列是95%的区域被盖住的还原效果








4、结论



简单且拓展很好的算法是整个深度学习的核心

简单：在vit的基础上，本文所提出来的东西相对来说比较简单
拓展性很好：能够跑比较大的数据集
整个MAE是基于vit，vit是基于transformer，就整个模型来说当然是不简单的，每一个transformer块中就有很多的东西，然后还要用很多层的transformer堆叠起来，当然是一个比较复杂的算法
而且扩展性好是说在很有钱的时候，当然是可以无限地往里面加数据，因为毕竟不需要标号
所以说这里的简单和拓展性好只是针对一小群人的


自监督学习在最近几年是比较火的，但是在计算机视觉中，还是主要靠有标号的数据作为训练



这篇文章的工作通过在ImageNet数据集上通过自编码器学习到可以媲美有标号的时候的效果，所以是一个非常强有力的工作



需要注意到的是图片和语言之间的一些区别，因为本文可以认为是BERT在计算机视觉上的拓展

首先，对于语言来说，一个词是一个语义的单元，它里面所包含的语义信息比较多(这点在vit中已经被提出来了)，在图片中虽然一个patch也含有一定的语义信息，但它不是语义的分割（也就是说这个patch中并不含有特定的物体，可能含有多个物体的一小块，或者是某一个物体重叠的一块），但是即使是在这样的情况下，MAE也能做很复杂的一些任务（作者认为MAE或者说是transformer确实能够学到隐藏的比较好的语义表达）


最后一段讲的是broder impacts﻿﻿

就是说如果工作要被出圈的话，那么对整个社会的影响是什么
因为只用了图片本身的信息去进行学习，如果图片里面有bias（比如说倾向于某一些图片，或者是有一些不那么好的图片的话，就可呢个有一些负面的社会影响）
这个模型可以用来生成不存在的内容，因为它是一个生成模型，它可以用来生成原始的像素，所以它和GAN一样确实可能会有误导大家的前提，所以这个工作如果想用在更多的地方的时候，一定要去考虑这些潜在的影响


读到这里，对这个工作是在干什么、效果怎么样就有了一个比较清晰的了解了，这里可以停下来，如果对完整的故事感兴趣的话，也可以接着往下读









5、导言



深度学习在过去一些年里有飞快的进展，但是对于计算机视觉来讲，还是依赖于需要百万级别的、甚至是更多的带有标注的图片



在自然语言处理中，自监督学习已经做的非常好了

比如说GPT系列是一个标准的语言模型，BERT是一个带掩码的自编码模型，这些模型使得在NLP中可以使用没有标注的数据训练得到千亿级别的可学习参数的模型


在计算机视觉中，使用带掩码的自编码也不是那么新鲜，比如说大家用的比较多的denoising autoencoder（就是在一个图片中加入很多噪音，然后通过去噪来学习对这个图片的理解），至少在十几年前就已经有相关的文章了，这一块中，最新的工作就是基于BERT，最近有很多工作将BERT拓展到计算机视觉上面，但是BERT在计算机视觉上的应用是落后于NLP的



作者尝试回答了：是什么使得这种带掩码的自编码模型在计算机视觉和自然语言处理上不一样？

第一个观点是说：直到最近以前，计算机视觉都是使用卷积神经网路络（就是在一张图片上面，使用一个卷积窗口，不断地平滑它来汇聚这些像素上面的信息以及进行模式识别），卷积窗口使得不便于将这种mask的东西放进去，因为在transformer中，一个mask是一个词，而这个词是一个特定的词，它会一直保留下来，跟别的词区分开来。但是在卷积上面做掩码就是将一块图片给盖住（比如说就是将它的像素换成某一个特定的值），但是卷积窗口在图片上滑动的时候其实是无法区分这个边界的，因为它无法将被盖住的这一个特定的东西给拎出来，所以导致这个掩码信息在后面的时候难以还原出来到底是什么，因为不好加入位置编码（这里是比较奇怪的在transformer中加位置编码是因为注意力机制无法拥有位置信息，但是卷积是自带位置信息的，就是说在卷积窗口不断平移的时候，是没有必要加入位置信息进去的），作者说其实这一块现在也不是问题了，因为最近的vit的工作已经使得transformer能够很好地运用在计算机视觉上面了，所以这个问题应该是之前有的但是现在不会再有了
第二个观点是是说：信息的密度有点不太一样，在自然语言中，一个词就是一个语义的实体（比如说字典中一个词的解释就是很长的一片），所以一句话中很难去去掉几个词（比如完形填空也不是一件简单的事情）。但是在图片中会有所不同，因为图片中每个像素是比较冗余的，取决于相机的分辨率有多大，所以如果是简单地去掉一个块的话，去掉的这一块很有可能可以通过邻居的像素值进行插值还原出来。作者在这里提出了一个简单的想法：把非常高比率的一些随机的块去掉，这样的话极大地降低了图片的冗余性，因为一个块和它边上的很多块都被去掉了，那么在很远的地方的块跟这个块的关系就不那么冗余了，这样的话就创造了一个非常有挑战性的任务，而且使得整个模型会去看全局的信息，而不是仅仅关注于学一个局部的模型将这些像素的值进行插值好就可以了（在图2和图4中展示了在ImageNet和COCO数据集上MAE差值出来的图片的效果，它确实是可以通过一些局部的一些很稀疏的块得到全局图片的重构）
第三个观点是说：关于自编码器的解码器，在计算机视觉中，需要还原到输入，也就是原始的像素，它相对来说是一个比较低层次的一些表示。但是在NLP中，需要还原的是词，也就是说预测的内容是所缺失的词，词相对来说在语义层面上比较高一点，所以在NLP中，比如说在BERT中，使用一个最简单的全连接层就能还原出所要的那些词，也就是对标号进行预测。但是在机器视觉中，因为要还原的是比较低层次一点的像素，所以一个MLP可能是不够的（在图片分类、目标检测等任务上，输出层，也就是解码器，就是一个全连接层就够了，但是在一些比较复杂的任务，比如说语义分割当中，对每个像素做像素级别的输出的话，通常来说，就不是简单地使用一个全连接层了，而是很有可能就是使用一个卷积神经网络，而且是一个转置的神经网络来做一个比较大的解码器）
基于以上这些分析，作者就提出了MAE，实际上是有这两个想法

第一个想法是随机遮住大量的输入的块，然后去重构这些被遮住的像素信息

第二个想法是使用一个非对称的编码器和解码器的机制

非对称是说编码器看到的和解码器看到的东西是不一样的，这里编码器只看到那些可见的块，解码器拿到编码器的输出之后，就去重构那些被遮挡住的块
为什么使用这些非对称的架构，因为大量的块都被遮住了，这样的话编码器只用看可见的那些块，可以极大地减轻计算的开销，也可以使得内存更大一点




实验结果



在有了MAE的预训练之后，就可以训练一个vit large和huge，只使用ImageNet-1K的数据，也就是说只使用100万的图片使得能够训练出在vit中要使用100倍大小以上的训练数据才能做出来的效果，而且这个地方是不用标号的



而且在迁移学习上的效果也是很好的，通过这样预训练出来的模型在物体检测、实例分割、语义分割上效果都非常好，所以这样的话，就能够得到跟NLP类似的效果。也就是说在大量的没有标号的数据上，通过自监督学习训练出来的模型在迁移学习上效果也是非常不错的



到这里导言就已经结束了，作者写了整整两页，这是一个非常长的导言，原因有两个：

一是因为作者使用了大量的图片，这些图片是非常占地方的，但是对于计算机视觉来讲，有很多图片也是一个加分项
二是因为作者使用了一种问问题、回答问题、再引出想法的写法，这种写法是非常好的，如果不这么写而就是按照一般的将摘要扩充一下的写法的话，基本上就只能得到导言的最后两段话


仔细观察的话会发现作者其实没有提供更多的技术细节，这些东西比较简单，在摘要中已经讲得很清楚了，如果在导言中再重复一下的话，整个故事就不那么流畅了。所以作者采用了回到更本质的问题：将BERT从NLP用到CV的时候会有什么问题，然后列出了三个问题，一一作答之后再引出MAE这个算法，也就是说作者江整个算法为什么要设计成这个样子的原因讲清楚了，这是一种非常推荐的写法

对一篇论文来讲，不仅应该要讲做的是什么、是怎么做的，更多的是要讲清楚为什么需要这么做，也就是要讲清楚对这个问题的一些动机，如果没有这些动机的话，这篇论文很有可能就变成一篇技术报告了（虽然之前讲的不是这么写的，比如AlexNet那篇文章就是讲的是做了些什么，然后得出了怎样的结果，但是他确实不代表一般的写作一般的写作一定要讲清楚为什么）








6、相关工作



带掩码的语言模型





自编码器在视觉中的应用

MAE其实也是一种形式上的带去噪的自编码，因为将图片中的某一块遮住，就等于是在图片中加入了很多噪音，但是它跟传统的DAE（Denoising autoencoder）不太一样，因为它毕竟是基于ViT，基于整个transformer架构




带掩码的编码器在计算机视觉中的应用

本文不是第一个做这个工作的，之前已经有很多工作了。比如说iGPT就是GPT在image上的应用；ViT的论文在最后一段也讲了怎样用BERT来训练模型；另外还有BEiT，它也是BERT在image上的应用，它和MAE的不同之处在于，它对每一个patch都给了一个离散标号，更接近于BERT，但是在MAE中直接还原的是原始的像素信息




自监督学习

比如最近一两年特别火的contrast learning，它在这一块主要使用的是数据增强，而MAE所用的和它不一样





相关工作中提到了四个领域，提到的文章还是比较多的，但是本文并没有特别地讲MAE跟它们的不同点是什么，特别是跟iGPT和BEiT这两篇文章的区别

写作时，建议在写相关工作的时候，尽量将那些特别相关的工作和自己工作的不同之处写出来，而不要让大家去猜








7、MAE模型



MAE是一个简单的自编码器

自编码器看到了部分的观察的数据，然后用它来重构完整的原始信号，它跟所有的编码器一样，它是将观察到的信号映射到一个latnet representation（潜表示）中，这个潜表示可以认为是语义空间上的表示，然后再通过解码器将这个潜表示用来重构出原始的信号
这个自编码器跟经典的自编码器不太一样，它是一个非对称的结构，因为编码器只看到可见的那些块，不可见的块它就看不到了，用来节省开销




掩码是如何工作的

和vit一样，将整个图片割成一块一块的，每一块作为一个patch，也就是作为一个词
在采样的时候，随机的均匀的采样一些出来进行保留，剩下的全部用掩码盖住，文中将这种方法叫做随机采样。关键技术在于只采样少量的块出来，然后剩下的全部盖住，这样采样少量的块，使得它们的冗余度不是很高，这样的话，任务就不是那么简单（不会说是用简单的插值就能解决问题了）




编码器

其实就是一个vit，没有做任何改动，但是它只作用于可见的那些块中，具体的做法跟vit是一样的：将每一块拿出来，然后做线性的投影，再加上位置信息，这样就做成一个词进去了
但是不一样的是，如果这一块被盖住的话就不会进去了。所以如果有25%被选中，那么进入vit的话就只有25%的样本，这样的话就可以降低计算量




解码器

因为要重构被盖住的块的像素信息，所以它需要看到所有的块，包括没有被盖住的块（虽然没有被盖住，但是已经通过编码器将它表示成了潜表示）和被盖住的块（没有进入编码器）
解码器是对这些所有的块，通过共享的可以学到的向量来表示，也就是说每一个被盖住的块都表示成同样的向量，这个向量的值是可以通过学习得到的
解码器其实就是另外一个transformer，所以需要加入位置信息，不然就无法区分它对应的到底是哪一个是掩码（这里有一点不确定：要不要对那些编码器输出的潜表示也加上位置信息，因为它们其实已经加上过一次了，那么这个地方要不要再加上一次？）
解码器主要只在预训练的时候使用，当将模型用于做一些别的任务的时候，解码器是不需要的，只需要用编码对一个图片进行编码就可以了，这样的话比较灵活，想用什么东西，就可以用什么东西
这里采用的是一个相对来说比较小的解码器的架构，使得它的计算开销相对于编码器来说，不到它的1/10




怎样重构出原始的像素

在解码器的最后一层是一个线性层。如果所切割成的一块中是16*16的像素的话，那么这个线性层就会投影到长为256的这样一个维度，拿到这个投影之后，再将它reshape成所要的16*16，就能还原出原始的像素信息了
损失函数使用的是MSE，就是说预测和原始的真正的像素相减再平方，和BERT一样，只在被盖住的那些块上面使用MSE，那些没有被盖住的块，因为输入已经看到了整个像素信息，所以就不在上面做损失了
也可以对要预测的像素值做一次normalization，就是说对每一个块里面的像素使它的均值变为0、方差变为1，使得在数值上更加稳定一点（这里并没有写清楚在预测的时候怎么办，在训练的时候当然知道标号的那些东西，当然可以把均值和方差算出来，但是预测的时候呢）




简单实现

首先生成了一些token列，token列就是一个一个的patch拿过来之后做一次线性的投影，再加上位置信息
接下来将这些序列随机打乱，然后将最后一块拿掉，因为随机打乱，把头部保存下来，就等价于是把头部均匀的、没有重置的将里面的东西采样出来，所以如果是采样25%的话就随机shuffle一下，然后将前25%留下来，后面的丢掉就行了，这样就完成了随机采样
在解码的时候，要在后面附上跟以前长度一样的掩码的词源mask tokens，它就是一个可以学习的向量，当然也要加上位置信息，然后再把它重新unshuffle一下还原到原来的顺序，这样的话，在算误差的时候跟之前那个原始的没有被做过任何操作的patches是能够一一对应起来的
要加上位置信息（这里还是有一个疑惑：这个位置信息要不要对编码器那边来的也给加上，还是说从编码器出来的那些潜表达可以寄跳过加位置信息这个步骤）
这里主要是说通过这样的shuffle和unshuffle使得不需要任何的稀疏操作，在实现起来是非常快的，也不影响后面所有的vit的操作




相对于前面来说，这里面补充了一些细节

解码器
怎样还原出原始的像素信息
在实现上，随机采样是如何实现的








8、实验



在ImageNet-1K数据集（100万张数据集）上先做自监督的预训练，就是不用标号，只把图片拿过来，然后再在同样的数据集上做有标号的监督训练



这里有两种做法

一个是做end to end的微调：允许该整个模型所有的可学习的参数

第二个是linear probing：只允许改最后一层的线性输出层



在验证集上报告top1的精度，所使用的是中心剪裁的24*24的图片，基线用的是vit large（vit-l/16，16表示的是图片块是16*16的），vit-l是一个非常大的模型，比ResNet大很多，它很容易overfitting



下图表格中比较的是三种情况


第一个是使用vit论文中所有的东西在ImageNet-1k上训练出来的结果，这个结果不是很稳定，然后作者再在上面根据之前的论文做了一些改进，加入了一些比较强的正则化（在附录的A2中有详细的解释），加入之后使得训练效果比较好，从72.5提升到了82.5
在vit文章中所讲的是模型需要很大很大的数据才可以，但是后来大家发现，如果加入合适的正则项的话，其实在小一点的数据集（比如说ImageNet-1k）上也是能够训练出来的
最后一个是说先使用MAE做预训练，然后再在ImageNet上做微调，这时候就不需要训练完整的200个epoches，只需要50个就可以了。可以发现从82.5提升到了84.9
因为数据集并没有发生变化（预训练和后面的微调都是用的ImageNet），这就表示MAE能够纯从图片上也能学到不错的信息，使得后面有所提升




主结果



下图中表一展示的是各种ablation study


表a讲的是解码器的深度（需要用到多少个transformer块），这里有1、2、4、8、12这几种选择。第一列是所有可以学习的权重都跟着调，第二列表示只调最后一个线性层，可以发现前面一种方式虽然比较贵，但是效果会好很多，而且在块里面，这个地方显示的是使用8块比较好，对于全部调的话好像关系并不是很大，都是84左右，但是如果只调最后一层的话，用深一点的会比较好
表b讲的是解码器的宽度（每个token表示成一个多长的向量），这里发现512比较好

表c讲的是在编码器中要不要加入被盖住的那些块，结果显示，不加入被盖住的那些块，精度反而更高一些，而且计算量更少（以不加作为基线的话，加进去的计算量是不加的3.3倍），所以本文采用的是非对称的架构，不仅是在性能上更好，在精度上也更好一些

表d讲的是在重构的时候的目标，最简单的就是对每个像素使用MSE，这里是加了normalization，就是将每一个块中的均值和方差分别变成0和1，发现效果不错。pca是做一次降维，最后一个其实是BEiT的做法（通过vit把每一块映射到一个离散的token上面，能够像BERT一样做预测），从表中可以发现，在fine-tune的话，值都是差不多的，所以在值差不多的情况下，当然是倾向于使用更简单的办法

表e讲的是如何做数据增强。none表示什么都不做，第二行表示只裁剪（固定大小），第三行表示按照随机的大小裁剪，最后一行表示再加上一些颜色的变化。从表中可以发现，做简单的随即大小的裁剪，效果就已经很不错了，所以作者说MAE对于数据的增强不那么敏感

表f讲的是如何采样，哪一些块需要被盖住。第一行表示的是随机采样，其实还有一块一块地采样，或者是按照格子来进行采样，但是最后发现随机采样这种做法最简单，效果也最好




下图是对以上表中结果的展开


上图表示的是使用不同的掩码率的时候的效果
掩码率越大，不管是对fine-tune也好，还是对于只调最后一层来讲也好，效果都是比较好的
特别是只调最后一层的话，对掩码率相对来讲更加敏感一点




下表表示的是训练时间


表中使用vit-large而且解码器只使用一层transformer块的时候，精度也是不错的，时间是最小的，和之前把所有的带掩码的块全部拿过来而且使用比较的解码器的时候相比，加速是3.7倍
如果是vit-huge的话，加速时间也是比较多的
绝对时间：这里使用的是128个TPU v3的core，而且使用的是tensorflow实现（这里比较有意思的是作者是facebook的人，这里使用的是tensorflow，而且使用的是TPU），训练的时间是10个小时
这里可能是因为使用了tensorflow和TPU，所以MAE并没有公布代码




下图表示采用不同的掩码采样策略的区别


第一列表示的是随机采样
第二列表示的是尽量按一块一块的来切
第三列表示的是尽量按照格点来切
通过图中可以发现，随机采样的效果比较好，而且比较简单，实现上也不难




下图显示的是预训练的轮数和最后再做微调的精度的对比


图中可以看到，在ImageNet-1k上训练1000个数据轮的话，能够看到精度的提升，这也是一个非常不错的性质，说明在一直训练的情况下，过拟合也不是特别严重（1000轮其实是非常多的，一般在ImageNet上训练200轮就差不多了）




4.1讲的是不同的超参数下的结果



下表展示的是跟前面一些工作相比，基本上MAE的效果是最好的






下图表示的是和vit里面的结果相比


图中的十字表示的是vit在JFT这个3亿个标号的图片数据集上的效果
实线指的是ImageNet-1K，也就是JFT数据集1/300数据的效果，可以发现这两根线是非常接近的，虽然有一点区别但是区别不大。当然这并不是一个很公平的比较，因为JFT数据集所包括的类数远远大于ImageNet，而且它们很多是一些google自己所关注的目标，而ImageNet很多都是一些猫猫狗狗的图片，而且比的话也是在ImageNet数据集上比，所以JFT多了很多标号不那么一样的图片，也许在这个地方看上去效果不那么明显，但是如果把验证集换到一个跟ImageNet不那么相似的数据集上可能差距会大一点
当然，这里主要比较的是算法而不是数据集




4.3讲的是输入到底调多少层比较好



之前可以看到，如果能将整个编码器里面所有的层进行微调和只调最后一层，它们的效果差别是非常大的，前者的效果要好很多



微调的时候，到底调多少层是有讲究的，调的越少当然越快，但是通常精度也就越差，调的越多，就越慢但是精度会越好




上图中，x轴表示的是有多少个transformer块要被调，y轴表示的是精度
从什么都不调，到简单地只调最后一层开始，再调到4、5层的时候基本上精度就可以回来了，而且后面也是比较平坦的，这表示底部那些层学到的东西稍微是比较低层次一点，在换另外一个任务的时候也不需要变化太多，但是上面的层还是和任务比较相关的，最好还是做一些调整




第五章讲的是迁移学习的效果



下图中的表四和表五做的是COCO数据集上的目标检测和分割


从表四中可以发现用MAE当作主干网络之后效果是最好的


表六表示的是和当还是像BEiT那样用dVAE学出来的标号重构原始的像素相比的结果


从表中可以发现二者的差别是非常少的，因为重构原始像素的方式很多，所以所用的方法当然越简单越好








9、评论



MAE的算法还是非常简单的，就是利用vit来做和BERT一样的自监督学习，vit已经做了类似的事情了，但是本文在此基础之上提出了两点

第一点是需要盖住更多的块，使得剩下的那些块，块与块之间的冗余度没有那么高，这样整个任务就变得复杂一点
第二个是使用一个transformer架构的解码器，直接还原原始的像素信息，使得整个流程更加简单一点
第三个是加上vit工作之后的各种技术，使得它的训练更加鲁棒一点
以上三点加起来，使得MAE能够在ImageNet-1k数据集上使用自监督训练的效果超过了之前的工作



从写作上来说，这篇文章非常简单，但是故事性写的非常好。从前面导言中为什么MAE要这么做，以及后面非常详细的数据显示了整个MAE中所有可以调的超参数到底是什么意思



简单的想法、非常好的结果、详细的实验，共同决定了这是一个非常高质量的工作









----end---- 作者：如果我是泡橘子 https://www.bilibili.com/read/cv14591955/?from=readlist&jump_opus=1 出处：bilibili