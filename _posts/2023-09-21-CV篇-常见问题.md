---
published: false
layout: post
title: "CV常见问题"
categories: 我的AI新书
date: 2023-09-21 00:00:00 +0800
excerpt: "CV常见问题"
---

**在卷积神经网络出现以前，图像分类，目标检测和分割的主流方法是什么？**

图像分类是指将图像分配到不同的类别，比如猫、狗、飞机等。在卷积神经网络出现以前，图像分类的主流方法是基于手工设计的特征，比如颜色、纹理、形状、边缘等，然后使用传统的机器学习算法，比如支持向量机（SVM）、随机森林（RF）、K近邻（KNN）等，来对特征进行分类。这些方法的缺点是特征的设计需要领域知识和经验，而且不能适应不同的场景和任务。12

目标检测是指在图像中定位和识别不同的物体，比如画出物体的边界框，并给出物体的类别。在卷积神经网络出现以前，目标检测的主流方法是基于滑动窗口的方法，即在图像中用不同大小和位置的窗口扫描图像，然后对每个窗口进行分类和回归，来判断是否包含物体以及物体的位置。这些方法的缺点是计算量非常大，而且难以处理多尺度和遮挡等问题。34

图像分割是指将图像分割成多个区域，使得每个区域具有一致或相似的性质，比如颜色、纹理、语义等。在卷积神经网络出现以前，图像分割的主流方法是基于阈值、区域增长、边缘检测、聚类、超像素等技术，来根据像素的灰度值、颜色值、梯度值等特征来划分区域。这些方法的缺点是对参数和阈值的选择非常敏感，而且难以处理复杂和噪声干扰的图像。5



**如何验证一个预训练模型初始化正确？**
用同一个数据，分别输入到两个网络，看输出是否一致，出自I3D视频



**目标检测数据标注格式有哪些？**
对象检测领域常用的数据标注格式有以下几种:

PASCAL VOC格式
这是历史上使用广泛的标注格式,每张图像一个XML文件,包含对象名称、difficult度、bounding box坐标等信息。

MS COCO格式
JSON格式,提供分类ID、区域边界框坐标和segmentation信息。

YOLO格式
简单的TXT格式,每行包含类别ID、矩形框中心坐标、宽高等。

TensorFlow Record 格式
TF提供的二进制格式,包含图像数据和标注信息,可直接输入到模型。

LabelMe JSON
开源标注工具LabelMe输出的JSON格式,包含标注形状、标签等信息。

CSV格式
使用逗号分隔的表格文件,常见的简单格式。

Darknet格式
YOLO作者开源的简单TXT格式,类似YOLO格式。

VOT 格式
VOT数据集使用的XML格式,包含目标边界框和多帧信息。

KITTI格式
自动驾驶数据集,包含标注车辆的位置、尺寸、方向等。

不同的标注格式有各自的优势,选择需要根据具体的检测模型和任务来决定。COCO和VOC格式使用广泛,YOLO格式适合其模型。


**请分别给出一个VOC和COCO的例子**
好的,下面分别给出PASCAL VOC和MS COCO两个格式的标注示例:

PASCAL VOC 格式
XML结构如下:
<annotation>
    <object>
        <name>cat</name>
        <pose>left</pose>
        <truncated>0</truncated>
        <difficult>0</difficult>
        <bndbox>
            <xmin>60</xmin>
            <ymin>40</ymin>
            <xmax>102</xmax>
            <ymax>112</ymax>
        </bndbox>
    </object>
</annotation>

其中包含对象名称、姿态、是否截断、是否困难以及边界框坐标信息。

MS COCO格式
JSON结构如下:
{
    "image_id": 289343,
    "category_id": 18, 
    "segmentation": [[510,463,...]],
    "area": 702.1057499999998,
    "bbox": [473,395,38,43],
    "iscrowd": 0
}

包含图像id、类别id、分割数据、区域面积、边界框坐标以及是否是一组对象等信息。

上面简单的示例展示了这两种典型标注格式的一些关键信息。VOC以XML格式提供基础的标注,而COCO提供了更丰富的实例分割和关键点数据。



**paddle inference进行推理时，  std::vector<int> input_shape = {1, chs, rows, cols}; chs是chanel数，rows是图像高，cols是图像宽，  std::vector<float> input_data(1 * chs * rows * cols, 0.0f);那么当使用input_data存储图象时，我是一行一行读还是一列一列的读？**

如果使用std::vector<float> input_data来存储图像,且图像参数为:

input_shape = {1, chs, rows, cols}
chs为通道数
rows为图像高度
cols为图像宽度
则input_data的大小应为 1 * chs * rows * cols。

其中:

第一维对应批量大小,这里是1张图像
第二维对应通道数chs
第三维对应图像高度rows
第四维对应图像宽度cols
读取图像像素时,要依次遍历各维度,将图像数据按顺序填充到input_data中。

具体的读取顺序是:一行一行读,而每一行则是一列一列读。

也就是说外层遍历图像高度行,内层遍历图像宽度列,依次读取每个像素点,顺序填充到input_data向量中。

这样可以保证input_data中的数据布局与图像的二维结构对应,第一维依次排列图像各行的数据,第二维对应一行中的像素。

这样就可以正确地将图像数据存储到连续的一维vector中,方便后续进行计算机视觉模型的处理。


**请帮我总结一下BiSeNet V1和BiSeNet V2的区别？**
BiSeNet V1和BiSeNet V2是两种用于实时语义分割的网络，它们都采用了双向结构，分别处理空间细节信息和高层语义信息。它们的主要区别有以下几点：

- BiSeNet V1使用了Xception或ResNet作为语义分支的backbone，BiSeNet V2使用了自定义的轻量级模块，如Stem Block，Gather-and-Expansion Layer和Context Embedding Block12。
- BiSeNet V1使用了双等级注意力机制来融合两个分支的特征，BiSeNet V2使用了双向引导层来实现特征融合12。
- BiSeNet V1在训练时只使用了一个辅助分割头，BiSeNet V2在训练时使用了多个辅助分割头来增强训练效果12。
总的来说，BiSeNet V2相比于BiSeNet V1，在保持高速度的同时，提高了精度和鲁棒性。


**都有哪些实时语义分割网络？**

目前常见和代表性的实时语义分割网络包括:

ENet:早期轻量级实时分割网络,使用encoder-decoder结构。
ESPNet:效率分割网络,使用空洞卷积和点卷积减小参数量。
ICNet:图像级联网络,多分辨率子网络提高速度。
BiSeNet:双分支结构,空间路径保证精度,上下文路径保证速度。
Fast-SCNN:学习空间信息的快速分割卷积网络。
DFANet:双流注意力网络,使用可深度可分离卷积。
LEDNet:轻量级编码解码器网络,使用aspp和双向特征金字塔网络。
FPN:特征金字塔网络,利用多尺度特征实现实时分割。
ERFNet:高效residualFACTOR模块,可实时高分辨率图像分割。
OCNet:对象语境网络,联合全局和局部特征。
SwiftNet:轻量级兼顾速度和精度的网络。
这些方法通过轻量网络、多分支设计、attentions等方式来实现实时推理。实时性和精度之间需要权衡。



**在什么情况下GPU推理速度反而会比CPU更慢？**
Answer one:
GPU推理的速度是否比CPU更快，取决于很多因素，例如数据的大小、类型和传输，以及计算的复杂度、并行度和优化。一般来说，GPU在处理大规模的浮点数运算时，比如矩阵乘法、卷积等，会有明显的加速优势，因为GPU有更多的并行处理单元和更高的内存带宽。但是，在以下一些情况下，GPU推理可能反而比CPU更慢：

- 数据量较小，无法充分利用GPU的并行能力，而且还要考虑数据从CPU内存到GPU内存的传输开销12。
- 计算中包含大量的控制指令，比如索引、重塑、求和等，这些操作在GPU上的执行效率可能比CPU低34。
- 计算中使用了双精度浮点数或者其他非32位的数据类型，这些类型在GPU上的性能会下降4。

因此，在使用GPU推理时，需要根据具体的任务和数据来选择合适的参数和优化方法，以达到最佳的性能。

Answer two:
对于一些小模型,GPU推理可能比CPU推理更慢的原因如下:

- 数据传输开销大
GPU和CPU内存分离,小模型频繁的数据传输开销可能超过GPU计算加速带来的收益。

- GPU初始化和启动开销
启动和初始化GPU也需要一定时间,对小模型这部分固定开销的比例更大。

- 小批量不利于并行
小模型一次性推理的数据量小,不利于GPU并行计算的发挥。

- 内存访问不连续
小模型可能导致GPU上内存访问不连续,缓存未命中,降低效率。

- 低算力利用率
小模型计算量小,导致GPU算力利用率不足,难以发挥GPU吞吐优势。

- 优化程度不足
部分小模型的GPU实现不够优化,算法没有充分考虑GPU架构。

综上,当模型比较小或批处理数据量较少时,GPU的并行计算优势难以充分发挥,反而数据传输和初始化开销成为瓶颈,从而导致GPU推理效率比CPU推理还低的情况。


**CNN中能加快推理速度的方法**
- Channel Pruning
Channel Pruning(通道修剪)是神经网络模型压缩中的一种重要方法,其主要思想是:

通过一定评估指标识别出模型中相对重要性较低的通道(channel),然后将其删除或修剪。
常用的评估指标包括每个channel的L1范数,对应的激活值,对应滤波器的平均值等。
通过删除冗余或不重要的通道,可以减小模型参数量和计算量,降低内存占用和计算负载。
通常在pruned模型后会进行微调,以恢复部分因修剪导致的精度损失。
Channel Pruning可应用于各类CNN模型,如ResNet、MobileNet等。
与直接减小网络宽度不同,Channel Pruning会考量每个channel的重要性,删除对最终精度影响较小的channel。
Channel Pruning是一个有效的模型压缩技术,可获得轻量级的模型,并保持较高的精度。
其可用于在移动端和嵌入式设备上部署神经网络模型。
综上,Channel Pruning通过识别和修剪冗余通道,可以有效压缩模型大小,是轻量化模型的重要方法之一。

- 量化(Quantization)
使用低精度数据类型(int8、bf16等)表示权重和激活,减少存储空间。

- 知识蒸馏(Knowledge Distillation)
让大型教师模型指导小型学生模型学习知识。

- 结构修剪(Structured Pruning)
按照一定策略修剪非重要的层或结构模块。

- 矩阵分解
使用低秩矩阵来逼近原始权重矩阵。

- 裁剪和调整模型输入分辨率
降低输入数据规模来减小计算量。

- 动态计算图
根据输入动态调整计算路径,避免无效计算。

- 轻量级架构設计
如SqueezeNet、MobileNet等专门设计的轻量网络。

- 网络搜索
通过自动搜索学习高效的神经架构。



**在CV中什么情况下会将BGR转换成RGB？**
在计算机视觉(CV)中,常见的情况需要将BGR颜色模式转换成RGB模式:

图像读取。OpenCV默认是以BGR模式读取和存储图像,而大多数其他算法和可视化需要RGB模式。所以读取OpenCV图像后通常需要BGR转RGB。
图像显示。如使用matplotlib,pyplot,PIL等显示图像时,需要确保图像是RGB模式,否则需要先转换。
预训练模型。一些模型如VGG,ResNet等都是以RGB模式pretrain的,输入也需要是RGB,所以需要确保输入图像为RGB。
网络训练。如果模型以RGB模式定义,但输入图像为BGR,同样需要转换输入以匹配模型。
图像转换。有时需要对BGR和RGB图像进行转换,以配合不同算法和应用的需求。
图像融合。将BGR和RGB图像融合处理时,需要确保都是同一种颜色空间。
可视化和渲染。渲染、图像处理等任务通常都假设图像为RGB格式。
总之,CV任务通常需要频繁地在BGR和RGB两种颜色空间中转换,以满足算法和应用的需要。


**[height, width, channels]和[channels, height, width]图像数据维度在内存布局上有什么差别？
**
在图像处理中,[height, width, channels]和[channels, height, width]表示的是图像数据的不同维度排布。

它们在内存中的布局上有以下区别:

[height, width, channels]表示通道最后的布局。即图像中的像素点按高度方向,然后宽度方向,最后颜色通道方向顺序存储。这是一种行优先的布局。
[channels, height, width]表示通道最先的布局。即先存储所有的红色通道像素,然后绿色通道,最后蓝色通道。这是一种通道优先的布局。
对于行优先,[height, width, channels],图像中的所有像素点在内存是连续存储的。这提供了加载和访问的便利。但跨通道的访问不连续。
对于通道优先,[channels, height, width], 同一个通道中的所有像素是连续的,有利于通道方向的计算。但跨行访问不连续。
有些算法要求特定的布局,如行优先或通道优先,所以可能需要重新排布图像数据的维度。
转换维度排布可以用transpose或permute之类的函数实现。
所以根据任务的不同,会采用不同的维度排布方式来优化图像数据的存储和访问。


**在CV中什么情况下ground truth是一个列表？**
在计算机视觉等领域,ground truth(地面真值)通常指的是图像或数据的参考标注。

以下几种情况下,ground truth会是一个列表,包含多个元素:

- 目标检测任务。ground truth会包括多个目标框的坐标,每个目标一个框,所以是Bounding Box的列表。
- 语义/实例分割任务。ground truth会有每个像素的类别标注,对每个实例分配不同id,所以是分割Mask的列表。
- 多标签分类。一个样本可能拥有多个标签,所以ground truth是这个样本的标签列表。


**使用CNN对图像进行分类时，在将特征图送入FC层之前，会将特征图进行flat，是如何展开的？请结合一个具体的例子讲解**
使用CNN对图像进行分类时，在将特征图送入FC层之前，会将特征图进行flat，是如何展开的？请结合一个具体的例子讲解。
特征图是CNN中卷积层和池化层的输出，它们是一个三维的张量，包含了多个通道，每个通道是一个二维的矩阵，表示了图像的某种特征。为了将特征图送入FC层，我们需要将特征图进行flat，也就是将三维的张量展开为一维的向量，这样才能和FC层的权重矩阵相乘。
具体的展开方法是，按照特征图的通道顺序，将每个通道的矩阵按行或按列展开为一维的数组，然后将所有通道的数组拼接起来，形成一个长的一维向量。例如，假设我们有一个特征图，它的形状是(2, 3, 3)，表示有2个通道，每个通道是一个3x3的矩阵，如下所示：
​147​258​369​​​101316​111417​121518​​
如果我们按行展开，那么我们得到的一维向量是：
[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18]
如果我们按列展开，那么我们得到的一维向量是：
[1,4,7,2,5,8,3,6,9,10,13,16,11,14,17,12,15,18]
这就是特征图进行flat的过程，你可以在这里和这里找到更多的解释和示例。希望这对你有帮助。



**CNN VS ANN**
Relative position and presence of certain patterns is important, and not their exact position on the image.



**小物体目标检测困难的根本原因是什么？**
小物体目标检测困难的根本原因主要包括以下几点：

信息量不足：小物体由于像素面积小，包含的判别性特征较少，导致rgb信息不足。
数据集不平衡：例如，在COCO数据集中，只有51.82%的图片包含小物体，存在图像级别的不平衡。
Anchor匹配问题：对于基于anchor的方法，小目标的ground truth box和anchor都很小，稍有偏移就会导致IoU变低，被网络判断为负样本。
网络结构问题：传统卷积网络的结构对小目标不敏感，细节信息在层层降维后丢失过多。
监督信号不足：小目标的数量占比较少，提供给网络的监督信息不够，难以学习到小目标的泛化特征。


**CNN和注意力机制的关系**
CNN是简化版的注意力机制，CNN只考虑感受野之内的像素点，但注意力关注的是整张图像的像素
https://arxiv.org/pdf/1911.03584.pdf




