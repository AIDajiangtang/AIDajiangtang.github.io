---
published: false
layout: post
title: "字节对编码（Byte-Pair Encoding，BPE）：基于子词的分词算法"
categories: 我的AI新书
date: 2023-09-21 00:00:00 +0800
excerpt: "字节对编码（Byte-Pair Encoding，BPE）：基于子词的分词算法"
---

人工智能的一个领域——自然语言处理（Natural Language Processing，NLP），旨在使机器能够理解和处理人类语言。对于机器来说，处理人类语言并不容易，因为机器处理的是数字而不是文本。💻 NLP是一个广泛研究的领域，我们不时地听到这个领域的新进展。研究人员正在努力让机器能够理解人类语言及其背后的语境。
在理解人类语言中，分词器（Tokenizer）发挥着重要的作用。分词算法可以是基于词、基于子词或基于字符的。每种类型的分词器以不同的方式帮助机器处理文本，每种都比其他方式有优势。如果你想了解在NLP中使用的不同类型的分词器，你可以阅读本文。本文是一个实践教程，将帮助你对这个主题有一个很好的理解。😇




在这些分词器中，最受欢迎的是基于子词的分词器。大多数最先进的NLP模型都使用这种分词器。那么，让我们首先了解一下基于子词的分词器是什么，然后再了解一下最先进的NLP模型使用的字节对编码（BPE）算法。🙃





子词分词器

基于子词的分词算法是介于基于词和基于字符的分词之间的解决方案。😎其主要思想是解决基于词的分词（词汇表非常庞大，大量的OOV标记和非常相似的单词有不同的含义）和基于字符的分词（序列非常长，单个标记含义较少）所面临的问题。
基于子词的分词算法不会将常用的单词拆分为更小的子词，而是将罕见的单词拆分为更小的有意义的子词。例如，“boy”不会被拆分，但“boys”会被拆分为“boy”和“s”。这有助于模型学习到“boys”这个词是由“boy”这个词形成的，具有略有不同的含义，但是相同的词根。
一些常见的基于子词的分词算法包括WordPiece、字节对编码（Byte-Pair Encoding，BPE）、Unigram和SentencePiece。在本文中，我们将介绍字节对编码（BPE）。BPE在GPT-2、RoBERTa、XLM、FlauBERT等语言模型中使用。其中一些模型使用空格分词作为预分词方法，而另一些模型使用Moses、spaCy、ftfy等提供的更高级的预分词方法。那么，让我们开始吧。🏃


字节对编码（Byte-Pair Encoding，BPE）

字节对编码（Byte-Pair Encoding，BPE）是一种简单的数据压缩算法，其中最常见的连续字节对会被替换为在该数据中不存在的字节。它最早在1994年发表的文章《一种新的数据压缩算法》中进行了描述。下面的示例将解释BPE，并引用自维基百科。
假设我们有需要进行编码（压缩）的数据 aaabdaaabac。最常见的字节对是 aa，因此我们将其替换为 Z，因为 Z 在我们的数据中不存在。现在我们有了 ZabdZabac，其中 Z = aa。接下来最常见的字节对是 ab，所以我们用 Y 替换它。现在我们有了 ZYdZYac，其中 Z = aa，Y = ab。剩下的唯一字节对是 ac，它只出现一次，因此我们不对其进行编码。我们可以使用递归字节对编码将 ZY 编码为 X。我们的数据现在变成了 XdXac，其中 X = ZY，Y = ab，Z = aa。由于没有出现重复的字节对，它无法进一步压缩。我们通过按相反的顺序进行替换来解压缩数据。
NLP中使用了BPE的变体。让我们一起了解一下它在NLP中的应用。🤗
BPE确保最常见的单词作为单个标记表示在词汇表中，而罕见的单词则被拆分为两个或多个子词标记，这与基于子词的分词算法的原理一致。


假设我们有一个语料库，其中包含以下单词（在基于空格的预分词之后）：old（旧的）、older（更旧的）、highest（最高的）和lowest（最低的），我们统计了它们在语料库中的出现频率。假设这些单词的频率如下：

{“old”: 7, “older”: 3, “finest”: 9, “lowest”: 4}

让我们在每个单词的末尾添加一个特殊的结束标记 "</w>"。

{“old</w>”: 7, “older</w>”: 3, “finest</w>”: 9, “lowest</w>”: 4}



在每个单词的末尾添加的 "</w>" 标记用于识别单词边界，以便算法知道每个单词的结束位置。这有助于算法遍历每个字符并找到最高频的字符配对。稍后当我们将 "</w>" 添加到字节对中时，我将详细解释这部分内容。
接下来，我们将将每个单词拆分为字符并计算它们的出现次数。初始的标记将是所有字符以及 "</w>" 标记。

<img src="/assets/images/new book/NLP/BPE/1.webp?raw=true" >


由于我们总共有23个单词，所以我们有23个 "</w>" 标记。第二高频的标记是 "e"。总共，我们有12个不同的标记。
BPE算法的下一步是寻找最频繁的配对，将它们合并，然后再次进行相同的迭代，直到达到我们的标记限制或迭代限制。
合并操作可以用最少数量的标记来表示语料库，这是BPE算法的主要目标，即数据压缩。为了进行合并，BPE会寻找最常出现的字节对。在这里，我们将字符视为字节。这在英语中是成立的，但在其他语言中可能会有所不同。现在，我们将合并最常见的字节对，以创建一个标记，并将其添加到标记列表中，并重新计算每个标记的出现频率。这意味着在每个合并步骤之后，我们的频率计数将发生变化。我们将继续执行此合并步骤，直到达到迭代次数或达到标记限制大小。


迭代
第一次迭代：我们将从第二常见的标记 "e" 开始。在我们的语料库中，与 "e" 最常见的字节对是 "e" 和 "s"（在 finest 和 lowest 中），它们共出现了9 + 4 = 13次。我们将它们合并成一个新的标记 "es"，并将其频率记为13。我们还将13从各个标记（"e" 和 "s"）的计数中减去。这将让我们知道剩余的 "e" 或 "s" 标记。我们可以看到 "s" 完全不单独出现，而 "e" 出现了3次。以下是更新后的表格：

<img src="/assets/images/new book/NLP/BPE/2.webp?raw=true" >

第二次迭代：现在我们将合并标记 "es" 和 "t"，因为它们在我们的语料库中出现了13次。因此，我们有一个新的标记 "est"，频率为13，并将 "es" 和 "t" 的频率减去13。

<img src="/assets/images/new book/NLP/BPE/3.webp?raw=true" >

第三次迭代：现在让我们处理 "</w>" 标记。我们可以看到字节对 "est" 和 "</w>" 在我们的语料库中出现了13次。

<img src="/assets/images/new book/NLP/BPE/4.webp?raw=true" >

注意：合并停止标记 "</w>" 非常重要。这有助于算法区分诸如 "estimate" 和 "highest" 这样的单词。这两个单词都有共同的 "est"，但一个单词的结尾有一个 "est" 标记，而另一个单词的开头有一个 "est" 标记。因此，像 "est" 和 "est</w>" 这样的标记将被区别对待。如果算法看到标记 "est</w>"，它将知道它是单词 "highest" 的标记，而不是单词 "estate" 的标记。
迭代
第四次迭代：查看其他标记，我们可以看到字节对 "o" 和 "l" 在我们的语料库中出现了7 + 3 = 10次。
<img src="/assets/images/new book/NLP/BPE/5.webp?raw=true" >

第五次迭代：现在我们可以看到字节对 "ol" 和 "d" 在我们的语料库中出现了10次。

<img src="/assets/images/new book/NLP/BPE/6.webp?raw=true" >

如果我们现在看一下我们的表格，我们可以看到 "f"、"i" 和 "n" 的频率都是9，但是我们只有一个包含这些字符的单词，所以我们不会将它们合并。为了简化本文的描述，让我们停止迭代，并仔细查看我们的标记。

<img src="/assets/images/new book/NLP/BPE/7.webp?raw=true" >

那些频率计数为0的标记已经从表格中删除。我们现在可以看到总的标记数量为11，少于我们最初的12个。这是一个很小的语料库，但实际上，大小会大大减小。这个由11个标记组成的列表将成为我们的词汇表。
您可能还注意到，当我们添加一个标记时，要么我们的计数增加，要么减少，要么保持不变。实际上，标记计数首先增加，然后减少。停止准则可以是标记的数量或迭代次数。我们选择这个停止准则是为了以最高效的方式将我们的数据集分解为标记。


编码与解码

现在让我们看看如何解码我们的示例。要进行解码，我们只需要将所有的标记连接在一起，即可得到完整的单词。例如，编码序列 ["the</w>", "high", "est</w>", "range</w>", "in</w>", "Seattle</w>"]，我们将解码为 ["the", "highest", "range", "in", "Seattle"]，而不是 ["the", "high", "estrange", "in", "Seattle"]。请注意 "est" 中的 "</w>" 标记。
对于编码新数据，过程同样简单。然而，编码本身在计算上是昂贵的。假设单词序列是 ["the</w>", "highest</w>", "range</w>", "in</w>", "Seattle</w>"]。我们将遍历我们在语料库中找到的所有标记，从最长到最短，并尝试使用这些标记替换给定单词序列中的子字符串。最终，我们将遍历所有的标记，我们的子字符串将被已经存在于标记列表中的标记替换。如果有一些子字符串剩下（对于我们在训练中未见过的单词），我们将用未知标记来替换它们。
通常情况下，词汇表的大小很大，但仍然存在未知单词的可能性。在实践中，我们将预分词的单词保存在字典中。对于未知（新的）单词，我们将应用上述的编码方法对其进行分词，并将新单词的分词结果添加到我们的字典中，以供将来参考。这有助于我们为将来进一步构建更强大的词汇表。


这不是贪婪的吗？🤔
为了以最高效的方式表示语料库，BPE在每次迭代中通过查看频率来考虑每个潜在的合并选项。因此，是的，它遵循贪婪的方法来优化找到的最佳解决方案。
无论如何，BPE是最广泛使用的子词标记算法之一，尽管它是贪婪的，但性能良好。💃
希望这篇文章能帮助您理解BPE算法背后的思想和逻辑。😍


References:

https://aclanthology.org/P16-1162.pdf
https://huggingface.co/transformers/tokenizer_summary.html
https://www.drdobbs.com/a-new-algorithm-for-data-compression/184402829
https://en.wikipedia.org/wiki/Byte_pair_encoding
