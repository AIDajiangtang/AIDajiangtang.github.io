---
published: false
layout: post
title: "预告"
categories: 我的AI新书
date: 2023-09-21 00:00:00 +0800
excerpt: "预告"
---


再谈转置卷积



转置卷积



转置卷积是一种卷积

转置卷积将输入和核进行了重新排列
卷积一般是做下采样，将高宽变得更小；转置卷积通常用作上采样，将输入的高宽变大
如果卷积将输入 (h,w) 变成了 (h’,w‘)，使用同样超参数的转置卷积将输入从 (h’,w‘) 变成了 (h,w) ，可以认为在超参数相同的情况下，从形状上转置卷积和卷积互为逆变换








重新排列输入和核



1、当填充为 0 ，步幅为 1 时

如果想要变成卷积的话，首先将输入上下左右都填充 k - 1 行或者列（ k 是核窗口的大小），下图中蓝色的区域是填充的行和列
将核矩阵上下、左右翻转
然后做正常卷积（填充 0 ，步幅 1），所得到的结果就等价于转置卷积的输出







2、当填充为 p （对于卷积来讲，加填充就使得输入或者输出变大；对于转置卷积来说，加填充会使得输出变小），步幅为 1 时

将输入填充 k - p - 1 （ k 是核窗口的大小）
将核矩阵上下、左右翻转
然后做正常卷积（填充 0 ，步幅 1），不管做转置卷积的填充是多少，换算成卷积之后，卷积都是正常卷积（填充为 0 ，步幅为 1 ），和输入的填充是无关的
下图所展示的是填充为 1 ，核窗口的大小为 2 的情况







3、当填充为 p ，步幅为 s （在卷积的时候加入步幅会将高宽成倍的减少；对于转置卷积来讲，会将高宽持续增加）时

在行和列之间插入 s - 1 行或列
将输入填充 k - p - 1 （ k 是核窗口的大小）
将核矩阵上下、左右翻转
然后做正常卷积（填充 0 ，步幅 1）
下图所展示的是填充为 0 ，步幅为 2 ，核窗口的大小为 2 的情况（蓝色区域中，外围的一圈是由于填充为 0 导致的，中间的十字是由于步幅为 2 导致的）











形状换算



假设输入高（宽）为 n ，核的大小为 k ，填充为 p ，步幅为 s

转置卷积的输出大小为 n' = sn + k - 2p - s （这里的 p 指的是上填充或者下填充，而没有将上下填充加起来）
如果考虑同样超参数的卷积，则卷积的输出大小为 n' = ⌊(n - k - 2p + s) / s⌋  （“⌊⌋” 表示向下取整，所以在 “⌊⌋” 中的式子不被整除的情况下，n 如果增加一点是不会影响输出的，所以卷积中的 n 如果增加的话，可能会导致最终的输出 n’ 不会有所变化）  --（可以推出在做逆变换的时候）-->  n >= sn' + k - 2p - s  （在 “⌊⌋” 中的式子能够被整除的情况下等号成立，即逆变换的输出和转置卷积输出的计算方式相同）
如果想要使用转置卷积让高宽成倍增加，即 n' = sn ，所以  k - 2p - s ，应该等于 0 ，那么 k = 2p + s

在全连接卷积神经网络中 p 如何取值使得高宽增加 32 倍？已知窗口大小是 64 ，步幅为 32 ，由上式可以得到 p 的值为16 ，这就是为什么在 FCN 中 p 的值为 32








同反卷积的关系



1、数学上的反卷积（deconvolution）是指卷积的逆运算，类似于傅里叶变换和反傅里叶变换互为逆运算



若 Y = conv(X,K)，则 X = deconv(Y,K)


但是转置卷积指的是形状上是逆运算，但是值不是，转置卷积本质上还是一种卷积




2、反卷积很少用在深度学习中



通常所说的反卷积神经网络指的是用了转置卷积的神经网络，而并不是指的是数学上的反卷积








总结



1、转置卷积本质上还是一种卷积，只不过是变化了输入和核，输入通常是加入了很多的 0 来得到上采样（使得输出的高宽变得更大）的目的，而核是进行了翻转

2、转置卷积不等同于数学上的反卷积操作，通常所说的反卷积神经网络实际上使用的是转置卷积，本质上还是一个卷积神经网络，只是对输入和核进行了变换









----end----

1、《动手学深度学习》，课程安排，https://courses.d2l.ai/zh-v2/assets/pdfs/part-2_15.pdf 作者：如果我是泡橘子 https://www.bilibili.com/read/cv16421749/?from=readlist&jump_opus=1 出处：bilibili