---
published: false
layout: post
title: "预告"
categories: 我的AI新书
date: 2023-09-21 00:00:00 +0800
excerpt: "预告"
---


Transformer

Transformer 模型是完全基于注意力机制，没有任何卷积层或循环神经网络
Transformer 最初应用在文本数据上的序列到序列学习，现在已经推广到各种现代的深度学习中，如语言、视觉、语音和强化学习领域




Transformer 架构


基于编码器-解码器的架构来处理序列对，Transformer 的编码器和解码器是基于自注意力的模块叠加而成的，源（source，输入）序列和目标（target，输出）序列的嵌入（embedding）表示通过加上位置编码（positional encoding）加入位置信息，再分别输入到编码器和解码器中

﻿
Transformer P1 - 02:30
﻿
1、Transformer 的编码器是由多个相同的层叠加而成的，每个层都有两个子层（每个子层都采用了残差连接，并且在残差连接的加法计算之后，都使用了层归一化，因此 Transformer 编码器都将输出一个 d 维表示向量）

第一个子层是多头自注意力汇聚

Transformer 块中的多头注意力实际上就是自注意力（自注意力同时具有并行计算和最短的最大路径长度这两个优势）
在计算编码器的自注意力时，key 、value 和 query 的值都来自前一个编码器层的输出
第二个子层是基于位置的前馈网络

Positionwise FFN 实际上是全连接
本质上和编码器-解码器的架构没有本质上的区别，将 Transformer 编码器最后一层的输出作为解码器的输入来完成信息的传递
2、Transformer 解码器也是由多个相同的层叠加而成的，每层都有三个子层，并且在每个子层中也使用了残差连接和层归一化

第一个子层是解码器自注意力（带掩码的多头自注意力）

在解码器自注意力中，key 、value 和 query 都来自上一个解码器层的输出
解码器中的每个位置只能考虑该位置之前的所有位置
带掩码的自注意力保留了自回归的属性，确保预测仅仅依赖于已生成的输出词元（为了在解码器中保留自回归的属性，带掩码的自注意力设定了有效长度（dec_valid_lens）作为参数，以便任何查询都只会与解码器中所有已经生成的词元的位置（即直到该查询为止）进行注意力计算，而不会对当前位置之后的 key-value 对进行注意力计算）
第二个子层是编码器-解码器注意力

除了编码器中所描述的两个子层之外，解码器还在这两个子层之间插入了编码器-解码器注意力层，作为第三个子层，它的 query 来自上一个解码器层的输出，key 和 value 来自整个编码器的输出
第三个子层是基于位置的前馈网络









对比 seq2seq


和使用注意力的 seq2seq 的不同之处在于：Transformer 是纯基于注意力（具体来讲，它是一个纯基于自注意力的架构，里面没有 RNN）
Transformer 将使用注意力的 seq2seq 中的 RNN 全部换成了 Transformer 块








多头注意力（Multi-head attention）


1、对同一个 key 、value 、query 抽取不同的信息

例如短距离关系和长距离关系
2、多头注意力使用 h 个独立的注意力池化

合并各个头（head）输出得到最终输出

key 、value 、query 都是长为 1 的向量，通过全连接层映射到一个低一点的维度，然后进入到注意力模块中









带掩码的多头注意力（Masked Multi-head attention）


1、解码器对序列中一个元素输出时，不应该考虑该元素之后的元素

注意力中是没有时间信息的，在输出中间第 i 个信息的时候，也能够看到后面的所有信息，这在编码的时候是可以的，但是在解码的时候是不行的，在解码的时候不应该考虑该元素本身或者该元素之后的元素
2、可以通过掩码来实现

也就是计算 xi 输出时，假装当前序列长度为 i








基于位置的前馈网络（Positionwise FFN）

基于位置的前馈网络对序列中的所有位置的表示进行变换时使用的是同一个多层感知机（MLP），这就是称前馈网络是基于位置的原因 

其实就是全连接层，将输入形状由（b，n，d）变成（bn，d），然后作用两个全连接层，最后输出形状由（bn，d）变回（b，n，d），等价于两层核窗口为 1 的一维卷积层

b：batchsize
n：序列长度
d：dimension
在做卷积的时候是将 n 和 d 合成一维，变成 nd ；但是现在 n 是序列的长度，会变化，要使模型能够处理任意的特征，所以不能将 n 作为一个特征，因此对每个序列中的每个元素作用一个全连接（将每个序列中的 xi 当作是一个样本）








残差连接和归一化（Add & norm）


1、Add 就是一个 Residual_block


1、加入归一化能够更好地训练比较深的网络，但是这里不能使用批量归一化，批量归一化对每个特征/通道里元素进行归一化

这里的特征指的是每个序列中 D 中的一维，所以在做归一化的时候就是将其方差变 1 ，均值变 0
在做 NLP 的时候，如果选择将 d 作为特征的话，那么批量归一化的输入是 n*b ，b 是批量大小，n 是序列长度，序列的长度是会变的，所以每次做批量归一化的输入大小都不同，所以会导致不稳定，训练和预测的长度本来就不一样，预测的长度会慢慢变长，所以批量归一化不适合长度会变的 NLP 应用
2、层归一化对每个样本里的元素进行归一化


b 代表 batchsize
d 代表特征维度
len 表示序列长度
层归一化和批量归一化的目标相同，但是层归一化是基于特征维度进行归一化的
层归一化和批量归一化的区别在于：批量归一化在 d 的维度上找出一个矩阵，将其均值变成 0 ，方差变成 1，层归一化每次选的是一个元素，也就是每个 batch 里面的一个样本进行归一化
尽管批量归一化在计算机视觉中被广泛应用，但是在自然语言处理任务中，批量归一化通常不如层归一化的效果好，因为在自然语言处理任务中，输入序列的长度通常是变化的
虽然在做层归一化的时候，长度也是变化的，但是至少来说还是在一个单样本中，不管批量多少，都给定一个特征，这样对于变化的长度来讲，稍微稳定一点，不会因为长度变化，导致稳定性发生很大的变化








信息传递


假设编码器中的输出是 y1，... ，yn ，将其作为解码中第 i 个 Transformer 块中多头注意力的 key 和 value

一共有三个多头注意力（包括一个带掩码的多头注意力），位于带掩码的多头注意力与其它两个不同，其他两个都是自注意力（key 、value 和 query 都相同），而它是普通的注意力（它的 key 和 value 来自编码器的输出， query 来自目标序列）
这就意味着编码器和解码器中块的个数和输出维度都是一样的









预测


预测第 t+1 个输出时，解码器中输入前 t 个预测值

在自注意力中，前 t 个预测值作为 key 和 value ，第 t 个预测值还作为 query
关于序列到序列模型，在训练阶段，输出序列的所有位置（时间步）的词元都是已知的；但是在预测阶段，输出序列的次元是逐个生成的

在任何解码器时间步中，只有生成的词元才能用于解码器的自注意力计算中








总结

1、和 seq2seq 有点类似，不同之处在于  Transformer 是一个纯使用注意力的编码-解码器

2、编码器和解码器都有 n 个 Transformer 块

3、每个块里使用多头（自）注意力（multi-head attention），基于位置的前馈网络（Positionwise FFN），残差连接和层归一化

编码器和解码器中各有一个自注意力，但是在编码器和解码器中传递信息的是一个正常的注意力
基于位置的前馈网络使用同一个多层感知机，作用是对所有序列位置的表示进行转换，实际上就是一个全连接，等价于 1*1 的卷积
Add & norm：Add 实际上就是 Residual block 可以帮助将网络做的更深，norm 使用的是 Layer Norm 使得训练起来更加容易；Transformer 中的残差连接和层规范化是训练非常深度模型的重要工具
4、在 Transformer 中，多头注意力用于表示输入序列和输出序列，但是解码器必须通过掩码机制来保留自回归属性









Q&A

1、正余弦编码和自动学习位置编码在最终效果上有区别吗？
﻿
QA P4 - 00:00
﻿
2、在视觉领域，transformer 适合应用在芯片上吗？比如用在车载领域？
﻿
QA P4 - 00:10
﻿
3、请问老师现在在企业里面多轮对话都是用什么算法啊，还是用规则？
﻿
QA P4 - 00:28
﻿
4、多头注意力，concat 和相加取平均，怎么选择呢？
﻿
QA P4 - 00:38
﻿
5、请问老师，在 Transformer 中，为什么在获取词向量之后，需要对词向量进行缩放？（乘以 embedding size 的开方之后再加上 PE ）
﻿
QA P4 - 00:52
﻿
6、老师，请问 NLP 没有竞赛吗？
﻿
QA P4 - 01:33
﻿
7、num of heads 做什么？
﻿
QA P4 - 01:45
﻿
8、后面还会讲 DETR 吗？
﻿
QA P4 - 01:57
﻿
9、老师，请问训练 transformer 的最低硬件要求，如果需要几千张 GPU ，那么大部分人怎么复现它？
﻿
QA P4 - 02:02
﻿
10、DETR 的 query 是否有什么实际的含义，好像和机器翻译的不太一样？
﻿
QA P4 - 02:21
﻿
11、k、v、q 的大小一般怎么选择呢？
﻿
QA P4 - 02:38
﻿
12、老师，pytorch 中的 nn.Embedding() 应该是用的标准正态初始化吧？
﻿
QA P4 - 02:53
﻿
13、老师，看论文时很多模型只使用了 encoder ，没有用 decoder ，想问下区别在哪里，有什么选择依据呢？
﻿
QA P4 - 03:17
﻿
14、transformer 可以处理非序列图像处理吗？
﻿
QA P4 - 03:25
﻿
15、请教是否有多智能体处理 RNN 的论文或者模型推荐，即每个智能体处理一部分序列输出，若都输出合适的序列的话才能获得正奖励，是否有相关的论文或者模型推荐？
﻿
QA P4 - 03:41
﻿
16、请问老师，我想用 BERT 特征做文章抄袭检测，一百万的文章库特征搜索太慢了，老师有什么好的技术方案可以指导一下吗，直接文本比较的话有什么好的算法吗？
﻿
QA P4 - 03:59
﻿
17、自回归和自编码有什么区别？
﻿
QA P4 - 04:57
﻿
18、RNN 处理非序列图像也可以很多个 batch 组成序列去处理吗？
﻿
QA P4 - 05:20
﻿








----end----

其他参考

1、《动手学深度学习》，课程 PPT ，https://courses.d2l.ai/zh-v2/assets/pdfs/part-4_5.pdf

2、《动手学深度学习》，教程，https://zh-v2.d2l.ai/chapter_attention-mechanisms/transformer.html 作者：如果我是泡橘子 https://www.bilibili.com/read/cv19811502/?from=readlist&jump_opus=1 出处：bilibili




**https://peterbloem.nl/blog/transformers**