---
published: false
layout: post
title: "3.机器学习"
categories: 我的AI新书
date: 2023-09-21 00:00:00 +0800
excerpt: "机器学习"
---

**机器学习发展历史**
传统机器学习算法的发展历史可以概括以下几个标志性事件:<br>

1957年,Rosenblatt提出感知机(Perceptron)算法,开创了神经网络和机器学习领域。<br>
1960年代,贝叶斯分类器、决策树、k近邻算法等机器学习方法被提出和发展。<br>
1970年代,回归分析算法广泛应用。统计学习理论也在这一时期产生。<br>
1986年,Rumelhart等人提出了反向传播算法,使神经网络训练成为可能。<br>
1996年,支持向量机(SVM)算法被提出,奠定统计学习理论的地位。<br>
1997年,长短期记忆(LSTM)递归神经网络被提出。<br>
2000年之后,随机森林、AdaBoost、梯度提升决策树等集成算法被广泛应用于实际问题。<br>
2005年,Hinton提出的深度信念网络开创了深度学习的新浪潮。<br>
2010年之后,深度学习算法在语音识别、图像识别等领域取得重大进展。<br>


人工智能就属于应用数学的领域范畴内，下面我们对这本进行简要分析。

第一章主要介绍了书名所涉及到的两大主题：什么是AI，以及AI所涉及到的数学，包括线性代数，概率与统计，微积分，优化算法，本书后续内容也正是围绕这些主题展开。



第二章讲数据，人工智能的意义是赋予机器推理的能力，而这种能力来自于从数据中学习的经验，数据的好坏至关重要。

用数学的话说，就是构造一个数学模型去拟合数据。

数据可以分为真实数据和模拟数据。

从一个已知的概率分布中进行采样，得到的就是模拟数据。

从现实世界中采集的数据称为真实数据，例如，传感器，测量设备，调查等等。

通常假设这些数据是从一个未知的概率分布中独立同分布地采样得到的，训练过程就是求解分布的未知参数。

同样，简单的数据，可以用线性模型去拟合，而对于复杂的数据，则需要拟合能力更强的非线性模型。

第三章讲模型，无论是机器学习还是深度学习，主要就是数据，模型，优化算法铁三角。

第二章讲了数据，这一章讲机器学习中的模型。

当1957年发明的感知器后来被证明无法解决异或问题后，人工智能虽步入寒冬，但并未终结，而是转向另一个领域，1967年决策树算法诞生。

这里称传统机器学习算法，是要与深度学习区分开。虽然两者都是以赋予机器智能为终极目的。

虽然都是要解决分类，回归，降维问题，但深度学习模型要远比机器学习模型复杂。

如果使用机器学习模型，你基本上可以知道模型大概是什么样子的，就算是稍微复杂的集成学习模型，基学习器的类型和个数也是人为设定的。

虽然神经网络的层数和每一层的算子也是人为设定的，但其庞大的参数量以及非线性算子的引入，已经让我们没办法想象或者可视化其拟合函数的样子。

常用的机器学习算法有：

线性回归
逻辑回归
支持向量机
决策树
随机森林
聚类
降维
贝叶斯分类
集成学习算法

虽然机器学习模型的拟合能力不如深度学习，但复杂度越高也就需要更多的数据来拟合，当数据量不多时，机器学习就是一个不错的选择。


第四章讲优化，优化的目的就是要到使目标函数最小化的模型参数。

神经网络模型复杂，优化过程也相对复杂。

在传统的机器学习算法中，普通的线性回归，逻辑回归，SVM都可以看作是凸优化问题，对于凸优化，局部最优解就是全局最优解，只要找到梯度等于零的点就可以了。

神经网络中非凸目标函数以及激活函数的存在，导致其优化问题常为非凸优化问题。

多个层的堆叠导致整个函数就像是个复杂的复合函数。没办法直接求梯度等于0的点。

既然没办法直接求得解析解，那就一步一步来，所幸，有迭代法的存在，才使得参数求解变得简单。

依据什么来更新参数呢？梯度下降法的思路是沿着梯度的负方向走一小段距离。

复合函数则是通过链式法则来更新每一层的参数。

当模型的复杂度太大，而数据量不足时，可能会出现过拟合现象，为了避免过拟合，要么增加数据量，要么需要对模型进行限制，正则化就是限制模型的好方法。

第五章讲卷积神经网络，卷积神经网络属于计算机视觉领域，也就是赋予机器一双洞察世界的双眼。

同时卷积神经网络也是深度学习的一个分支，此外，还有循环神经网络，前馈神经网络。

其实卷积在1980年就已经应用到边缘检测任务中了，有名的有Sobel，Laplacian，Canny算子。但这些算子的选择和调整需要依赖专家经验。还算不上人工智能。

直到1998年Yann LeCun提出的LeNet，才真正引出了卷积神经网络的概念。

与传统的卷积算子相比，最大的区别就是卷积核的参数不用人算，而是机器自主学习。

从线性代数角度看，卷积可以看作是一种线性变换，那么即使是多个层的累加仍然是线性变换啊！线性变换的能力有限，为什么卷积神经网络还那么复杂呢？这里仍然是激活函数的作用，以及dropout等正则化的作用。

第六章讲奇异值分解，可算来到了线性代数的章节，其实在第二章讲数据时就应该讲向量化。

向量化将机器学习带入线性代数世界。

用向量表示输入输出特征，用矩阵表示模型参数，前向计算就可以表示成向量与矩阵的乘法，反向传播过程对矩阵求导，对向量求导。

矩阵分解就是一个普通矩阵分解为具有良好性质的矩阵，例如，将矩阵分解为对角矩阵，对角矩阵有一个非常好的性质，矩阵的任意次方都等于矩阵对角元素的任意次方。

与奇异值分解相似的是特征值分解，后者只用于方阵，而前者则适用于任何矩阵。

PCA就是通过协方差矩阵特征值分解来实现降维。

第七章讲自然语言处理NLP，与计算机视觉CV一样，都是人工智能重要的技术领域，但与CV处理图像矩阵，与ANN处理特征向量都不同，NLP处理时序序列。

数据格式不同，模型自然也就不同，CV应用CNN，ANN应用前馈神经网络，NLP则使用RNN，但RNN不能解决长程依赖问题，以及串行计算效率低下，导致了后续的各种变体的出现，如LSTM，Transformr等。

显然，Transformer以其出色的自注意力机制以及并行处理能力，已经成为当下处理自然语言最成功的模型。

当然，CNN也可以处理时序序列，但自然语言终究是与图像不同，CNN在捕获上下文的能力也不如Transformer，CNN参数共享机制在CV领域是一种优点，但在NLP则变成一种缺点。

Transformer为当下大模型的出现奠定了基础。

语言模型是给定输入序列，例如，一段中文，输出另一段序列，例如，英文翻译。

从概率的视角看，可以有两种方式。

第一种是判别模型，先说下条件概率。

条件概率是指在给定某个事件或条件发生的情况下，另一个事件发生的概率。条件概率用符号P(Y|X)表示，表示在已知X的条件下，Y发生的概率。

判别模型是一种用于预测或分类任务的模型，它关注的是给定输入变量X的情况下，预测输出变量Y的概率分布。判别模型的目标是学习一个条件概率模型P(Y|X)，即在给定输入变量X的情况下，预测输出变量Y的条件概率。

第二种是生成模型，先说下联合概率。

在概率论中，联合概率是指多个随机变量同时取某一组特定取值的概率。假设我们有两个随机变量X和Y，它们的联合概率表示为P(X, Y)。

生成式模型是一种通过学习联合概率分布来生成数据的模型。生成式模型的目标是学习一个概率模型P(X, Y)，其中X表示输入变量，Y表示输出变量。通过学习联合概率分布，生成式模型可以在给定输入变量X的情况下，生成相应的输出变量Y。

等九章讲图模型，图用于构建实体之间关系的模型。实体用节点表示，关系用边表示。

马尔科夫链是一种随机过程,其中下一个状态只依赖于当前状态,不依赖更早的状态。它通过状态转移概率矩阵描述状态之间的转移概率。

随机游走表示按照状态转移概率矩阵进行的随机状态转移过程。每次从当前状态,根据概率转移到下一个状态。

若随机游走进行足够长时间后,状态转移将收敛到一个稳定分布,这个分布称为马尔科夫链的稳态分布。它反映了长时间过程中各状态的概率。

但由于马尔科夫性假设的存在，以及固定的概率转移矩阵，导致其无法对动态的系统建模，处理复杂系统的能力受限。

所以图神经网络出现了。

图神经网络提供了更强大的表示学习和建模能力。用于预测边属性和节点属性。
