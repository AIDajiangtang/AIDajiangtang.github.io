---
published: false
layout: post
title: "预告"
categories: 我的AI新书
date: 2023-09-21 00:00:00 +0800
excerpt: "预告"
---


使用注意力机制的 seq2seq

注意力机制在 NLP 中的应用，也是最早的工作之一




动机


在机器翻译的时候，每个生成的词可能相关于源句子中不同的词
在语言翻译的时候，中文和英文之间的翻译可能会存在倒装，但是可能在西方语言之间，相同意思的句子中的词的位置可能近似地是对应的，所以在翻译句子的某个部位的时候，只需要去看源句子中对应的位置就可以了
然而，Seq2Seq 模型中不能对此直接建模。Seq2Seq 模型中编码器向解码器中传递的信息是编码器最后时刻的隐藏状态，解码器只用到了编码器最后时刻的隐藏状态作为初始化，从而进行预测，所以解码器看不到编码器最后时刻的隐藏状态之前的其他隐藏状态
源句子中的所有信息虽然都包含在这个隐藏状态中，但是要想在翻译某个词的时候，每个解码步骤使用编码相同的上下文变量，但是并非所有输入（源）词元都对解码某个词元有用。将注意力关注在源句子中的对应位置，这也是将注意力机制应用在Seq2Seq 模型中的动机








加入注意力


编码器对每次词的输出（隐藏状态）作为 key 和 value，序列中有多少个词元，就有多少个 key-value 对，它们是等价的，都是第 i 个词元的 RNN 的输出
解码器 RNN 对上一个词的预测输出（隐藏状态）是 query（假设 RNN 的输出都是在同一个语义空间中，所以在解码器中对某个词元进行解码的时候，需要用到的是 RNN 的输出，而不能使用词嵌入之后的输入，因为  key 和 value 也是 RNN 的输出，所以 key 和 query 做匹配的时候，最好都使用 RNN 的输出，这样能够保证它们差不多在同一个语义空间）
注意力的输出和下一个词的词嵌入合并进入 RNN 解码器 
对 Seq2Seq 的改进之处在于：之前 Seq2Seq 的 RNN 解码器的输入是 RNN 编码器最后时刻的隐藏状态，加入注意力机制之后的模型相当于是对所有的词进行了加权平均，根据翻译的词的不同使用不同时刻的 RNN 编码器输出的隐藏状态








总结

1、Seq2Seq 中通过编码器最后时刻的隐藏状态在编码器和解码器中传递信息

2、注意力机制可以根据解码器 RNN 的输出来匹配到合适的编码器 RNN 的输出来更有效地传递信息

3、在预测词元时，如果不是所有输入词元都是相关的，加入注意力机制能够使 RNN 编码器-解码器有选择地统计输入序列的不同部分（通过将上下文变量视为加性注意力池化的输出来实现）









Q&A

1、attention 在搜索的时候是在当前句子搜索，还是所有的文本搜索？
﻿
QA P3 - 00:00
﻿
2、q 是 decoder 的输出，那第一次 q 是怎么得来的？
﻿
QA P3 - 00:27
﻿
3、一般都是在 decoder 加入注意力吗，不可以在 encoder 加入吗？
﻿
QA P3 - 00:53
﻿
4、老师，enc_valid_lens 的值能再讲下如何设置的吗？可以用时间序列为例子吗？
﻿
QA P3 - 01:31
﻿
5、注意力机制是不是和昨天讲的束搜索有些类似？
﻿
QA P3 - 02:23
﻿
6、课程快结束了，沐神可以再推荐一些其他的学习资源吗？
﻿
QA P3 - 02:56
﻿
7、有没有比赛？
﻿
QA P3 - 04:30
﻿
8、能大概说一下图像 attention 吗？
﻿
QA P3 - 05:06
﻿
9、后期有没有方式可以交流？
﻿
QA P3 - 06:24
﻿








---end----

其他参考

1、《动手学深度学习》，教程，https://zh-v2.d2l.ai/chapter_attention-mechanisms/bahdanau-attention.html 作者：如果我是泡橘子 https://www.bilibili.com/read/cv19632720/?from=readlist&jump_opus=1 出处：bilibili