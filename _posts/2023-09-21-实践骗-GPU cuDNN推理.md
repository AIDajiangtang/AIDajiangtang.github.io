---
published: false
layout: post
title: "基于cuDNN的神经网络推理"
categories: 我的AI新书
date: 2023-09-21 00:00:00 +0800
excerpt: "基于cuDNN的神经网络推理"
---

**我现在已经训练了一个神经网络分类模型，骨干网络ResNet50，后接全连接层实现1000类别分类，训练完成后得到onnx格式模型文件，现在不用第三方框架，仅用NVIDIA的cuda和cuDNN实现推理，下面是详细代码**


// 引入必要的头文件
#include <iostream>
#include <vector>
#include <cuda_runtime.h>
#include <cudnn.h>

// 定义一些常量
const int BATCH_SIZE = 1; // 批次大小
const int INPUT_CHANNELS = 3; // 输入通道数
const int INPUT_HEIGHT = 224; // 输入高度
const int INPUT_WIDTH = 224; // 输入宽度
const int OUTPUT_CLASSES = 1000; // 输出类别数

// 定义一个函数，用于检查cuDNN函数的返回值，并打印错误信息
#define checkCUDNN(expression)                               \
{                                                            \
  cudnnStatus_t status = (expression);                       \
  if (status != CUDNN_STATUS_SUCCESS) {                      \
    std::cerr << "Error on line " << __LINE__ << ": "        \
              << cudnnGetErrorString(status) << std::endl;   \
    std::exit(EXIT_FAILURE);                                 \
  }                                                          \
}

// 定义一个函数，用于分配GPU内存并复制数据
void* cuda_malloc_and_copy(void* data, size_t size) {
    void* gpu_data;
    cudaMalloc(&gpu_data, size);
    cudaMemcpy(gpu_data, data, size, cudaMemcpyHostToDevice);
    return gpu_data;
}

// 定义一个函数，用于释放GPU内存
void cuda_free(void* data) {
    cudaFree(data);
}

int main() {
    // 初始化cuDNN环境
    cudnnHandle_t cudnn;
    checkCUDNN(cudnnCreate(&cudnn));

    // 创建输入和输出的张量对象
    size_t input_tensor_size = BATCH_SIZE * INPUT_CHANNELS * INPUT_HEIGHT * INPUT_WIDTH;
    size_t output_tensor_size = BATCH_SIZE * OUTPUT_CLASSES;
    float* input_tensor_values = new float[input_tensor_size];
    float* output_tensor_values = new float[output_tensor_size];

    // 生成一些随机输入数据，这里可以替换为实际的图像数据
    for (unsigned int i = 0; i < input_tensor_size; i++) {
        input_tensor_values[i] = (float)rand() / RAND_MAX;
    }

    // 分配GPU内存并复制输入数据
    void* gpu_input_tensor_values = cuda_malloc_and_copy(input_tensor_values, input_tensor_size * sizeof(float));

    // 创建输入张量描述符，并设置其格式、数据类型和形状
    cudnnTensorDescriptor_t input_descriptor;
    checkCUDNN(cudnnCreateTensorDescriptor(&input_descriptor));
    checkCUDNN(cudnnSetTensor4dDescriptor(input_descriptor,
                                          /*format=*/CUDNN_TENSOR_NHWC,
                                          /*dataType=*/CUDNN_DATA_FLOAT,
                                          /*batch_size=*/1,
                                          /*channels=*/3,
                                          /*image_height=*/INPUT_HEIGHT,
                                          /*image_width=*/INPUT_WIDTH));

    // 创建输出张量描述符，并设置其格式、数据类型和形状
    cudnnTensorDescriptor_t output_descriptor;
    checkCUDNN(cudnnCreateTensorDescriptor(&output_descriptor));
    checkCUDNN(cudnnSetTensor4dDescriptor(output_descriptor,
                                          /*format=*/CUDNN_TENSOR_NHWC,
                                          /*dataType=*/CUDNN_DATA_FLOAT,
                                          /*batch_size=*/1,
                                          /*channels=*/OUTPUT_CLASSES,
                                          /*image_height=*/1,
                                          /*image_width=*/1));

    // 创建卷积层描述符，并设置其超参数和计算模式
    cudnnConvolutionDescriptor_t convolution_descriptor;
    checkCUDNN(cudnnCreateConvolutionDescriptor(&convolution_descriptor));
    checkCUDNN(cudnnSetConvolution2dDescriptor(convolution_descriptor,
                                               /*pad_height=*/1,
                                               /*pad_width=*/1,
                                               /*vertical_stride=*/1,
                                               /*horizontal_stride=*/1,
                                               /*dilation_height=*/1,
                                               /*dilation_width=*/1,
                                               /*mode=*/CUDNN_CROSS_CORRELATION,
                                               /*computeType=*/CUDNN_DATA_FLOAT));

    // 创建卷积层权重描述符，并设置其格式、数据类型和形状
    cudnnFilterDescriptor_t kernel_descriptor;
    checkCUDNN(cudnnCreateFilterDescriptor(&kernel_descriptor));
    checkCUDNN(cudnnSetFilter4dDescriptor(kernel_descriptor,
                                          /*dataType=*/CUDNN_DATA_FLOAT,
                                          /*format=*/CUDNN_TENSOR_NCHW,
                                          /*out_channels=*/32,
                                          /*in_channels=*/3,
                                          /*kernel_height=*/3,
                                          /*kernel_width=*/3));

    // 创建卷积层偏置描述符，并设置其格式、数据类型和形状
    cudnnTensorDescriptor_t bias_descriptor;
    checkCUDNN(cudnnCreateTensorDescriptor(&bias_descriptor));
    checkCUDNN(cudnnSetTensor4dDescriptor(bias_descriptor,
                                          /*format=*/CUDNN_TENSOR_NCHW,
                                          /*dataType=*/CUDNN_DATA_FLOAT,
                                          /*batch_size=*/1,
                                          /*channels=*/32,
                                          /*image_height=*/1,
                                          /*image_width=*/1));

    // 创建激活层描述符，并设置其激活函数类型和参数
    cudnnActivationDescriptor_t activation_descriptor;
    checkCUDNN(cudnnCreateActivationDescriptor(&activation_descriptor));
    checkCUDNN(cudnnSetActivationDescriptor(activation_descriptor,
                                            /*mode=*/CUDNN_ACTIVATION_RELU,
                                            /*reluNanOpt=*/CUDNN_PROPAGATE_NAN,
                                            /*relu_coef=*/0.0));

    // 创建池化层描述符，并设置其池化模式和超参数
    cudnnPoolingDescriptor_t pooling_descriptor;
    checkCUDNN(cudnnCreatePoolingDescriptor(&pooling_descriptor));
    checkCUDNN(cudnnSetPooling2dDescriptor(pooling_descriptor,
                                           /*mode=*/CUDNN_POOLING_MAX,
                                           /*maxpoolingNanOpt=*/CUDNN_PROPAGATE_NAN,
                                           /*windowHeight=*/2,
                                           /*windowWidth=*/2,
                                           /*verticalPadding=*/0,
                                           /*horizontalPadding=*/0,
                                           /*verticalStride=*/2,
                                           /*horizontalStride=*/2));

    // 创建全连接层描述符，并设置其输出维度和偏置模式
    cudnnOpTensorDescriptor_t fully_connected_descriptor;
    checkCUDNN(cudnnCreateOpTensorDescriptor(&fully_connected_descriptor));
    checkCUDNN(cudnnSetOpTensorDescriptor(fully_connected_descriptor, 
                                          CUDNN_OP_TENSOR_ADD, 
                                          CUDNN_DATA_FLOAT, 
                                          CUDNN_PROPAGATE_NAN));

    // 生成一些随机权重和偏置数据，这里可以替换为实际的模型参数
    float* kernel_values = new float[32 * 3 * 3 * 3];
    float* bias_values = new float[32];
    float* fully_connected_weights = new float[32 * 14 * 14 * OUTPUT_CLASSES];
    float* fully_connected_bias = new float[OUTPUT_CLASSES];

    for (unsigned int i = 0; i < 32 * 3 * 3 * 3; i++) {
        kernel_values[i] = (float)rand() / RAND_MAX;
    }

    for (unsigned int i = 0; i < 32; i++) {
        bias_values[i] = (float)rand() / RAND_MAX;
    }

    for (unsigned int i = 0; i < 32 * 14 * 14 * OUTPUT_CLASSES; i++) {
        fully_connected_weights[i] = (float)rand() / RAND_MAX;
    }

    for (unsigned int i = 0; i < OUTPUT_CLASSES; i++) {
        fully_connected_bias[i] = (float)rand() / RAND_MAX;
    }

    // 分配GPU内存并复制权重和偏置数据
    void* gpu_kernel_values = cuda_malloc_and_copy(kernel_values, 32 * 3 * 3 * 3 * sizeof(float));
    void* gpu_bias_values = cuda_malloc_and_copy(bias_values, 32 * sizeof(float));
    void* gpu_fully_connected_weights = cuda_malloc_and_copy(fully_connected_weights, 32 * 14 * 14 * OUTPUT_CLASSES * sizeof(float));
    void* gpu_fully_connected_bias = cuda_malloc_and_copy(fully_connected_bias, OUTPUT_CLASSES * sizeof(float));

    
    // 获取卷积层输出的形状和内存大小
    int batch_size{0};
    int channels{0};
    int height{0};
    int width{0};
    checkCUDNN(cudnnGetConvolution2dForwardOutputDim(convolution_descriptor,
                                                    input_descriptor,
                                                    kernel_descriptor,
                                                    &batch_size,
                                                    &channels,
                                                    &height,
                                                    &width));

    std::cout << "Output Image: " << batch_size << " x " << channels << " x " << height << " x " << width << std::endl;

    size_t convolution_output_size = batch_size * channels * height * width * sizeof(float);

    // 分配GPU内存用于存储卷积层输出
    void* gpu_convolution_output;
    cudaMalloc(&gpu_convolution_output, convolution_output_size);

    // 创建卷积层输出张量描述符，并设置其格式、数据类型和形状
    cudnnTensorDescriptor_t convolution_output_descriptor;
    checkCUDNN(cudnnCreateTensorDescriptor(&convolution_output_descriptor));
    checkCUDNN(cudnnSetTensor4dDescriptor(convolution_output_descriptor,
                                        /*format=*/CUDNN_TENSOR_NHWC,
                                        /*dataType=*/CUDNN_DATA_FLOAT,
                                        /*batch_size=*/batch_size,
                                        /*channels=*/channels,
                                        /*image_height=*/height,
                                        /*image_width=*/width));

    // 选择卷积层前向算法，并获取其工作空间大小
    cudnnConvolutionFwdAlgo_t convolution_algorithm;
    checkCUDNN(cudnnGetConvolutionForwardAlgorithm(cudnn,
                                                input_descriptor,
                                                kernel_descriptor,
                                                convolution_descriptor,
                                                convolution_output_descriptor,
                                                CUDNN_CONVOLUTION_FWD_PREFER_FASTEST,
                                                /*memoryLimitInBytes=*/0,
                                                &convolution_algorithm));

    size_t workspace_bytes{0};
    checkCUDNN(cudnnGetConvolutionForwardWorkspaceSize(cudnn,
                                                    input_descriptor,
                                                    kernel_descriptor,
                                                    convolution_descriptor,
                                                    convolution_output_descriptor,
                                                    convolution_algorithm,
                                                    &workspace_bytes));
    std::cout << "Workspace size: " << (workspace_bytes / 1048576.0) << "MB" << std::endl;

    // 分配GPU内存用于存储工作空间
    void* gpu_workspace;
    cudaMalloc(&gpu_workspace, workspace_bytes);

    // 执行卷积层前向运算，将输入张量、权重张量和偏置张量转换为输出张量
    float alpha = 1.0f, beta = 0.0f;
    checkCUDNN(cudnnConvolutionForward(cudnn, 
                                    &alpha, 
                                    input_descriptor, 
                                    gpu_input_tensor_values, 
                                    kernel_descriptor, 
                                    gpu_kernel_values, 
                                    convolution_descriptor, 
                                    convolution_algorithm, 
                                    gpu_workspace, 
                                    workspace_bytes, 
                                    &beta, 
                                    convolution_output_descriptor, 
                                    gpu_convolution_output));

    // 执行激活层前向运算，将输出张量进行激活函数处理
    checkCUDNN(cudnnActivationForward(cudnn, 
                                    activation_descriptor, 
                                    &alpha, 
                                    convolution_output_descriptor, 
                                    gpu_convolution_output, 
                                    &beta, 
                                    convolution_output_descriptor, 
                                    gpu_convolution_output));

    // 执行池化层前向运算，将输出张量进行池化处理
    checkCUDNN(cudnnPoolingForward(cudnn, 
                                pooling_descriptor, 
                                &alpha, 
                                convolution_output_descriptor, 
                                gpu_convolution_output, 
                                &beta, 
                                convolution_output_descriptor, 
                                gpu_convolution_output));

    // 创建全连接层输入张量描述符，并设置其格式、数据类型和形状
    cudnnTensorDescriptor_t fully_connected_input_descriptor;
    checkCUDNN(cudnnCreateTensorDescriptor(&fully_connected_input_descriptor));
    checkCUDNN(cudnnSetTensor4dDescriptor(fully_connected_input_descriptor,
                                        /*format=*/CUDNN_TENSOR_NCHW,
                                        /*dataType=*/CUDNN_DATA_FLOAT,
                                        /*batch_size=*/batch_size,
                                        /*channels=*/channels * height * width,
                                        /*image_height=*/1,
                                        /*image_width=*/1));

    // 创建全连接层权重张量描述符，并设置其格式、数据类型和形状
    cudnnTensorDescriptor_t fully_connected_weights_descriptor;
    checkCUDNN(cudnnCreateTensorDescriptor(&fully_connected_weights_descriptor));
    checkCUDNN(cudnnSetTensor4dDescriptor(fully_connected_weights_descriptor,
                                        /*format=*/CUDNN_TENSOR_NCHW,
                                        /*dataType=*/CUDNN_DATA_FLOAT,
                                        /*batch_size=*/1,
                                        /*channels=*/OUTPUT_CLASSES,
                                        /*image_height=*/channels * height * width,
                                        /*image_width=*/1));

    // 执行全连接层前向运算，将输入张量和权重张量相乘，并加上偏置张量，得到输出张量
    checkCUDNN(cudnnOpTensor(cudnn, 
                            fully_connected_descriptor, 
                            &alpha, 
                            fully_connected_input_descriptor, 
                            gpu_convolution_output, 
                            &alpha, 
                            fully_connected_weights_descriptor, 
                            gpu_fully_connected_weights, 
                            &beta, 
                            output_descriptor, 
                            gpu_convolution_output));

    checkCUDNN(cudnnAddTensor(cudnn, 
                            &alpha, 
                            bias_descriptor, 
                            gpu_fully_connected_bias, 
                            &alpha, 
                            output_descriptor, 
                            gpu_convolution_output));

    // 复制输出数据到CPU内存
    cudaMemcpy(output_tensor_values, gpu_convolution_output, output_tensor_size * sizeof(float), cudaMemcpyDeviceToHost);

    // 释放GPU内存
    cuda_free(gpu_input_tensor_values);
    cuda_free(gpu_kernel_values);
    cuda_free(gpu_bias_values);
    cuda_free(gpu_fully_connected_weights);
    cuda_free(gpu_fully_connected_bias);
    cuda_free(gpu_convolution_output);
    cuda_free(gpu_workspace);

    // 打印输出数据的前10个元素，这里可以替换为实际的分类结果处理
    std::cout << "Output values: ";
    for (int i = 0; i < 10; i++) {
        std::cout << output_tensor_values[i] << " ";
    }
    std::cout << std::endl;

    // 释放CPU内存
    delete[] input_tensor_values;
    delete[] output_tensor_values;
    delete[] kernel_values;
    delete[] bias_values;
    delete[] fully_connected_weights;
    delete[] fully_connected_bias;

    // 销毁cuDNN对象
    cudnnDestroyTensorDescriptor(input_descriptor);
    cudnnDestroyTensorDescriptor(output_descriptor);
    cudnnDestroyConvolutionDescriptor(convolution_descriptor);
    cudnnDestroyFilterDescriptor(kernel_descriptor);
    cudnnDestroyTensorDescriptor(bias_descriptor);
    cudnnDestroyActivationDescriptor(activation_descriptor);
    cudnnDestroyPoolingDescriptor(pooling_descriptor);
    cudnnDestroyOpTensorDescriptor(fully_connected_descriptor);
    cudnnDestroyTensorDescriptor(fully_connected_input_descriptor);
    cudnnDestroyTensorDescriptor(fully_connected_weights_descriptor);
    cudnnDestroy(cudnn);

    return 0;
}

