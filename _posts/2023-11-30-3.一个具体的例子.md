---
published: true
layout: post
title: "一起学AI 3.一个具体的例子"
categories: 一起学AI

banner:
  video: https://vjs.zencdn.net/v/oceans.mp4
  loop: true
  volume: 0.1
  start_at: 8.5
  image: https://bit.ly/3xTmdUP
  opacity: 0.618
  background: "#000"
  height: "100vh"
  min_height: "38vh"
  heading_style: "font-size: 4.25em; font-weight: bold; text-decoration: underline"
  subheading_style: "color: gold"
excerpt: "一起学AI 3.一个具体的例子"
top: 4
---

[一起学AI 0-序](./2023-11-24-0.序.md)

[一起学AI 1-AI的坎](./2023-11-26-1.人工智能的坎.md)

[一起学AI 2-数学的迷](./2023-11-29-2.数学的迷.md)



## 回顾总结

前三篇文章主要介绍了写“一起学AI”专栏的初衷、AI以及数学的历史和分支。

最终目的是要帮你形成**完整的AI知识体系**。

但像这种综述性的文章不好写，它需要作者有很“广”的知识储备，以及很“深”的见解才能让文章的内容丰富饱满，且结构性、逻辑性更强。这样在读完之后脑子里就会形成一张图。

我不确定这三篇文章是否完成了构建完整的AI知识体系的使命，若您读后仍感迷雾重重,不妨忘却所有细节,仅记住下面这张图就好了。

下面是数学原理，上面是AI应用。


![alt]({{ "assets/images/new book/3/aimldp.png" | relative_url }})


若您希望进一步扩展认知体系,亦可记住以下模型训练流程图。该图清晰描绘了利用数学原理实现AI应用的过程。

![alt]({{ "assets/images/new book/3/UdHXtri.png" | relative_url }})



## 一个具体的例子

若前文与图示仍未解开您所有的疑惑,不妨我们换个思路,从零开始用python代码实现一个模型,使其涵盖所有关键概念。

**首先明确具体需求**——我们希望AI完成什么任务。为便于演示,本文选择一个简单的二分类任务。

除二分类外,常见任务还包括多分类、线性回归,以及CV领域的图像分类、目标检测、图像分割;NLP领域的文本分类、总结、生成等。

**第二步准备数据。**

![alt]({{ "assets/images/new book/3/geogebra-export.png" | relative_url }})

红色的点代表正样本y=1，蓝色的点代表负样本y=0，水平轴$x_{1}$和垂直轴$x_{2}$代表特征。

这种数据我们也称为表格数据，除此之外，还有图像，文本数据。

|x1	|x2	|y|
| ------- | ------- | ------- |
|5.59	|6.98	|1|
|4.83	|6.56	|1|
|5.41|	6.38|	1|
|4.71	|5.88|	1|
|7.03|	6.56|	0|
|6.63	|6.28|	0|
|7|	6	|0|
|6.43	|5.6|	0|
|6.47	|5.06|	0|

通常在开始训练之前,需对数据进行预处理,例如通过相关性分析删除与分类结果无关的特征,删除重复或高度相关的特征,并进行归一化以消除量纲影响等。

为简化过程,本文将忽略数据预处理这一步。

```python
data = [[5.59,6.98,1], [4.83,6.56,1], [5.41,6.38,1], [4.71,5.88,1],[7.03,6.56,1],[6.63,6.28,1],[7,6,1],[6.43,5.6,1],[6.47,5.06,1]]
X = np.array(data)
label = [1,1,1,1,0,0,0,0,0]
y = np.array(label)
```


**第三步,去上面的图片中挑选一个适合任务的模型**。许多模型可完成此二分类任务,如逻辑回归、SVM、决策树、随机森林、Adaboost等传统机器学习模型;也可以使用深度学习中的ANN模型。为简便起见,本文选择最简单的逻辑回归模型。

如前所述,模型的作用是将输入映射到输出。对二分类任务而言,模型只需输出样本属于正类的概率。”

有人发现sigmod函数的性质正好满足这个要求。


$$
f( x) =\frac{1}{1+e^{x}}
$$

sigmoid 函数的输出界于 0-1 之间,符合概率的性质。当 x 大于 0 时,函数输出也将大于 0.5。根据设定,输出超过 0.5 即可判断该样本属于正样本类。

![alt]({{ "assets/images/new book/3/S-curve.png" | relative_url }})

对于我们的任务来讲，可以写成下面形式：

$$
f( x_{1} ，x_{2}) =\frac{1}{1+e^{-( w_{1} x_{1} +w_{2} x_{2} +b)}}
$$

只要$w_{1} x_{1} +w_{2} x_{2} +b\  >0$就可以判定为正样本.

我们也可以认为分类器的决策边界是$w_{1} x_{1} +w_{2} x_{2} +b\ \ =0$这条直线，这样的分类器被称为线性分类器，如果数据线性不可分，则也可以使用非线性函数，例如，多项式函数。


![alt]({{ "assets/images/new book/3/db.png" | relative_url }})

更进一步地，可以将上面公式转换成向量形式。

$$
f(\vec{x}) =\frac{1}{1+e^{-\vec{w}^{T}\vec{x}}}
$$

$\vec{x} =[ x_{1} ,x_{2} ,1] ,\vec{w} =[ w_{1} ,w_{2} ,b]$均为列向量。

这样我们就已经进入到线性代数的领域，然后用线性代数中的工具进行计算了。

向量$\vec{w}$就是要求解的参数，它可以看作是机器智能的精华所在。

```python
# 用theta表示模型的参数W
theta = np.zeros(3)
# 逻辑回归模型
def sigmoid(x):
    return 1 / (1 + np.exp(-x))
```

**第四步开始训练**

首先要选择损失函数，对二分类任务,每个样本仅属于正负样本中的一个类别。对于第i个样本，如果是正样本，概率为$f( X_{i})$,如果是负样本，因为sigmod函数输出的是属于正样本的概率，所以概率为$1-f( X_{i})$

因为所有样本都是独立同分布的，所以对于整个训练数据集来讲，整体概率为单个样本的乘积。


$$ L(\vec{w}) =\\prod _{i=1}^{i=k} f( X_{i})\\prod _{i=k+1}^{i=n}( 1-f( X_{i})) $$

X是所有训练数据组成的矩阵，$X_{i}$表示第i个样本的特征向量。

n为训练样本总数，k为正样本个数。

这实际上就是概率与统计中的似然函数，“似然”就是可能性的意思，**我们的目标就是求得参数W使得L(W)取得最大值**。

$$
Max_{\vec{w}} L(\vec{w})
$$

因为标签$y_{i}$要么是0，要么是1，所以上面的似然函数可以进一步写成下面形式。

$$
L(\vec{w}) =\prod _{i=1}^{i=n} f( X_{i})^{y_{i}}( 1-f( X_{i}))^{1-y_{i}}
$$

多个指数函数的乘积不方便求解。我们可以对其取自然对数,将乘法转换为加法。根据 sigmoid 函数的性质,取对数后不改变函数本身的性质。

由于目标是求函数最大值,可以将函数乘以-1后转化为求最小值问题。同时,对于n个数据,累加值可能非常大,使用梯度下降时则易导致梯度爆炸。为避免这一问题,可将累加式除以样本总数n进行归一化，它也被称为对数损失函数。

$L(\vec{w}) =\frac{1}{n}\sum\limits _{i}^{n} -y_{i} f( X_{i}) -( 1-y_{i}) ln( 1-f( X_{i}))$

如果你了解交叉熵损失函数，那么，上面的公式是不是就很熟悉了？其实，经过证明，使用交叉熵损失函数和使用最大似然估计，两者是等价的。

**现在我们的目标就是求得参数W使得L(W)取得最小值。**

$$
Min_{\vec{w}} L(\vec{w})
$$

```python
# 损失函数    
def compute_loss(X, y, theta):
    y_pred = sigmoid(X.dot(theta))  
    loss = -np.mean(y*np.log(y_pred) + (1-y)*np.log(1-y_pred))
    return loss
```

这时该优化算法出场了，利用梯度下降进行迭代求解。

首先需要L(W)对w求偏导数。

$$
\frac{\partial L(\vec{w})}{\partial \vec{w}} \ =\sum\limits _{i}^{n}( f( X_{i}) -y_{i}) X_{i}
$$

最后对参数进行更新：

$$
\vec{w}_{t+1} =\vec{w}_{t} \ -\ learning\_rate\ *\frac{\partial L(\vec{w})}{\partial \vec{w}}
$$

```python
# 梯度下降    
for i in range(num_iterations):
    y_pred = sigmoid(X.dot(theta))
    gradient = X.T.dot(y_pred - y) / 9
    theta = theta - learning_rate * gradient
    loss = compute_loss(X, y, theta) 
    print(f"Iteration {i}: Loss {loss:.3f}")
```

训练10000轮后，最终得到参数W=[-3.94707138  3.6713051   0.40745513],最后一个是b。





完整代码：

```python
import numpy as np
​
data = [[5.59,6.98,1], [4.83,6.56,1], [5.41,6.38,1], [4.71,5.88,1],[7.03,6.56,1],[6.63,6.28,1],[7,6,1],[6.43,5.6,1],[6.47,5.06,1]]
X = np.array(data)
​
label = [1,1,1,1,0,0,0,0,0]
​
y = np.array(label)
​
print(X.shape)
​
# 参数初始化
num_iterations = 10000
learning_rate = 0.01
theta = np.zeros(3)
# 逻辑回归模型
def sigmoid(x):
    return 1 / (1 + np.exp(-x))
# 损失函数    
def compute_loss(X, y, theta):
    y_pred = sigmoid(X.dot(theta))  
    loss = -np.mean(y*np.log(y_pred) + (1-y)*np.log(1-y_pred))
    return loss
# 梯度下降    
for i in range(num_iterations):
    y_pred = sigmoid(X.dot(theta))
    gradient = X.T.dot(y_pred - y) / 9
    theta = theta - learning_rate * gradient
    loss = compute_loss(X, y, theta) 
    print(f"Iteration {i}: Loss {loss:.3f}")
​
​
print(theta)
```

上面我们从0开始实现了一个二分类模型，实际上，我们可以使用AI框架仅用2行代码就能完成上述任务。

```python
# 创建逻辑回归分类器
log_reg = LogisticRegression()
# 拟合训练集数据
log_reg.fit(X_train, y_train)   
```

后面我会通过一篇文章介绍AI常用的工具框架，工欲善其事，必先利其器。磨刀不误砍柴工。

## 展望未来
完整的知识体系对后面展开的细节是非常重要的，这也是我为什么费尽心思的让你去构建它的原因。
对于AI的学习路径,我们面临着两种选择——自底向上或自上而下。

传统教学往往采取前者,先建立数学基础,再介绍具体模型与应用。就像建造房屋,先打好地基。这符合人们的直觉。

但是,对AI学习来说,我认为后者会更为实用。我们并不需要完整掌握深奥的数学原理才能进入AI应用的大门。事实上,许多成功的AI从业者都是从实际问题出发,根据遇到的困难与需求来驱动对理论知识的学习。

举个例子,机器学习中的线性回归模型,实现一个简单的预测就可入门;遇到过拟合等问题时,我们再回头细究正则化等理论,这样学习才会更有目标性。

相比之下,如果先学习完所有复杂度理论与数学推导,等真正运用时,脱离实践场景,很多数学原理提出时间要比AI早很多，因为它们不是转为AI而生的。我们仍难以将原理与应用有机联系起来,达不到事半功倍的效果。

所以,我主张对AI学习采取自上而下的方法会更为高效,从应用入手,需要时再探索原理。这样既可快速建立实践能力,又不失去深入理解的动力。





