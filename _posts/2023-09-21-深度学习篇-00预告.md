---
published: false
layout: post
title: "预告"
categories: 我的AI新书
date: 2023-09-21 00:00:00 +0800
excerpt: "预告"
---


预告



深度学习是人工智能最热的领域，人工智能在过去十年中最热的突破主要来自深度学习
深度学习的核心是神经网络，它是一个非常灵活的框架，允许通过组合不同的神经元来表达对一个问题不同的先验知识，在很大程度上，神经网络可以看成是一门语言
深度学习的关键是要动手


《动手学深度学习》

课程安排（含ppt）：https://courses.d2l.ai/zh-v2/
教材：https://zh-v2.d2l.ai/index.html



----end---- 作者：如果我是泡橘子 https://www.bilibili.com/read/cv15300846/?from=readlist&jump_opus=1 出处：bilibili

**既然已经有了传统的机器学习算法，为什么还会出现深度学习呢？**
虽然传统机器学习算法在某些领域表现不错,但随着数据规模和任务复杂性的增长,其Performance也面临瓶颈。深度学习产生的主要原因有:

传统方法依赖人工特征工程,需要大量域知识,不够通用。而深度学习可以自动学习特征表示。
多层非线性结构使得深度网络拥有更强大的特征学习和建模能力。
新的激活函数、优化算法、正则化手段提高了深度网络的训练性能。
大量标注数据的出现提供了训练复杂模型的素材。
GPU等硬件的发展使大规模网络训练成为可能。
新颖网络结构不断得到探索,如ResNet解决梯度消失等问题。
注意力机制、图网络等为网络引入新思路。
在CV、NLP等领域,深度学习已超过传统方法,提供了更强的能力。
深度学习框架的发展降低了应用门槛。
综上,深度学习克服了传统方法的局限,提供了一个更强大、通用的代表复杂函数的框架,理论和工程实现上都取得了长足进展。这使其成为前沿的人工智能技术。

**如果模型很小，但数据量很大，会有什么后果？**
如果模型规模很小,但训练数据量非常大,通常会导致以下几个后果:

欠拟合(Underfitting)
小模型的表达能力有限,无法充分学习到训练数据中的模式,导致欠拟合。

高方差(High Variance)
小模型容易被数据中的随机噪声影响,不同数据会产生很不同的结果,导致高方差。

误差下降缓慢
在非常大的数据集上,小模型只能慢慢地从中获取信息,误差下降速度较慢。

时间和算力浪费
小模型难以充分利用大数据的信息,造成时间和计算资源的浪费。

模型欠复杂
小模型可能无法表示数据集中的复杂模式,限制了性能上限。

特征不充分
小模型学习到的特征比较肤浅和不充分。

验证集错误高
验证集的性能通常不会很理想,与训练集存在差距。

为避免上述问题,模型大小通常需要与数据规模相匹配,大数据需要选择足够复杂的模型以获取信息并减小方差。


**神经网络来了**
人类的感知器官能够观察世界，能够聆听声音，能够用话语表达，这就是造物主赋予人类耳，鼻，眼的意义。

既然要赋予机器人类的智能，那就是要让机器具有类似的能力。

现实世界的数据也无非三种种，表格数据，图像，声音文本。

AI也用三种网络去处理这三种数据，ANN，CNN，RNN。
但是这三种网络并不是井水不犯河水。
例如，ANN虽然善于处理表格数据，例如，波士顿房价预测。但如果将图像抻平了，同样可以用ANN来处理。

像声音文本这类时序数据，用RNN类型的网络最适合，但同样也可以用CNN来处理，只不过，CNN是用堆积多个层来实现时序数据中的长程依赖问题，这在图像领域被称为感受野。

同样RNN网络也涉足图像领域，不光是涉足，而且大有后来者居上的意味。
基于Transformer的Vit现在已经在很多数据集上拿下state of the art,但因为模型结构原因，要想取得好成绩，就要依赖大量的数据，如果这个问题能解决，估计后续就没CNN什么事了。

具体使用什么网络来处理，要看你把数据表现成什么形式，例如，一幅图像，可以用矩阵表示，那用CNN处理作为合适，但如果你把它打成一个个patch，那用Transformer最合适，如果你把它像素当作节点，那么就需要用图神经网络处理。

文本也是如此，你把它当作时序数据，用RNN最合适，但如果你把一个句子当作一个1*N的矩阵，那就可以用CNN处理了，如果你把每个单词看作节点，那就可以用GNN处理。

所以，事情是没有绝对界限的。


**请你帮我列出CNN，RNN，ANN的起源**
你好，这是一个很有意思的问题。CNN，RNN，ANN是三种不同类型的神经网络，它们在深度学习领域都有着广泛的应用和发展。它们的起源可以追溯到以下几个关键的时间点和论文：

- ANN（人工神经网络）是最早的神经网络类型，它受到生物神经系统的启发，由多个感知器（或神经元）组成，可以执行分类或回归等任务。ANN的起源可以追溯到1943年，当时McCulloch和Pitts提出了一种简单的二进制神经元模型1。1958年，Rosenblatt提出了感知器算法2，可以通过学习权重来实现线性分类。1969年，Minsky和Papert指出了感知器算法的局限性3，导致神经网络研究陷入低潮。1986年，Rumelhart等人提出了反向传播算法，可以训练多层感知器（MLP），从而使神经网络研究重新兴起。
- CNN（卷积神经网络）是一种特殊的神经网络类型，它利用卷积核来提取输入数据中的局部特征，并具有平移不变性和参数共享等优点。CNN主要用于处理图像、语音、文本等高维数据。CNN的起源可以追溯到1980年，当时Fukushima提出了Neocognitron，受到生物视觉系统的启发，采用了分层结构和局部连接方式，模拟了人类对图像特征的提取过程4。1989年，LeCun提出了LeNet-5，结合了反向传播算法和梯度下降法来训练网络参数，并成功应用于手写数字识别等任务。2012年，Krizhevsky等人提出了AlexNet，在ImageNet图像分类竞赛中大幅度超越了其他算法，并引发了深度学习领域的革命。
- RNN（循环神经网络）是一种特殊的神经网络类型，它在隐藏层上有一个循环连接，使得网络能够处理序列数据，并记忆之前的信息。RNN主要用于处理时间序列、文本、语音等数据。RNN的起源可以追溯到1986年，当时Jordan提出了一种带有输出反馈连接的循环网络。1989年，Williams和Zipser提出了一种基于梯度下降法的RNN训练算法。1990年，Elman提出了一种带有隐藏状态反馈连接的循环网络。1997年，Hochreiter和Schmidhuber提出了长短期记忆（LSTM）网络，解决了RNN在处理长序列时的梯度消失或爆炸问题。2014年，Cho等人提出了门控循环单元（GRU）网络，简化了LSTM的结构和计算。


**万能逼近定理**
该定理表明,含有一个隐层的前馈神经网络(多层感知机),如果给予足够多的节点,以及适当的权重和偏置参数,就可以逼近任何连续函数，深度学习的底气。