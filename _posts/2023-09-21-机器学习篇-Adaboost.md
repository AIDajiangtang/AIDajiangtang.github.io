---
published: false
layout: post
title: "对比学习常见问题"
categories: 我的AI新书
date: 2023-09-21 00:00:00 +0800
excerpt: "对比学习常见问题"
---
https://blog.csdn.net/joejoeqian/article/details/129764067

Adaboost
0.参考内容及思维导图链接
1.集成学习
1.1 思想
1.1.1 类比
1.1.2 补充概念
1.2 优点
1.3 需要关注的问题
1.3.1 个体学习器如何训练得到？
1.3.2 如何将个体学习器组合？
1.4 分类
1.4.1 对于boosting
1.4.2 对于bagging
2.Adaboost
2.1 参考资料
2.2 思路
2.2.1 数学表达
2.2.2 基本思路
2.3 算法流程
2.3.1 基本流程
2.3.2 例题
2.4 加法模型
2.4.1 预测函数
2.4.2 损失函数的设计
2.5 算法原理
2.5.1 需要优化的问题：二分类
2.5.2 模型：加法模型
2.5.3 最终分类器
2.5.4 损失函数：指数损失函数
2.5.5 优化方法：前向分步算法
3.四个问题
3.1 如何计算学习误差率e m e_me 
m
​
 ?
3.2 如何得到基学习器权重系数α \alphaα？
3.3 如何更新样本权重w ww?
3.4 使用何种结合策略？
4.小结
0.参考内容及思维导图链接
参考连接：AdaBoost 作者：老弓的学习日记
思维导图：https://github.com/QianJoe/Ensemble-Learning
1.集成学习
1.1 思想
集成学习通过构建并结合多个学习器来完成学习任务

1.1.1 类比
三个臭皮匠，顶个诸葛亮
如下图：

1.1.2 补充概念
在集成学习中，学习器相当于模型，又分为强学习器和弱学习器，弱分类器又叫基分类器。

强学习器：可以认为它是一种准确率很高但相对复杂的模型，比如一些神经网络构建出来的模型，它耗费时间精力多，对算力要求高，花费代价高；
弱学习器：可以认为它是一种比较简单的模型，预测效果不太好，如逻辑回归等简单的模型。
总结就是三个臭皮匠顶个诸葛亮



1.2 优点
集成学习通过将多个学习器进行结合，常常可以获得比单一学习器更加显著的优越性能。

1.3 需要关注的问题
1.3.1 个体学习器如何训练得到？
改变训练数据的权值或者概率分布，如何改变？
个体学习器就是弱学习器，多个弱学习器学到的东西必须是有所差异的，它们各自的强项不一样。

举例：

一个班级里有G 1 ( x ) , ⋯   , G n ( x ) G_1(x),\cdots,G_n(x)G 
1
​
 (x),⋯,G 
n
​
 (x)，其中G 1 ( x ) G_1(x)G 
1
​
 (x)语文好，G 2 ( x ) G_2(x)G 
2
​
 (x)英语好，⋯ \cdots⋯，G n ( x ) G_n(x)G 
n
​
 (x)数学好，将他们各自好的学科组合起来，就可以抗衡学校里年级第一的学生了。

所以只需要让各学习器，学习不一样的数据就可以了，继续上面的例子

其中G 1 ( x ) G_1(x)G 
1
​
 (x)语文好，所以让G 1 ( x ) G_1(x)G 
1
​
 (x)侧重的去学习语文（花更多时间），G 2 ( x ) G_2(x)G 
2
​
 (x)英语好，所以让G 2 ( x ) G_2(x)G 
2
​
 (x)侧重的去学习英语，⋯ \cdots⋯，G n ( x ) G_n(x)G 
n
​
 (x)数学好，所以让G n ( x ) G_n(x)G 
n
​
 (x)侧重的去学习数学。虽然所有学生各科目都学，但是确保发挥各个学生擅长，改变大家的侧重点，比如在固定学习时长中，增长其擅长科目所占时间。

这样相当于改变了训练数据的权值或者概率分布。

1.3.2 如何将个体学习器组合？
是通过线性相加，还是其他方法？
后面会详细说

1.4 分类
当训练和组合有不一样的办法时，就得到两种类别：

boosting
bagging
1.4.1 对于boosting

1.主要特点
个体学习器间存在强依赖关系、必须串行生成的序列化方法。

2.工作机制
也就是如何解决训练和组合问题：

1.提高那些在前一轮被弱分类器分错的样本的权值；减少那些在前一轮被弱分类器分对的样本的权值，使得误分的样本在后续受到更多的关注，体现了串行。
举例：
如你有3 33个分身A , B , C A,B,CA,B,C，期末考试需要考语数英三门。在平时学习时，你先让A AA去学习语文，然后通过平时的测验发现A AA学习语文不错，但是学习数学和英语不行，这时，你可以派B BB去学习，减少对语文的学习的时间，增多数学和英语的时间，然后通过平时的测验发现B BB数学学的不错，这时你可以派C CC去学习，减少对数学和语文的学习的时间，增多英语的时间。

2.加法模型将弱分类器进行线性组合
3.四个问题

1.如何计算学习误差率e m e_me 
m
​
 ?
2.如何得到基学习器权重系数α \alphaα？
3.如何更新样本权重w ww?
4.使用何种结合策略？
1.4.2 对于bagging

1.特点
个体学习器间不存在强依赖关系，可同时生成的并行化方。

2.工作机制

1.从原始样本集中抽取k个训练集
每轮从原始样本集中使用Boostraping法（自助法，是一种有放回的抽样方法，可能抽到重复的样本）抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中），共进行j轮抽取，得到k个训练集（k个训练集相互独立）。
随机森林中，还会随机抽取一定数量的特征。
举例：待补充。
2.k个训练集分别训练，共得到k个模型，体现了并行。
3.将上步得到的k个模型，通过一定方式组合起来
分类问题：将上步得到的k个模型采用投票的方式得到分类结果。
回归问题：计算上述模型的均值作为最后的结果。
3.代表
随机森林

2.Adaboost
Adaboost解决的是二分类问题。

2.1 参考资料
《机器学习方法》作者：李航

2.2 思路
2.2.1 数学表达
表达式
f ( x ) = ∑ m = 1 M α m G m ( x ) = α 1 G 1 ( x ) + ⋯ + α m G m ( x ) + ⋯ + α M G M ( x )
f(x)=∑m=1MαmGm(x)=α1G1(x)+⋯+αmGm(x)+⋯+αMGM(x)
�
(
�
)
=
∑
�
=
1
�
�
�
�
�
(
�
)
=
�
1
�
1
(
�
)
+
⋯
+
�
�
�
�
(
�
)
+
⋯
+
�
�
�
�
(
�
)
f(x)
​
  
= 
m=1
∑
M
​
 α 
m
​
 G 
m
​
 (x)
=α 
1
​
 G 
1
​
 (x)+⋯+α 
m
​
 G 
m
​
 (x)+⋯+α 
M
​
 G 
M
​
 (x)
​
 

其中α m \alpha_mα 
m
​
 由G m ( x ) G_m(x)G 
m
​
 (x)的"分类误差率"决定，训练样本G m ( x ) G_m(x)G 
m
​
 (x)：提高前一轮“错误分类”的样本的权值，降低前一轮“正确分类”的样本的权值。
注意到，α m \alpha_mα 
m
​
 不是之前说的权重值，之前说的权值是w m w_mw 
m
​
 。

2.2.2 基本思路
1.每一轮中，分别记录好那些被当前弱分类器正确分类和错误分类的样本，在下一轮训练时，提高错误分类样本的权值，同时降低正确分类样本的权重，用以训练新的弱分类器。这样一来，那些没有被正确分类的数据，由于其权值加大，会受到后一轮的更大关注。

如何理解改变样本的权值?

举例：
初始有3个样本，即其所占比例
D 1 D_1D 
1
​
 ：

样本	( x 1 , y 1 ) (x_1,y_1)(x 
1
​
 ,y 
1
​
 )	( x 2 , y 2 ) (x_2,y_2)(x 
2
​
 ,y 
2
​
 )	( x 3 , y 3 ) (x_3,y_3)(x 
3
​
 ,y 
3
​
 )
所占比例	1 3 \frac{1}{3} 
3
1
​
 	1 3 \frac{1}{3} 
3
1
​
 	1 3 \frac{1}{3} 
3
1
​
 
经过一轮学习后，G 1 G_1G 
1
​
 分类器成功将( x 2 , y 2 ) , ( x 3 , y 3 ) (x_2,y_2),(x_3,y_3)(x 
2
​
 ,y 
2
​
 ),(x 
3
​
 ,y 
3
​
 )分类成功，所以我们需要将这两个样本的权重降低，提高第一个样本的权值，即下面表格：

D 2 D_2D 
2
​
 ：

样本	( x 1 , y 1 ) (x_1,y_1)(x 
1
​
 ,y 
1
​
 )	( x 2 , y 2 ) (x_2,y_2)(x 
2
​
 ,y 
2
​
 )	( x 3 , y 3 ) (x_3,y_3)(x 
3
​
 ,y 
3
​
 )
所占比例	2 3 \frac{2}{3} 
3
2
​
 	1 6 \frac{1}{6} 
6
1
​
 	1 6 \frac{1}{6} 
6
1
​
 
所以增大权值相当于将初始样本中的错误分类的数量变多了，变成了这样：

{ ( x 1 , y 1 ) ( x 2 , y 2 ) ( x 3 , y 3 )
⎧⎩⎨(x1,y1)(x2,y2)(x3,y3)
{
(
�
1
,
�
1
)
(
�
2
,
�
2
)
(
�
3
,
�
3
)
⎩
⎨
⎧
​
  
(x 
1
​
 ,y 
1
​
 )
(x 
2
​
 ,y 
2
​
 )
(x 
3
​
 ,y 
3
​
 )
​
  → \rightarrow→ { ( x 1 , y 1 ) , ( x 1 , y 1 ) , ( x 1 , y 1 ) , ( x 1 , y 1 ) ( x 2 , y 2 ) ( x 3 , y 3 )
⎧⎩⎨(x1,y1),(x1,y1),(x1,y1),(x1,y1)(x2,y2)(x3,y3)
{
(
�
1
,
�
1
)
,
(
�
1
,
�
1
)
,
(
�
1
,
�
1
)
,
(
�
1
,
�
1
)
(
�
2
,
�
2
)
(
�
3
,
�
3
)
⎩
⎨
⎧
​
  
(x 
1
​
 ,y 
1
​
 ),(x 
1
​
 ,y 
1
​
 ),(x 
1
​
 ,y 
1
​
 ),(x 
1
​
 ,y 
1
​
 )
(x 
2
​
 ,y 
2
​
 )
(x 
3
​
 ,y 
3
​
 )
​
 

2.加权多数表决

1.加大分类误差率小的弱分类器的权值α m \alpha_mα 
m
​
 ，使其在表决中起较大作用；
2.减少分类误差率大的弱分类器的权值α m \alpha_mα 
m
​
 ，使其在表决中起较小作用。
2.3 算法流程
2.3.1 基本流程
1.二分类训练数据集
T = { ( x 1 , y 1 ) , ⋯   , ( x N , y N ) } T=\{(x_1,y_1),\cdots,(x_N,y_N)\}T={(x 
1
​
 ,y 
1
​
 ),⋯,(x 
N
​
 ,y 
N
​
 )}，其中，每个样本点由实例与标记组成。实例x i ∈ X ⊆ R n x_i\in \mathcal{X}\subseteq R^nx 
i
​
 ∈X⊆R 
n
 ，标记y i ∈ Y = { − 1 , + 1 } y_i\in \mathcal{Y}=\{-1,+1\}y 
i
​
 ∈Y={−1,+1}，X \mathcal{X}X是实例空间，Y \mathcal{Y}Y是标记集合。
2.定义基分类器（弱分类器）G ( x ) G(x)G(x)
大部分情况下，G 1 ∼ G m G1\sim GmG1∼Gm都是同一类型的分类器。
比如：G 1 ( x ) = { 1 , x < k − 1 , x > k G_1(x)=
{1,−1,x<kx>k
{
1
,
�
<
�
−
1
,
�
>
�
G 
1
​
 (x)={ 
1,
−1,
​
  
x<k
x>k
​
 ，其中k kk为阈值
3.具体算法流程

1.初始化/更新当前训练数据的权值分布

如果是初始化： D 1 = ( w 11 , w 12 , ⋯   , w 1 N ) , w 1 i = 1 N , i = 1 , 2 , ⋯   , N ) D_1=(w_{11},w_{12},\cdots,w_{1N}),w_{1i}=\frac{1}{N},i=1,2,\cdots,N)D 
1
​
 =(w 
11
​
 ,w 
12
​
 ,⋯,w 
1N
​
 ),w 
1i
​
 = 
N
1
​
 ,i=1,2,⋯,N)，初始化权值（这个是训练数据的权值，不是基分类器的）使用数量的平均值1 N \frac{1}{N} 
N
1
​
 。

如果是更新（不是第一次）：
，其中w m , i = { w m − 1 , i Z m − 1 e − α m − 1 , G m − 1 ( x i ) = y i w m − 1 , i Z m − 1 e α m − 1 , G m − 1 ( x i ) ≠ y i w_{m,i}=
{wm−1,iZm−1e−αm−1,Gm−1(xi)=yiwm−1,iZm−1eαm−1,Gm−1(xi)≠yi
{
�
�
−
1
,
�
�
�
−
1
�
−
�
�
−
1
,
�
�
−
1
(
�
�
)
=
�
�
�
�
−
1
,
�
�
�
−
1
�
�
�
−
1
,
�
�
−
1
(
�
�
)
≠
�
�
w 
m,i
​
 ={ 
Z 
m−1
​
 
w 
m−1,i
​
 
​
 e 
−α 
m−1
​
 
 ,G 
m−1
​
 (x 
i
​
 )=y 
i
​
 
Z 
m−1
​
 
w 
m−1,i
​
 
​
 e 
α 
m−1
​
 
 ,G 
m−1
​
 (x 
i
​
 )

=y 
i
​
 
​
 ，由此可知，被基本分类器G m ( x ) G_m(x)G 
m
​
 (x)误分类样本的权值得以扩大（e α m − 1 > 1 e^{\alpha_{m-1}}>1e 
α 
m−1
​
 
 >1），而被正确分类样本的权值却在缩小（e − α m − 1 < 1 e^{-\alpha_{m-1}}<1e 
−α 
m−1
​
 
 <1）。规范化因子Z m − 1 Z_{m-1}Z 
m−1
​
 通俗地讲，是将更新后的权重之和归为 1，保证权重序列以一个离散的概率分布出现。

2.训练当前基分类器G m ( x ) G_m(x)G 
m
​
 (x)

使用具有权值分布D m D_mD 
m
​
 的训练数据集学习，得到基分类器G m ( x ) G_m(x)G 
m
​
 (x)。比如，你选择的是逻辑回归模型，就要交叉熵作为损失函数，用梯度下降作为优化方法来训练。

3.计算当前基分类器的权值α m \alpha_mα 
m
​
 

1.先要计算当前G m ( x ) G_m(x)G 
m
​
 (x)在训练集上的分类误差率
e m = ∑ i = 1 N P ( G m ( x i ) ≠ y i ) = ∑ i = 1 N w m i I ( G m ( x i ) ≠ y i ) = ∑ G m ( x i ) ≠ y i w m i e_m=\sum_{i=1}^NP(G_m(x_i)\neq y_i)=\sum_{i=1}^N w_{mi} I(G_m(x_i)\neq y_i)=\sum_{G_m(x_i)\neq y_i}w_{mi}e 
m
​
 =∑ 
i=1
N
​
 P(G 
m
​
 (x 
i
​
 )

=y 
i
​
 )=∑ 
i=1
N
​
 w 
mi
​
 I(G 
m
​
 (x 
i
​
 )

=y 
i
​
 )=∑ 
G 
m
​
 (x 
i
​
 )

=y 
i
​
 
​
 w 
mi
​
 ，其中I II为指示函数，I = { 1 , G m ( x i ) ≠ y i 0 , G m ( x i ) = y i I=
{1,0,Gm(xi)≠yiGm(xi)=yi
{
1
,
�
�
(
�
�
)
≠
�
�
0
,
�
�
(
�
�
)
=
�
�
I={ 
1,
0,
​
  
G 
m
​
 (x 
i
​
 )

=y 
i
​
 
G 
m
​
 (x 
i
​
 )=y 
i
​
 
​
 
其中e m e_me 
m
​
 一定满足0 ≤ e m ≤ 0.5 0\leq e_m \leq 0.50≤e 
m
​
 ≤0.5，下面是简单的解释：
基分类器的权重α m = 1 2 log ⁡ 1 − e m e m \alpha_m=\frac{1}{2}\log\frac{1-e_m}{e_m}α 
m
​
 = 
2
1
​
 log 
e 
m
​
 
1−e 
m
​
 
​
 ，e m e_me 
m
​
 越大，α m \alpha_mα 
m
​
 越小，也就是说，误差率小的基分类器权重越大，当e m < 0.5 e_m<0.5e 
m
​
 <0.5，α m > 0 \alpha_m>0α 
m
​
 >0；当e m > 0.5 e_m>0.5e 
m
​
 >0.5，α m < 0 \alpha_m<0α 
m
​
 <0，但是必有α m > 0 \alpha_m>0α 
m
​
 >0，所以分类误差率e m < 0.5 e_m<0.5e 
m
​
 <0.5，如果e m > 0.5 e_m>0.5e 
m
​
 >0.5，那就把该分类器的结果都取一个相反的结果，这样一来取反后的分类器误差率还是小于0.5 0.50.5。

举例：


4.将α m ⋅ G m ( x ) \alpha_m\cdot G_m(x)α 
m
​
 ⋅G 
m
​
 (x)更新到加法模型f ( x ) f(x)f(x)中

f ( x ) = α 1 G 1 ( x ) + ⋯ + α m G m ( x ) f(x)=\alpha_1G_1(x)+\cdots+\alpha_m G_m(x)f(x)=α 
1
​
 G 
1
​
 (x)+⋯+α 
m
​
 G 
m
​
 (x)

5.判断是否满足循环退出条件，如定义的M次或者模型精度

分类器个数是否达到M
总分类器误差率是否低于设定的精度（s i g n ( f ( x ) ) sign(f(x))sign(f(x))）
2.3.2 例题
1.二分类训练数据集

2.定义基分类器（弱分类器）
1.基分类器定义如下：
G 1 ( x ) = { 1 , x < k − 1 , x > k G_1(x)=
{1,−1,x<kx>k
{
1
,
�
<
�
−
1
,
�
>
�
G 
1
​
 (x)={ 
1,
−1,
​
  
x<k
x>k
​
 
，其中k kk为阈值
2.训练方法

在x中划分出候选阈值（如图中红色箭头指向的所示），
从中选出使得误差率e m e_me 
m
​
 最小的，作为我们最终阈值构建好G ( x ) G(x)G(x)：

根据上图选择阈值为2.5 2.52.5，这时分错3个，误差率e m = 3 10 e_m=\frac{3}{10}e 
m
​
 = 
10
3
​
 

3.循环M次

M=1时：
1.初始化/更新当前训练数据的权值分布

初始化:D 1 = ( w 11 , w 12 , ⋯   , w 1 N ) , w 1 i = 1 N , i = 1 , 2 , ⋯   , N ) D_1=(w_{11},w_{12},\cdots,w_{1N}),w_{1i}=\frac{1}{N},i=1,2,\cdots,N)D 
1
​
 =(w 
11
​
 ,w 
12
​
 ,⋯,w 
1N
​
 ),w 
1i
​
 = 
N
1
​
 ,i=1,2,⋯,N)
2.训练当前基分类器

使用具有权值分布D m D_mD 
m
​
 的训练数据集学习，得到基分类器G m ( x ) G_m(x)G 
m
​
 (x)
训练方法：1.在x xx中划分出各候选阈值，构建好候选G m ( x ) G_m(x)G 
m
​
 (x)；2.找到使误差e m e_me 
m
​
 最小的G m ( x ) G_m(x)G 
m
​
 (x)，作为本轮的基分类器。
在权值分布为D 1 D_1D 
1
​
 的训练数据上，阈值k kk取2.5时分类误差最低，故基本分类器为
G 1 ( x ) = { 1 , x > 2.5 − 1 , x < 2.5 G_1(x)=
{1,−1,x>2.5x<2.5
{
1
,
�
>
2.5
−
1
,
�
<
2.5
G 
1
​
 (x)={ 
1,
−1,
​
  
x>2.5
x<2.5
​
 
3.计算当前基分类器的权值

1.计算当前G m ( x ) G_m(x)G 
m
​
 (x)在训练集上的分类误差率
e m = ∑ i = 1 N P ( G m ( x i ) ≠ y i ) = ∑ i = 1 N w m i I ( G m ( x i ) ≠ y i ) = ∑ G m ( x i ) ≠ y i w m i e_m=\sum_{i=1}^NP(G_m(x_i)\neq y_i)=\sum_{i=1}^N w_{mi} I(G_m(x_i)\neq y_i)=\sum_{G_m(x_i)\neq y_i}w_{mi}e 
m
​
 =∑ 
i=1
N
​
 P(G 
m
​
 (x 
i
​
 )

=y 
i
​
 )=∑ 
i=1
N
​
 w 
mi
​
 I(G 
m
​
 (x 
i
​
 )

=y 
i
​
 )=∑ 
G 
m
​
 (x 
i
​
 )

=y 
i
​
 
​
 w 
mi
​
 ，其中I II为指示函数，I = { 1 , G m ( x i ) ≠ y i 0 , G m ( x i ) = y i I=
{1,0,Gm(xi)≠yiGm(xi)=yi
{
1
,
�
�
(
�
�
)
≠
�
�
0
,
�
�
(
�
�
)
=
�
�
I={ 
1,
0,
​
  
G 
m
​
 (x 
i
​
 )

=y 
i
​
 
G 
m
​
 (x 
i
​
 )=y 
i
​
 
​
 

2.根据分类误差率e m e_me 
m
​
 ，计算基分类器G m ( x ) G_m(x)G 
m
​
 (x)的权重系数

3.计算G 1 ( x ) G_1(x)G 
1
​
 (x)的系数：α 1 = 1 2 log ⁡ 1 − e 1 e 1 = 0.4236 \alpha_1=\frac{1}{2}\log \frac{1-e_1}{e_1}=0.4236α 
1
​
 = 
2
1
​
 log 
e 
1
​
 
1−e 
1
​
 
​
 =0.4236

4.将α m ⋅ G m ( x ) \alpha_m\cdot G_m(x)α 
m
​
 ⋅G 
m
​
 (x)更新到加法模型f ( x ) f(x)f(x)中
得f 1 ( x ) = 0.4236 G 1 ( x ) f_1(x)=0.4236G_1(x)f 
1
​
 (x)=0.4236G 
1
​
 (x)

5.判断是否满足循环退出条件

分类器个数是否达到M
总分类器误差是否低于设定的精度

M=2时：
1.初始化/更新当前训练数据的权值分布

更新:

2.训练当前基分类器

使用具有权值分布D m D_mD 
m
​
 的训练数据集学习，得到基分类器G m ( x ) G_m(x)G 
m
​
 (x)
训练方法：1.在x xx中划分出各候选阈值，构建好候选G m ( x ) G_m(x)G 
m
​
 (x)；2.找到使误差e m e_me 
m
​
 最小的G m ( x ) G_m(x)G 
m
​
 (x)，作为本轮的基分类器。

3.计算当前基分类器的权值

1.计算当前G m ( x ) G_m(x)G 
m
​
 (x)在训练集上的分类误差率
e m = ∑ i = 1 N P ( G m ( x i ) ≠ y i ) = ∑ i = 1 N w m i I ( G m ( x i ) ≠ y i ) = ∑ G m ( x i ) ≠ y i w m i e_m=\sum_{i=1}^NP(G_m(x_i)\neq y_i)=\sum_{i=1}^N w_{mi} I(G_m(x_i)\neq y_i)=\sum_{G_m(x_i)\neq y_i}w_{mi}e 
m
​
 =∑ 
i=1
N
​
 P(G 
m
​
 (x 
i
​
 )

=y 
i
​
 )=∑ 
i=1
N
​
 w 
mi
​
 I(G 
m
​
 (x 
i
​
 )

=y 
i
​
 )=∑ 
G 
m
​
 (x 
i
​
 )

=y 
i
​
 
​
 w 
mi
​
 ，其中I II为指示函数，I = { 1 , G m ( x i ) ≠ y i 0 , G m ( x i ) = y i I=
{1,0,Gm(xi)≠yiGm(xi)=yi
{
1
,
�
�
(
�
�
)
≠
�
�
0
,
�
�
(
�
�
)
=
�
�
I={ 
1,
0,
​
  
G 
m
​
 (x 
i
​
 )

=y 
i
​
 
G 
m
​
 (x 
i
​
 )=y 
i
​
 
​
 


2.根据分类误差率e m e_me 
m
​
 ，计算基分类器G m ( x ) G_m(x)G 
m
​
 (x)的权重系数
α m = 1 2 log ⁡ 1 − e m e m \alpha_m=\frac{1}{2}\log\frac{1-e_m}{e_m}
α 
m
​
 = 
2
1
​
 log 
e 
m
​
 
1−e 
m
​
 
​
 

3.计算G 2 ( x ) G_2(x)G 
2
​
 (x)的系数：α 2 = 1 2 log ⁡ 1 − e 1 e 1 = 0.6496 \alpha_2=\frac{1}{2}\log \frac{1-e_1}{e_1}=0.6496α 
2
​
 = 
2
1
​
 log 
e 
1
​
 
1−e 
1
​
 
​
 =0.6496

4.将α m ⋅ G m ( x ) \alpha_m\cdot G_m(x)α 
m
​
 ⋅G 
m
​
 (x)更新到加法模型f ( x ) f(x)f(x)中
得
f 1 ( x ) = 0.4236 G 1 ( x ) + 0.6496 G 2 ( x ) f_1(x)=0.4236G_1(x)+0.6496G_2(x)
f 
1
​
 (x)=0.4236G 
1
​
 (x)+0.6496G 
2
​
 (x)

5.判断是否满足循环退出条件

分类器个数是否达到M
总分类器误差是否低于设定的精度

M=3时：
f 3 ( x ) = 0.4236 G 1 ( x ) + 0.6496 G 2 ( x ) + 0.7514 G 3 ( x ) f_3(x)=0.4236G_1(x)+0.6496G_2(x)+0.7514G_3(x)
f 
3
​
 (x)=0.4236G 
1
​
 (x)+0.6496G 
2
​
 (x)+0.7514G 
3
​
 (x)



2.4 加法模型
2.4.1 预测函数
f ( x ) = ∑ m = 1 M β m b ( x ; γ m ) f(x)=\sum_{m=1}^M\beta_mb(x;\gamma_m)
f(x)= 
m=1
∑
M
​
 β 
m
​
 b(x;γ 
m
​
 )

其中
类比Adaboost的预测函数：可知Adaboost就是一个加法模型：
f ( x ) = ∑ m = 1 M β m b ( x ; γ m ) f(x)=\sum_{m=1}^M\beta_mb(x;\gamma_m)
f(x)= 
m=1
∑
M
​
 β 
m
​
 b(x;γ 
m
​
 )

f ( x ) = ∑ m = 1 M α m G m ( x ) f(x)=\sum_{m=1}^M\alpha_m G_m(x)
f(x)= 
m=1
∑
M
​
 α 
m
​
 G 
m
​
 (x)

2.4.2 损失函数的设计
1.自定义损失函数L ( y , f ( x ) ) L(y,f(x))L(y,f(x))

2.回归问题
MSE均方误差

3.分类问题

指数函数
交叉熵损失
4.优化方法

为什么不用梯度？
梯度下降的缺点：整体损失极小化：
min ⁡ β m , γ m ∑ i = 1 N L ( y i , ∑ m = 1 M β m b ( x i ; γ m ) ) \min_{\beta_m,\gamma_m}\sum_{i=1}^NL(y_i,\sum_{m=1}^M\beta_mb(x_i;\gamma_m))
β 
m
​
 ,γ 
m
​
 
min
​
  
i=1
∑
N
​
 L(y 
i
​
 , 
m=1
∑
M
​
 β 
m
​
 b(x 
i
​
 ;γ 
m
​
 ))
缺点：复杂度高；举例：假设M = 2 M=2M=2，假设L为MSE，
∑ i = 1 N L ( y i , ∑ m = 1 M β m b ( x i ; γ m ) ) = ∑ i = 1 N ( y i − ( β 1 b ( x i ; γ 1 + β 2 b ( x i ; γ 2 ) ) \sum_{i=1}^NL(y_i,\sum_{m=1}^M\beta_mb(x_i;\gamma_m))=\sum_{i=1}^N\left(y_i-(\beta_1b(x_i;\gamma_1+\beta_2b(x_i;\gamma_2)\right)
i=1
∑
N
​
 L(y 
i
​
 , 
m=1
∑
M
​
 β 
m
​
 b(x 
i
​
 ;γ 
m
​
 ))= 
i=1
∑
N
​
 (y 
i
​
 −(β 
1
​
 b(x 
i
​
 ;γ 
1
​
 +β 
2
​
 b(x 
i
​
 ;γ 
2
​
 ))
如：假设你用梯度下降优化，单词迭代过程，就需要同时优化4个参数β 1 , β 2 , γ 1 , γ 2 \beta_1,\beta_2,\gamma_1,\gamma_2β 
1
​
 ,β 
2
​
 ,γ 
1
​
 ,γ 
2
​
 （而且γ \gammaγ是向量），如果M MM再大些，单词迭代需要同时更新2 ⋅ M 2\cdot M2⋅M个参数，复杂度高。

使用的方法：前向分步算法
具体步骤：
2.5 算法原理
2.5.1 需要优化的问题：二分类
1.二分类训练数据集
2.
T = { ( x 1 , y 1 ) , ⋯   , ( x N , y N ) } T=\{(x_1,y_1),\cdots,(x_N,y_N)\}
T={(x 
1
​
 ,y 
1
​
 ),⋯,(x 
N
​
 ,y 
N
​
 )}
，其中，每个样本点由实例与标记组成。实例x i ∈ X ⊆ R n x_i\in \mathcal{X}\subseteq R^nx 
i
​
 ∈X⊆R 
n
 ，标记y i ∈ Y = { − 1 , + 1 } y_i\in \mathcal{Y}=\{-1,+1\}y 
i
​
 ∈Y={−1,+1}，X \mathcal{X}X是实例空间，Y \mathcal{Y}Y是标记集合。

2.5.2 模型：加法模型
f ( x ) = ∑ m = 1 M α m G m ( x ) f(x)=\sum_{m=1}^M\alpha_mG_m(x)
f(x)= 
m=1
∑
M
​
 α 
m
​
 G 
m
​
 (x)

2.5.3 最终分类器
G ( x ) = s i g n [ f ( x ) ] G(x)=sign[f(x)]
G(x)=sign[f(x)]



2.5.4 损失函数：指数损失函数
1.二分类问题，使用指数损失函数
L ( y , f ( x ) ) = exp ⁡ [ − y f ( x ) ] L(y,f(x))=\exp[-yf(x)]
L(y,f(x))=exp[−yf(x)]

当f ( x ) f(x)f(x)分类正确时，与y yy同号，L ( y , f ( x ) ) < = 1 L(y,f(x))<=1L(y,f(x))<=1。
当f ( x ) f(x)f(x)分类错误时，与y yy异号，L ( y , f ( x ) ) > 1 L(y,f(x))>1L(y,f(x))>1。
2.将损失函数视为训练数据的权值
w ˉ m i = exp ⁡ [ − y i f m − 1 ( x i ) ] \bar w_{mi}=\exp[-y_if_{m-1}(x_i)]
w
ˉ
  
mi
​
 =exp[−y 
i
​
 f 
m−1
​
 (x 
i
​
 )]

非常符合训练样本中：提高前一轮“错误分类”的样本权值；降低前一轮“正确分类”的样本的权值。（就是因为e − y ⋅ f m − 1 ( x ) e^{-y\cdot f_{m-1}(x)}e 
−y⋅f 
m−1
​
 (x)
 ）

3.单个样本损失函数
L ( y , f ( x ) ) = exp ⁡ [ − y f m ( x ) ] = exp ⁡ [ − y ∑ m = 1 M α m G m ( x ) ] = exp ⁡ [ − y ( f m − 1 ( x ) + α m G m ( x ) ) ]
L(y,f(x))=exp[−yfm(x)]=exp[−y∑m=1MαmGm(x)]=exp[−y(fm−1(x)+αmGm(x))]
�
(
�
,
�
(
�
)
)
=
exp
⁡
[
−
�
�
�
(
�
)
]
=
exp
⁡
[
−
�
∑
�
=
1
�
�
�
�
�
(
�
)
]
=
exp
⁡
[
−
�
(
�
�
−
1
(
�
)
+
�
�
�
�
(
�
)
)
]
L(y,f(x))
​
  
=exp[−yf 
m
​
 (x)]
=exp[−y 
m=1
∑
M
​
 α 
m
​
 G 
m
​
 (x)]
=exp[−y(f 
m−1
​
 (x)+α 
m
​
 G 
m
​
 (x))]
​
 

4.总体损失函数（把所有样本放进去）
∑ i = 1 N exp ⁡ [ − y i ( f m − 1 ( x i ) + α m G m ( x i ) ) ] \sum_{i=1}^N \exp[-y_i(f_{m-1}(x_i)+\alpha_mG_m(x_i))]
i=1
∑
N
​
 exp[−y 
i
​
 (f 
m−1
​
 (x 
i
​
 )+α 
m
​
 G 
m
​
 (x 
i
​
 ))]

2.5.5 优化方法：前向分步算法
1.算法流程


2.第m轮

1.极小化损失函数：( α m , G m ( x ) ) = a r g min ⁡ α , G ∑ i = 1 N exp ⁡ [ − y i ( f m − 1 ( x i ) + α G m ( x i ) ) ] (\alpha_m,G_m(x))=arg \min_{\alpha,G}\sum_{i=1}^N \exp[-y_i(f_{m-1}(x_i)+\alpha G_m(x_i))](α 
m
​
 ,G 
m
​
 (x))=argmin 
α,G
​
 ∑ 
i=1
N
​
 exp[−y 
i
​
 (f 
m−1
​
 (x 
i
​
 )+αG 
m
​
 (x 
i
​
 ))]

2.式子变换：
( α m , G m ( x ) ) = a r g min ⁡ α , G ( e − α ⋅ ∑ y i = G ( x ) w ˉ m i + e α ⋅ ∑ y i ≠ G ( x ) w ˉ m i ) (\alpha_m,G_m(x))=arg \min_{\alpha,G}(e^{-\alpha}\cdot \sum_{y_i=G(x)}\bar w_{mi}+e^{\alpha}\cdot \sum_{y_i\neq G(x)}\bar w_{mi})(α 
m
​
 ,G 
m
​
 (x))=argmin 
α,G
​
 (e 
−α
 ⋅∑ 
y 
i
​
 =G(x)
​
  
w
ˉ
  
mi
​
 +e 
α
 ⋅∑ 
y 
i
​
 

=G(x)
​
  
w
ˉ
  
mi
​
 )，其中w ˉ m i = exp ⁡ [ − y i f m − 1 ( x i ) ] \bar w_{mi}=\exp[-y_if_{m-1}(x_i)] 
w
ˉ
  
mi
​
 =exp[−y 
i
​
 f 
m−1
​
 (x 
i
​
 )]
推导过程：
( α m , G m ( x ) ) = a r g min ⁡ α , G ∑ i = 1 N exp ⁡ [ − y i f m ( x i ) ] = a r g min ⁡ α , G ∑ i = 1 N exp ⁡ [ − y i ∑ m = 1 M α m G m ( x i ) ] = a r g min ⁡ α , G ∑ i = 1 N exp ⁡ [ − y i ( f m − 1 ( x i ) + α m G m ( x i ) ) ] = a r g min ⁡ α , G ∑ i = 1 N exp ⁡ [ − y i f m − 1 ( x i ) ] ⋅ exp ⁡ [ − y i α m G m ( x i ) ] = a r g min ⁡ α , G ∑ i = 1 N w ˉ m i ⋅ exp ⁡ [ α m G m ( x i ) ] , 其中 w ˉ m i = exp ⁡ [ − y i ( f m − 1 ( x i ) ] , 而 G ( x ) 有两种取值可能 { G ( x i ) = y i G ( x i ) ≠ y i = a r g min ⁡ α , G ( ∑ y i = G ( x i ) w ˉ m i ⋅ exp ⁡ [ − y i α m G m ( x i ) ] + ∑ y i ≠ G ( x i ) w ˉ m i ⋅ exp ⁡ [ − y i α m G m ( x i ) ] ) = a r g min ⁡ α , G ( ∑ y i = G ( x i ) w ˉ m i ⋅ exp ⁡ [ − α m ] + ∑ y i ≠ G ( x i ) w ˉ m i ⋅ exp ⁡ [ α m ] ) , 这一步是因为， y i ⋅ G m ( x i ) = − 1 o r 1 = a r g min ⁡ α , G ( e − α m ⋅ ∑ y i = G ( x i ) w ˉ m i + e α m ⋅ ∑ y i ≠ G ( x i ) w ˉ m i )
(αm,Gm(x))=argminα,G∑i=1Nexp[−yifm(xi)]=argminα,G∑i=1Nexp[−yi∑m=1MαmGm(xi)]=argminα,G∑i=1Nexp[−yi(fm−1(xi)+αmGm(xi))]=argminα,G∑i=1Nexp[−yifm−1(xi)]⋅exp[−yiαmGm(xi)]=argminα,G∑i=1Nw¯mi⋅exp[αmGm(xi)],其中w¯mi=exp[−yi(fm−1(xi)],而G(x)有两种取值可能{G(xi)=yiG(xi)≠yi=argminα,G⎛⎝∑yi=G(xi)w¯mi⋅exp[−yiαmGm(xi)]+∑yi≠G(xi)w¯mi⋅exp[−yiαmGm(xi)]⎞⎠=argminα,G⎛⎝∑yi=G(xi)w¯mi⋅exp[−αm]+∑yi≠G(xi)w¯mi⋅exp[αm]⎞⎠,这一步是因为，yi⋅Gm(xi)=−1or1=argminα,G⎛⎝e−αm⋅∑yi=G(xi)w¯mi+eαm⋅∑yi≠G(xi)w¯mi⎞⎠
(
�
�
,
�
�
(
�
)
)
=
�
�
�
min
�
,
�
∑
�
=
1
�
exp
⁡
[
−
�
�
�
�
(
�
�
)
]
=
�
�
�
min
�
,
�
∑
�
=
1
�
exp
⁡
[
−
�
�
∑
�
=
1
�
�
�
�
�
(
�
�
)
]
=
�
�
�
min
�
,
�
∑
�
=
1
�
exp
⁡
[
−
�
�
(
�
�
−
1
(
�
�
)
+
�
�
�
�
(
�
�
)
)
]
=
�
�
�
min
�
,
�
∑
�
=
1
�
exp
⁡
[
−
�
�
�
�
−
1
(
�
�
)
]
⋅
exp
⁡
[
−
�
�
�
�
�
�
(
�
�
)
]
=
�
�
�
min
�
,
�
∑
�
=
1
�
�
¯
�
�
⋅
exp
⁡
[
�
�
�
�
(
�
�
)
]
,
其
中
�
¯
�
�
=
exp
⁡
[
−
�
�
(
�
�
−
1
(
�
�
)
]
,
而
�
(
�
)
有
两
种
取
值
可
能
{
�
(
�
�
)
=
�
�
�
(
�
�
)
≠
�
�
=
�
�
�
min
�
,
�
(
∑
�
�
=
�
(
�
�
)
�
¯
�
�
⋅
exp
⁡
[
−
�
�
�
�
�
�
(
�
�
)
]
+
∑
�
�
≠
�
(
�
�
)
�
¯
�
�
⋅
exp
⁡
[
−
�
�
�
�
�
�
(
�
�
)
]
)
=
�
�
�
min
�
,
�
(
∑
�
�
=
�
(
�
�
)
�
¯
�
�
⋅
exp
⁡
[
−
�
�
]
+
∑
�
�
≠
�
(
�
�
)
�
¯
�
�
⋅
exp
⁡
[
�
�
]
)
,
这
一
步
是
因
为
，
�
�
⋅
�
�
(
�
�
)
=
−
1
�
�
1
=
�
�
�
min
�
,
�
(
�
−
�
�
⋅
∑
�
�
=
�
(
�
�
)
�
¯
�
�
+
�
�
�
⋅
∑
�
�
≠
�
(
�
�
)
�
¯
�
�
)
(α 
m
​
 ,G 
m
​
 (x))
​
  
=arg 
α,G
min
​
  
i=1
∑
N
​
 exp[−y 
i
​
 f 
m
​
 (x 
i
​
 )]
=arg 
α,G
min
​
  
i=1
∑
N
​
 exp[−y 
i
​
  
m=1
∑
M
​
 α 
m
​
 G 
m
​
 (x 
i
​
 )]
=arg 
α,G
min
​
  
i=1
∑
N
​
 exp[−y 
i
​
 (f 
m−1
​
 (x 
i
​
 )+α 
m
​
 G 
m
​
 (x 
i
​
 ))]
=arg 
α,G
min
​
  
i=1
∑
N
​
 exp[−y 
i
​
 f 
m−1
​
 (x 
i
​
 )]⋅exp[−y 
i
​
 α 
m
​
 G 
m
​
 (x 
i
​
 )]
=arg 
α,G
min
​
  
i=1
∑
N
​
  
w
ˉ
  
mi
​
 ⋅exp[α 
m
​
 G 
m
​
 (x 
i
​
 )],其中 
w
ˉ
  
mi
​
 =exp[−y 
i
​
 (f 
m−1
​
 (x 
i
​
 )],而G(x)有两种取值可能{ 
G(x 
i
​
 )=y 
i
​
 
G(x 
i
​
 )

=y 
i
​
 
​
 
=arg 
α,G
min
​
  
​
  
y 
i
​
 =G(x 
i
​
 )
∑
​
  
w
ˉ
  
mi
​
 ⋅exp[−y 
i
​
 α 
m
​
 G 
m
​
 (x 
i
​
 )]+ 
y 
i
​
 

=G(x 
i
​
 )
∑
​
  
w
ˉ
  
mi
​
 ⋅exp[−y 
i
​
 α 
m
​
 G 
m
​
 (x 
i
​
 )] 
​
 
=arg 
α,G
min
​
  
​
  
y 
i
​
 =G(x 
i
​
 )
∑
​
  
w
ˉ
  
mi
​
 ⋅exp[−α 
m
​
 ]+ 
y 
i
​
 

=G(x 
i
​
 )
∑
​
  
w
ˉ
  
mi
​
 ⋅exp[α 
m
​
 ] 
​
 ,这一步是因为，y 
i
​
 ⋅G 
m
​
 (x 
i
​
 )=−1or1
=arg 
α,G
min
​
  
​
 e 
−α 
m
​
 
 ⋅ 
y 
i
​
 =G(x 
i
​
 )
∑
​
  
w
ˉ
  
mi
​
 +e 
α 
m
​
 
 ⋅ 
y 
i
​
 

=G(x 
i
​
 )
∑
​
  
w
ˉ
  
mi
​
  
​
 
​
 

3.求解

1.优化G m ( x ) G_m(x)G 
m
​
 (x)

从G m ( x ) G_m(x)G 
m
​
 (x)本身的意义上来讲，最优的G m ( x ) G_m(x)G 
m
​
 (x)要使得误差最小
G m ∗ ( x ) = a r g min ⁡ G ∑ i = 1 N w ˉ m i I ( y i ≠ G ( x ) ) G_m^*(x)=arg\min_G \sum_{i=1}^N\bar w_{mi}I(y_i \neq G(x))G 
m
∗
​
 (x)=argmin 
G
​
 ∑ 
i=1
N
​
  
w
ˉ
  
mi
​
 I(y 
i
​
 

=G(x))
2.优化α m \alpha_mα 
m
​
 

α m = 1 2 log ⁡ 1 − e m e m \alpha_m=\frac{1}{2}\log\frac{1-e_m}{e_m}α 
m
​
 = 
2
1
​
 log 
e 
m
​
 
1−e 
m
​
 
​
 
推导过程:
式子变换：
a r g min ⁡ α m ( e − α m ⋅ ∑ y i = G ( x ) w ˉ m i + e α m ⋅ ∑ y i ≠ G ( x ) w ˉ m i ) → a r g min ⁡ α m ( e − α m ⋅ ∑ y i = G ( x ) w ˉ m i + e − α m ⋅ ∑ y i ≠ G ( x i ) w ˉ m i ⏟ − e − α m ⋅ ∑ y i ≠ G ( x i ) w ˉ m i + e α m ⋅ ∑ y i ≠ G ( x ) w ˉ m i ⏟ ) → a r g min ⁡ α m ( e − α m ⋅ ∑ i = 1 N w ˉ m i + ( e α m − e − α m ) ⋅ ∑ y i ≠ G ( x ) w ˉ m i )
argminαm⎛⎝e−αm⋅∑yi=G(x)w¯mi+eαm⋅∑yi≠G(x)w¯mi⎞⎠→argminαm⎛⎝⎜⎜⎜e−αm⋅∑yi=G(x)w¯mi+e−αm⋅∑yi≠G(xi)w¯mi−e−αm⋅∑yi≠G(xi)w¯mi+eαm⋅∑yi≠G(x)w¯mi⎞⎠⎟⎟⎟→argminαm⎛⎝e−αm⋅∑i=1Nw¯mi+(eαm−e−αm)⋅∑yi≠G(x)w¯mi⎞⎠
�
�
�
min
�
�
(
�
−
�
�
⋅
∑
�
�
=
�
(
�
)
�
¯
�
�
+
�
�
�
⋅
∑
�
�
≠
�
(
�
)
�
¯
�
�
)
→
�
�
�
min
�
�
(
�
−
�
�
⋅
∑
�
�
=
�
(
�
)
�
¯
�
�
+
�
−
�
�
⋅
∑
�
�
≠
�
(
�
�
)
�
¯
�
�
⏟
−
�
−
�
�
⋅
∑
�
�
≠
�
(
�
�
)
�
¯
�
�
+
�
�
�
⋅
∑
�
�
≠
�
(
�
)
�
¯
�
�
⏟
)
→
�
�
�
min
�
�
(
�
−
�
�
⋅
∑
�
=
1
�
�
¯
�
�
+
(
�
�
�
−
�
−
�
�
)
⋅
∑
�
�
≠
�
(
�
)
�
¯
�
�
)
​
  
arg 
α 
m
​
 
min
​
  
​
 e 
−α 
m
​
 
 ⋅ 
y 
i
​
 =G(x)
∑
​
  
w
ˉ
  
mi
​
 +e 
α 
m
​
 
 ⋅ 
y 
i
​
 

=G(x)
∑
​
  
w
ˉ
  
mi
​
  
​
 
→arg 
α 
m
​
 
min
​
  
​
  
e 
−α 
m
​
 
 ⋅ 
y 
i
​
 =G(x)
∑
​
  
w
ˉ
  
mi
​
 +e 
−α 
m
​
 
 ⋅ 
y 
i
​
 

=G(x 
i
​
 )
∑
​
  
w
ˉ
  
mi
​
 
​
 − 
e 
−α 
m
​
 
 ⋅ 
y 
i
​
 

=G(x 
i
​
 )
∑
​
  
w
ˉ
  
mi
​
 +e 
α 
m
​
 
 ⋅ 
y 
i
​
 

=G(x)
∑
​
  
w
ˉ
  
mi
​
 
​
  
​
 
→arg 
α 
m
​
 
min
​
  
​
 e 
−α 
m
​
 
 ⋅ 
i=1
∑
N
​
  
w
ˉ
  
mi
​
 +(e 
α 
m
​
 
 −e 
−α 
m
​
 
 )⋅ 
y 
i
​
 

=G(x)
∑
​
  
w
ˉ
  
mi
​
  
​
 
​
 

凸优化:
求导：
∂ ( e − α m ⋅ ∑ i = 1 N w ˉ m i + ( e α m − e − α m ) ⋅ ∑ y i ≠ G ( x ) w ˉ m i ) ∂ α m = − e − α m ⋅ ∑ i = 1 N w ˉ m i + ( e α m + e − α m ) ⋅ ∑ y i ≠ G ( x ) w ˉ m i = − e − α m ⋅ ( ∑ i = 1 N w ˉ m i − ∑ y i ≠ G ( x ) w ˉ m i ) + e α m ⋅ ∑ y i ≠ G ( x ) w ˉ m i = − e − α m ⋅ ∑ y i = G ( x ) w ˉ m i + e α m ⋅ ∑ y i ≠ G ( x ) w ˉ m i ≜ ψ
∂(e−αm⋅∑Ni=1w¯mi+(eαm−e−αm)⋅∑yi≠G(x)w¯mi)∂αm=−e−αm⋅∑i=1Nw¯mi+(eαm+e−αm)⋅∑yi≠G(x)w¯mi=−e−αm⋅⎛⎝∑i=1Nw¯mi−∑yi≠G(x)w¯mi⎞⎠+eαm⋅∑yi≠G(x)w¯mi=−e−αm⋅∑yi=G(x)w¯mi+eαm⋅∑yi≠G(x)w¯mi≜ψ
∂
(
�
−
�
�
⋅
∑
�
=
1
�
�
¯
�
�
+
(
�
�
�
−
�
−
�
�
)
⋅
∑
�
�
≠
�
(
�
)
�
¯
�
�
)
∂
�
�
=
−
�
−
�
�
⋅
∑
�
=
1
�
�
¯
�
�
+
(
�
�
�
+
�
−
�
�
)
⋅
∑
�
�
≠
�
(
�
)
�
¯
�
�
=
−
�
−
�
�
⋅
(
∑
�
=
1
�
�
¯
�
�
−
∑
�
�
≠
�
(
�
)
�
¯
�
�
)
+
�
�
�
⋅
∑
�
�
≠
�
(
�
)
�
¯
�
�
=
−
�
−
�
�
⋅
∑
�
�
=
�
(
�
)
�
¯
�
�
+
�
�
�
⋅
∑
�
�
≠
�
(
�
)
�
¯
�
�
≜
�
​
  
∂α 
m
​
 
∂(e 
−α 
m
​
 
 ⋅∑ 
i=1
N
​
  
w
ˉ
  
mi
​
 +(e 
α 
m
​
 
 −e 
−α 
m
​
 
 )⋅∑ 
y 
i
​
 

=G(x)
​
  
w
ˉ
  
mi
​
 )
​
 
=−e 
−α 
m
​
 
 ⋅ 
i=1
∑
N
​
  
w
ˉ
  
mi
​
 +(e 
α 
m
​
 
 +e 
−α 
m
​
 
 )⋅ 
y 
i
​
 

=G(x)
∑
​
  
w
ˉ
  
mi
​
 
=−e 
−α 
m
​
 
 ⋅ 
​
  
i=1
∑
N
​
  
w
ˉ
  
mi
​
 − 
y 
i
​
 

=G(x)
∑
​
  
w
ˉ
  
mi
​
  
​
 +e 
α 
m
​
 
 ⋅ 
y 
i
​
 

=G(x)
∑
​
  
w
ˉ
  
mi
​
 
=−e 
−α 
m
​
 
 ⋅ 
y 
i
​
 =G(x)
∑
​
  
w
ˉ
  
mi
​
 +e 
α 
m
​
 
 ⋅ 
y 
i
​
 

=G(x)
∑
​
  
w
ˉ
  
mi
​
 ≜ψ
​
 

导数为0：令ψ = 0 \psi=0ψ=0，即：
e α m ⋅ ∑ y i ≠ G ( x ) w ˉ m i = e − α m ⋅ ∑ y i = G ( x ) w ˉ m i , 两边取对数 → α m + ln ⁡ ( ∑ y i ≠ G ( x ) w ˉ m i ) = − α m ln ⁡ ( ∑ y i = G ( x ) w ˉ m i ) → 2 α m = ln ⁡ ( ∑ y i = G ( x ) w ˉ m i ∑ y i ≠ G ( x ) w ˉ m i ) → α m = 1 2 ln ⁡ ( ∑ i = 1 N w ˉ m i − ∑ y i ≠ G ( x ) w ˉ m i ∑ y i ≠ G ( x ) w ˉ m i )
eαm⋅∑yi≠G(x)w¯mi=e−αm⋅∑yi=G(x)w¯mi,两边取对数→αm+ln(∑yi≠G(x)w¯mi)=−αmln(∑yi=G(x)w¯mi)→2αm=ln(∑yi=G(x)w¯mi∑yi≠G(x)w¯mi)→αm=12ln(∑Ni=1w¯mi−∑yi≠G(x)w¯mi∑yi≠G(x)w¯mi)
�
�
�
⋅
∑
�
�
≠
�
(
�
)
�
¯
�
�
=
�
−
�
�
⋅
∑
�
�
=
�
(
�
)
�
¯
�
�
,
两
边
取
对
数
→
�
�
+
ln
⁡
(
∑
�
�
≠
�
(
�
)
�
¯
�
�
)
=
−
�
�
ln
⁡
(
∑
�
�
=
�
(
�
)
�
¯
�
�
)
→
2
�
�
=
ln
⁡
(
∑
�
�
=
�
(
�
)
�
¯
�
�
∑
�
�
≠
�
(
�
)
�
¯
�
�
)
→
�
�
=
1
2
ln
⁡
(
∑
�
=
1
�
�
¯
�
�
−
∑
�
�
≠
�
(
�
)
�
¯
�
�
∑
�
�
≠
�
(
�
)
�
¯
�
�
)
​
  
e 
α 
m
​
 
 ⋅ 
y 
i
​
 

=G(x)
∑
​
  
w
ˉ
  
mi
​
 =e 
−α 
m
​
 
 ⋅ 
y 
i
​
 =G(x)
∑
​
  
w
ˉ
  
mi
​
 ,两边取对数
→α 
m
​
 +ln( 
y 
i
​
 

=G(x)
∑
​
  
w
ˉ
  
mi
​
 )=−α 
m
​
 ln( 
y 
i
​
 =G(x)
∑
​
  
w
ˉ
  
mi
​
 )
→2α 
m
​
 =ln( 
∑ 
y 
i
​
 

=G(x)
​
  
w
ˉ
  
mi
​
 
∑ 
y 
i
​
 =G(x)
​
  
w
ˉ
  
mi
​
 
​
 )
→α 
m
​
 = 
2
1
​
 ln( 
∑ 
y 
i
​
 

=G(x)
​
  
w
ˉ
  
mi
​
 
∑ 
i=1
N
​
  
w
ˉ
  
mi
​
 −∑ 
y 
i
​
 

=G(x)
​
  
w
ˉ
  
mi
​
 
​
 )
​
 

公式转换：
α m = 1 2 ln ⁡ ( ∑ i = 1 N w ˉ m i − ∑ y i ≠ G ( x ) w ˉ m i ∑ y i ≠ G ( x ) w ˉ m i ) = 1 2 ln ⁡ ( ∑ i = 1 N w ˉ m i ∑ i = 1 N w ˉ m i − ∑ y i ≠ G ( x ) w ˉ m i ∑ i = 1 N w ˉ m i ∑ y i ≠ G ( x ) w ˉ m i ∑ i = 1 N w ˉ m i ) = 1 2 ln ⁡ 1 − e m e m
αm=12ln(∑Ni=1w¯mi−∑yi≠G(x)w¯mi∑yi≠G(x)w¯mi)=12ln⎛⎝⎜⎜∑Ni=1w¯mi∑Ni=1w¯mi−∑yi≠G(x)w¯mi∑Ni=1w¯mi∑yi≠G(x)w¯mi∑Ni=1w¯mi⎞⎠⎟⎟=12ln1−emem
�
�
=
1
2
ln
⁡
(
∑
�
=
1
�
�
¯
�
�
−
∑
�
�
≠
�
(
�
)
�
¯
�
�
∑
�
�
≠
�
(
�
)
�
¯
�
�
)
=
1
2
ln
⁡
(
∑
�
=
1
�
�
¯
�
�
∑
�
=
1
�
�
¯
�
�
−
∑
�
�
≠
�
(
�
)
�
¯
�
�
∑
�
=
1
�
�
¯
�
�
∑
�
�
≠
�
(
�
)
�
¯
�
�
∑
�
=
1
�
�
¯
�
�
)
=
1
2
ln
⁡
1
−
�
�
�
�
α 
m
​
 
​
  
= 
2
1
​
 ln( 
∑ 
y 
i
​
 

=G(x)
​
  
w
ˉ
  
mi
​
 
∑ 
i=1
N
​
  
w
ˉ
  
mi
​
 −∑ 
y 
i
​
 

=G(x)
​
  
w
ˉ
  
mi
​
 
​
 )
= 
2
1
​
 ln 
​
  
∑ 
i=1
N
​
  
w
ˉ
  
mi
​
 
∑ 
y 
i
​
 

=G(x)
​
  
w
ˉ
  
mi
​
 
​
 
∑ 
i=1
N
​
  
w
ˉ
  
mi
​
 
∑ 
i=1
N
​
  
w
ˉ
  
mi
​
 
​
 − 
∑ 
i=1
N
​
  
w
ˉ
  
mi
​
 
∑ 
y 
i
​
 

=G(x)
​
  
w
ˉ
  
mi
​
 
​
 
​
  
​
 
= 
2
1
​
 ln 
e 
m
​
 
1−e 
m
​
 
​
 
​
 

其中w ˉ \bar w 
w
ˉ
 是损失并不是[ 0 − 1 ] [0-1][0−1]限定空间里面的，而之前算法流程中的w是加了规范化因子进行归一化操作，保证了系数和为1 11。

3.前向更新f m ( x ) f_m(x)f 
m
​
 (x)：
f m ( x ) = f m − 1 + α m G m ( x ) f_m(x)=f_{m-1}+\alpha_mG_m(x)
f 
m
​
 (x)=f 
m−1
​
 +α 
m
​
 G 
m
​
 (x)

4.更新训练数据权值w ˉ m + 1 , i \bar w_{m+1,i} 
w
ˉ
  
m+1,i
​
 ：
w ˉ m i { 1 N , m = 1 w ˉ m − 1 , i ⋅ exp ⁡ [ − y i ⋅ α m − 1 G m − 1 ( x i ) ] , m > 1 \bar w_{mi}
{1Nw¯m−1,i⋅exp[−yi⋅αm−1Gm−1(xi)],m=1,m>1
{
1
�
,
�
=
1
�
¯
�
−
1
,
�
⋅
exp
⁡
[
−
�
�
⋅
�
�
−
1
�
�
−
1
(
�
�
)
]
,
�
>
1
w
ˉ
  
mi
​
 { 
N
1
​
 
w
ˉ
  
m−1,i
​
 ⋅exp[−y 
i
​
 ⋅α 
m−1
​
 G 
m−1
​
 (x 
i
​
 )]
​
  
,m=1
,m>1
​
 

推导：
更新w ˉ m + 1 , i \bar w_{m+1,i} 
w
ˉ
  
m+1,i
​
 :
根据公式：w ˉ m i = exp ⁡ [ − y i ⋅ f m − 1 ( x i ) ] \bar w_{mi}=\exp[-y_i\cdot f_{m-1}(x_i)] 
w
ˉ
  
mi
​
 =exp[−y 
i
​
 ⋅f 
m−1
​
 (x 
i
​
 )]
有：
exp ⁡ [ − y i ⋅ f m ( x i ) ] = exp ⁡ [ − y i ⋅ ( f m − 1 ( x i ) + α m G m ( x i ) ) ] = exp ⁡ [ − y i ⋅ f m − 1 ( x i ) ] ⋅ exp ⁡ [ − y i ⋅ α m G m ( x i ) ] = w ˉ m i ⋅ exp ⁡ [ − y i ⋅ α m G m ( x i ) ]
exp[−yi⋅fm(xi)]=exp[−yi⋅(fm−1(xi)+αmGm(xi))]=exp[−yi⋅fm−1(xi)]⋅exp[−yi⋅αmGm(xi)]=w¯mi⋅exp[−yi⋅αmGm(xi)]
exp
⁡
[
−
�
�
⋅
�
�
(
�
�
)
]
=
exp
⁡
[
−
�
�
⋅
(
�
�
−
1
(
�
�
)
+
�
�
�
�
(
�
�
)
)
]
=
exp
⁡
[
−
�
�
⋅
�
�
−
1
(
�
�
)
]
⋅
exp
⁡
[
−
�
�
⋅
�
�
�
�
(
�
�
)
]
=
�
¯
�
�
⋅
exp
⁡
[
−
�
�
⋅
�
�
�
�
(
�
�
)
]
exp[−y 
i
​
 ⋅f 
m
​
 (x 
i
​
 )]
​
  
=exp[−y 
i
​
 ⋅(f 
m−1
​
 (x 
i
​
 )+α 
m
​
 G 
m
​
 (x 
i
​
 ))]
=exp[−y 
i
​
 ⋅f 
m−1
​
 (x 
i
​
 )]⋅exp[−y 
i
​
 ⋅α 
m
​
 G 
m
​
 (x 
i
​
 )]
= 
w
ˉ
  
mi
​
 ⋅exp[−y 
i
​
 ⋅α 
m
​
 G 
m
​
 (x 
i
​
 )]
​
 
3.四个问题
3.1 如何计算学习误差率e m e_me 
m
​
 ?
根据分类误差率e m e_me 
m
​
 ，计算基分类器G m ( x ) G_m(x)G 
m
​
 (x)的权重系数：
α m = 1 2 log ⁡ 1 − e m e m \alpha_m=\frac{1}{2}\log\frac{1-e_m}{e_m}
α 
m
​
 = 
2
1
​
 log 
e 
m
​
 
1−e 
m
​
 
​
 

为什么这样计算基学习器权重系数？

0 ≤ e m ≤ 1 2 → 1 − e m e m ≥ 1 → α m ≥ 0 0\leq e_m \leq \frac{1}{2} \rightarrow \frac{1-e_m}{e_m}\geq 1 \rightarrow \alpha_m \geq 00≤e 
m
​
 ≤ 
2
1
​
 → 
e 
m
​
 
1−e 
m
​
 
​
 ≥1→α 
m
​
 ≥0，e m e_me 
m
​
  下降时，1 − e m e m \frac{1-e_m}{e_m} 
e 
m
​
 
1−e 
m
​
 
​
 单调增，α m \alpha_mα 
m
​
 单调增。

推导过程：
看2.5 算法原理

3.2 如何得到基学习器权重系数α \alphaα？
3.3 如何更新样本权重w ww?
w ˉ m i { 1 N , m = 1 w ˉ m − 1 , i ⋅ exp ⁡ [ − y i ⋅ α m − 1 G m − 1 ( x i ) ] , m > 1 \bar w_{mi}
{1Nw¯m−1,i⋅exp[−yi⋅αm−1Gm−1(xi)],m=1,m>1
{
1
�
,
�
=
1
�
¯
�
−
1
,
�
⋅
exp
⁡
[
−
�
�
⋅
�
�
−
1
�
�
−
1
(
�
�
)
]
,
�
>
1
w
ˉ
  
mi
​
 { 
N
1
​
 
w
ˉ
  
m−1,i
​
 ⋅exp[−y 
i
​
 ⋅α 
m−1
​
 G 
m−1
​
 (x 
i
​
 )]
​
  
,m=1
,m>1
​
 

更新w ˉ m + 1 , i \bar w_{m+1,i} 
w
ˉ
  
m+1,i
​
 :

根据公式：w ˉ m i = exp ⁡ [ − y i ⋅ f m − 1 ( x i ) ] \bar w_{mi}=\exp[-y_i\cdot f_{m-1}(x_i)] 
w
ˉ
  
mi
​
 =exp[−y 
i
​
 ⋅f 
m−1
​
 (x 
i
​
 )]
有：
exp ⁡ [ − y i ⋅ f m ( x i ) ] = exp ⁡ [ − y i ⋅ ( f m − 1 ( x i ) + α m G m ( x i ) ) ] = exp ⁡ [ − y i ⋅ f m − 1 ( x i ) ] ⋅ exp ⁡ [ − y i ⋅ α m G m ( x i ) ] = w ˉ m i ⋅ exp ⁡ [ − y i ⋅ α m G m ( x i ) ]
exp[−yi⋅fm(xi)]=exp[−yi⋅(fm−1(xi)+αmGm(xi))]=exp[−yi⋅fm−1(xi)]⋅exp[−yi⋅αmGm(xi)]=w¯mi⋅exp[−yi⋅αmGm(xi)]
exp
⁡
[
−
�
�
⋅
�
�
(
�
�
)
]
=
exp
⁡
[
−
�
�
⋅
(
�
�
−
1
(
�
�
)
+
�
�
�
�
(
�
�
)
)
]
=
exp
⁡
[
−
�
�
⋅
�
�
−
1
(
�
�
)
]
⋅
exp
⁡
[
−
�
�
⋅
�
�
�
�
(
�
�
)
]
=
�
¯
�
�
⋅
exp
⁡
[
−
�
�
⋅
�
�
�
�
(
�
�
)
]
exp[−y 
i
​
 ⋅f 
m
​
 (x 
i
​
 )]
​
  
=exp[−y 
i
​
 ⋅(f 
m−1
​
 (x 
i
​
 )+α 
m
​
 G 
m
​
 (x 
i
​
 ))]
=exp[−y 
i
​
 ⋅f 
m−1
​
 (x 
i
​
 )]⋅exp[−y 
i
​
 ⋅α 
m
​
 G 
m
​
 (x 
i
​
 )]
= 
w
ˉ
  
mi
​
 ⋅exp[−y 
i
​
 ⋅α 
m
​
 G 
m
​
 (x 
i
​
 )]
​
 

这里w ˉ \bar w 
w
ˉ
  是除以过规范化因子Z ZZ的，从公式w ˉ m i \bar w_{mi} 
w
ˉ
  
mi
​
 可以看出，如果有第i ii个样本分类错误，则y i ⋅ G m − 1 ( x i ) < 0 y_i\cdot G_{m-1}(x_i)<0y 
i
​
 ⋅G 
m−1
​
 (x 
i
​
 )<0，导致样本的权重在第m mm个基分类器中增大，如果分类正确，则权重在第m mm个基分类器中减少。

3.4 使用何种结合策略？
f ( x ) = s i g n ( ∑ m = 1 M α m G m ( x ) ) f(x)=sign(\sum_{m=1}^M\alpha_mG_m(x))f(x)=sign(∑ 
m=1
M
​
 α 
m
​
 G 
m
​
 (x))

4.小结
Adaboost的主要优点有：

1.Adaboost作为分类器时，分类精度很高
2.在Adaboost的框架下，可以使用各种回归分类模型来构建弱学习器，非常灵活。
3.作为简单的二元分类器时，构造简单，结果可理解。
4.不容易发生过拟合
Adaboost的主要缺点有：

1.对异常样本敏感，异常样本在迭代中可能会获得较高的权重，影响最终的强学习器的预测准确性。
————————————————
版权声明：本文为CSDN博主「joejoeqian」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/joejoeqian/article/details/129764067