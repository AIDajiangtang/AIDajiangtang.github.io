---
published: false
layout: post
title: "预告"
categories: 我的AI新书
date: 2023-09-21 00:00:00 +0800
excerpt: "预告"
---


VGG



Alexnet虽然证明了深层神经网络是有效果的，但是它最大的问题是模型不规则，结构不是很清晰，没有提供一个通用的模板来指导后续的研究人员设计新的网络。如果模型想要变得更大、更深，则需要很好的设计思想，使得整个框架更加规则








如何使模型更深更大：



更多的全连接层（缺点是全连接层很大的话会占用很多的内存）
更多的卷积层（AlexNet是先将LeNet的模型给扩大之后，再加了三个卷积层，不好实现对模型进一步的加大、加深；VGG的思想是先将卷积层组成小块，然后再将卷积层进行堆叠）
将卷积层组合成块（VGG提出了VGG块的概念，其实就是AlexNet思路的拓展：AlexNet中是三个一模一样的卷积层（3*3，384通道，padding等于1）加上一个池化层（3*3，最大池化层，stride=2）组成了一个小块：VGG块是在此基础上的拓展，它并不限制块中卷积层的层数和通道数），最大池化层重新用回了LeNet中的最大池化层窗口（2*2，最大池化层，stride=2）



VGG的核心思想是使用大量由一定数目的3*3的卷积层和一个最大池化层组成的VGG块进行堆叠，最终得到最后的网络

为什么使用的卷积层是3*3，而不是5*5？5*5的卷积层也用过，但是5*5的卷积层的计算量更大，所以层数就不会太大，VGG块就会变得浅一点，最终通过对比发现，在同样的计算开销之下，大量的3*3的卷积层堆叠起来比少量的5*5的卷积层堆叠起来的效果更好，也就是说模型更深、卷积窗口更小的情况下，效果会更好一点
VGG块由两部分组成：多个填充为1的3*3卷积层（它有两个超参数：层数n、通道数m）和一个步幅为2的2*2最大池化层








经典卷积神经网络的基本组成部分：



带填充的卷积层（用填充来保持分辨率）
非线性激活函数（如ReLu）
汇聚层（如最大汇聚层）








VGG基本组成部分：



带有3*3的卷积核、填充为1（为了保持宽度和高度）的卷积层
带有2*2汇聚窗口、步幅为2（每个块后的分辨率减半）的最大汇聚层








VGG架构



其实就是使用多个VGG块进行堆叠来替换掉AlexNet中的卷积部分

VGG块重复的次数不同可以得到不同的架构，比如VGG-16、VGG-19，···
最后还是使用了两个4096的全连接层得到输出
VGG对AlexNet最大的改进是：将AlexNet在LeNet的基础上新加的卷积层抽象出了VGG块，替换掉了AlexNet中原先并不规则的部分
类似于AlexNet、LeNet，VGG网络也可以分成两部分：第一部分主要由卷积层和汇聚层组成，第二部分由全连接层组成。从AlexNet到VGG，本质上都是块设计

原始的VGG网络有5个块，前2个块各有一个卷积层，后3个块个包含两个卷积层；第一个模块有64个输出通道，每个后续模块将输出通道的数量翻倍，直到达到512，由于该网络使用了8个卷积层和三个全连接层，因此通常被称为VGG-11（这里为什么是5块？因为原始输入图像的大小是224，每经过一个VGG块，输出的通道数会翻倍、高宽会减半，当减到第五次时输出的高宽为7，就不能再经过VGG块进行减半了）








发展



LeNet（1995）

由2个卷积层+池化层和2个全连接层组成
AlexNet（2012）

比LeNet更大更深
加入了ReLu、Dropout、数据增强
VGG

实际上就是一个更大更深的AlexNet








GluonCV Model Zoo


X轴表示不同的模型每秒钟所做的推断（Inference）的个数，越往右表示越快
Y轴表示模型在ImageNet上的准确率（Accuracy），越往上表示准确率越高
AlexNet很快，但是精度并不是很高
VGG相比于AlexNet来说提升较大，但是代价就是牺牲速度来换取模型深度的增加，速度大概是AlexNet的1/6到1/5左右
图中圆圈的大小表示内存的使用，圆圈越大表示所占用的内存就越多，可以看到，VGG所占用的内存较大
因为VGG中可以选择不同的VGG块的个数，所以就产生了一系列的VGG模型，模型越小精度越低但速度越快，模型越大精度越高但速度越慢
VGG模型虽然相比于AlexNet来说速度较慢，但是随着硬件的提升，二者的在速度上的差距会越来越小








总结



VGG使用可重复使用的卷积块来构建深度卷积神经网络（将ALexNet中不规则的部分抽象出来做成了VGG块，它是一种可复用的卷积块）
通过配置不同的卷积块个数和通道数可以得到不同复杂度的变种（不同的VGG模型可以通过每个块中卷积层数量和输出通道数量的差异来定义）
这个思想在之后被大量使用：1、使用可重复的块来构建深度神经网络；2、网络产生不同的配置
块的使用导致网络定义的非常简洁。使用块可以有效地设计复杂的网络。
深层且窄的卷积（3×3）比浅层且宽的卷积更有效








Q&A



在视觉领域，人工特征的研究还有没有进展？研究如何设计更好的特征是不是也还有意义？尤其是提升研究能力方面？﻿
QA P3 - 00:07
﻿


我们要学习特征值、特征向量和奇异值分解的知识吗？﻿
QA P3 - 01:30
﻿


Colab单个会话的最长连续运行时间为12h，另外在运行中会输入验证码，有没有更好的方法？﻿
QA P3 - 01:58
﻿


训练loss一直下降，测试loss从开始起就一直不降，成水平状，是什么原因呢？﻿
QA P3 - 02:14
﻿


为什么VGG（1，1，224，224）的输入高宽减半后，通道数是64？﻿
QA P3 - 02:52
﻿








----end----

其它参考：

1、《动手学深度学习》，https://zh-v2.d2l.ai/chapter_convolutional-modern/vgg.html 作者：如果我是泡橘子 https://www.bilibili.com/read/cv15300443/?from=readlist&jump_opus=1 出处：bilibili