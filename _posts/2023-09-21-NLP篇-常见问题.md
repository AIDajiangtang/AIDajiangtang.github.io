---
published: false
layout: post
title: "字节对编码（Byte-Pair Encoding，BPE）：基于子词的分词算法"
categories: 我的AI新书
date: 2023-09-21 00:00:00 +0800
excerpt: "字节对编码（Byte-Pair Encoding，BPE）：基于子词的分词算法"
---



**既然已经有马尔可夫模型和自回归网络处理时时序数据，为什么还需要RNN，Transformer呢？马尔可夫模型有什么缺点？后者又有什么优势？**

尽管马尔可夫模型和自回归网络都可以处理时间序列数据,但它们有以下缺点:

马尔可夫模型基于概率图模型,需要人工构建状态图,不够灵活。参数估计比较困难,不适合大规模时间序列。
传统自回归模型如ARIMA等都是线性模型,表达能力有限,无法捕捉时间序列中的非线性模式。
马尔可夫模型和自回归模型难以处理长期依赖问题,无法记住长期上下文。
相比之下,RNN和Transformer有以下优势:

RNN通过循环结构捕捉时间序列的连续性,并通过隐状态传递长期信息。
RNN和Transformer都运用了神经网络,具有强大的非线性表达能力,可自适应地学习时间序列的复杂模式。
Transformer引入了自注意力机制,更好地建模了时间序列的全局上下文和长距离依赖。
RNN和Transformer都可以端到端训练,不需要人为构建图模型。
计算效率更高,可扩展到大规模时间序列场景。
总之,RNN和Transformer克服了马尔可夫和自回归模型的局限,可获得更强大和灵活的时序建模能力,因此被广泛使用。但在某些场景下,马尔可夫和自回归模型由于解析性和简单性,也具有其价值。

**使用隐状态的好处是什么？**
使用隐状态的主要好处有:

强大的建模能力。隐状态可以表示观测数据中无法直接观测到的潜在属性或模式,使用隐状态进行建模可以更好地拟合复杂的数据。
提供了一种抽象的表示方式。隐状态通过抽象表示将原始观测数据进行压缩,提取关键信息,降低数据的维度和复杂性。
处理不可观测和隐私数据。在许多应用中,部分重要信息出于隐私考虑无法观测,引入隐状态可以用于这类数据的建模。
引入随机性。隐状态增加了模型的随机性,使其对噪声更具鲁棒性,避免过拟合。
处理缺失和不完整的数据。隐状态可以填补数据中的空白,应对观测数据的不完整性。
实现生成过程。隐状态可以表示数据的潜在生成过程,用于解空间较大的生成任务。
传递重要上下文信息。在序列数据中,隐状态可以传递历史信息,应对长期依赖问题。
实现集成建模。隐状态可以将不同模型集成起来,进行联合建模。
总之,引入隐状态为我们在数据挖掘提供了更大的想象空间,使得复杂问题的建模变得可能。是许多强大模型的关键组成部分。



**当下常见的自然语言预训练模型有哪些？**

1.Word embedding

2.基于Transformer的预训练模型
- Bert：transformer编码器，基于掩码进行上下文理解，善于文本理解，即完型填空
- GPT：transformer解码器，自回归预测下一个词，善于文本生成


**transformer中会计算注意力，那么在自然语言中，注意力到底指的是什么？**
在自然语言处理中,Transformer模型通过注意力机制来聚焦于输入序列的不同部分。这里的“注意力”可以从以下几个方面理解:

对词的关注程度:注意力分数表示了模型对某个词的关注或重视程度。分数越高,表示该词在理解当前语义上下文时越重要。
词之间的相关性:注意力分数反映了词与词之间在意义上的相关性。分数越高,表示这两个词语义更加相关。
信息提取:注意力机制通过加权聚合,抽取出输入序列中的关键信息并进行聚焦。
全局依赖:自注意力通过查询权重值建模全局依赖,而非仅仅局部连续的短距离依赖。
内部思维:某种意义上,注意力分布反映了模型内部的思维状态,表示它更加关注输入的哪些部分。
可解释性:通过可视化注意力分布,我们可以解释模型的内部行为,说明它“注意”到了哪些内容。
简而言之,在自然语言处理任务中,注意力是模型关注语义信息的一种抽象表示,既反映了输入的语法结构,也反映了模型的内部理解状态。它带来了非局部依赖建模能力,也提高了模型的可解释性。


**seq2seq**
是一种编码器与解码器架构
编码器是没有输出的的RNN
编码器最后时间步的隐状态
作为解码器的初始隐状态

**如何衡量seq2seq生成序列的好坏？**
BLEU 0-1，越接近1越好

BLEU(Bilingual Evaluation Understudy) 是一种常用的机器翻译评价算法,用于评估机器翻译质量。

BLEU的主要思想是:将机器翻译与人工参考翻译进行比较,计算机器翻译与参考翻译的n-gram(一般为1-4 gram)重合程度,重合程度越高,BLEU分数越高,说明翻译质量越好。

BLEU的计算方法包括:

计算机器翻译中的n-gram在参考翻译中出现的次数。
计算参考翻译中的n-gram总数。
取上述比例的对数平均值作为BLEU分数,并带入长度惩罚因子。
一般会采用多个参考翻译,取BLEU分数的算术平均值。
BLEU的优点是计算简单快速, corneal与人类评价高,可直接用于自动评测系统。缺点是忽略了语法和语义。

BLEU通常score在0到1之间,越接近1表示翻译质量越好。BLEU是机器翻译评价中使用最广泛的自动指标之一。

**注意力与自注意力之间的区别？**
注意力(Attention)机制和自注意力(Self-Attention)机制的主要区别有:

注意力是从编码器的输入中生成注意力向量,来关注解码器中不同位置的信息。自注意力完全在编码器内部,关注编码器输入的不同位置。
注意力需要编码器和解码器两个结构,自注意力只在编码器内部计算注意力。
注意力的查询 Query 来自解码器当前输出,键 Key 和值 Value 来自编码器输出。自注意力的查询、键、值全部来自同一条输入序列。
注意力是序列到序列结构,输出是解码的每个位置对输入序列的关注分布。自注意力是序列内部的关联,输出是序列中每个位置对其他位置的关注权重。
注意力机制通常用于机器翻译等seq2seq模型。自注意力被更广泛地应用于 transformer、BERT等模型中。
注意力计算简单,自注意力增加了可学习的投影矩阵以学习查询、键、值的变换表示。
注意力促进编码信息的选择利用,自注意力通过内部关系学习输入序列的内在表示。
综上,自注意力可以看作是注意力机制的推广和扩展,但更侧重学习输入本身的内在关联,而不是输入和输出之间的对应关系。它也可以单独用在编码器中。

注意力和自注意力是两种常用的深度学习模型中的技术，它们都可以帮助模型关注输入或输出中的重要部分，从而提高模型的性能和效率。它们的区别主要在于它们关注的对象不同：

注意力机制是指模型在处理一个输入序列（例如一句话或一张图片）和一个输出序列（例如另一种语言的翻译或一个标签）时，可以根据输出序列中的每个元素（例如一个单词或一个像素），计算出输入序列中与之最相关的部分（例如另一个单词或一个区域），并给予更高的权重，从而实现输入和输出之间的对齐和关联。注意力机制可以帮助模型捕捉输入和输出之间的长距离依赖关系，提高模型的表达能力和泛化能力。注意力机制的一个典型应用是机器翻译，其中模型可以根据目标语言中的每个单词，找出源语言中与之最匹配的单词或短语，从而生成更准确和流畅的翻译。12

自注意力机制是指模型在处理一个输入序列或一个输出序列时，可以根据序列中的每个元素，计算出序列中其他元素与之的相似度或相关度，并给予更高的权重，从而实现序列内部的自对齐和自关联。自注意力机制可以帮助模型捕捉序列内部的结构和语义信息，提高模型的理解能力和生成能力。自注意力机制的一个典型应用是Transformer，其中模型可以根据输入或输出序列中的每个单词，找出与之最相关的其他单词，从而构建出一个全局的上下文表示。3

总之，注意力机制和自注意力机制都是一种基于权重分配的方法，它们都可以让模型关注重要的信息，忽略不相关的信息。它们的区别在于注意力机制是在不同的序列之间进行权重分配，而自注意力机制是在同一个序列内部进行权重分配。


**自注意力中的位置编码有什么用？加或者不加位置编码会有什么差别？**
自注意力机制中位置编码的主要作用是引入序列顺序信息。

自注意力计算的是文本序列中单词之间的相关性,本身不包含位置和顺序信息。
为了让模型感知序列的顺序,需要在输入嵌入中加入一定的位置编码。
一般使用不同的正弦和余弦函数为每个位置编码,以此引入顺序信息。
不加位置编码,自注意力可以学习词语之间的关联,但无法感知词序,会对许多任务产生影响。
加入位置编码后,才可以同时利用序列顺序信息和内容信息进行建模。
一些基于自注意力的模型如Transformer就是通过位置编码来引入顺序信息的。
在预测任务中,有位置编码会直接影响输出的顺序和位置判断。
总之,自注意力本身不含位置信息,因此需要在输入中加入位置编码,以便模型识别序列的顺序与位置,这对许多自然语言处理任务至关重要。适当的位置编码可改善自注意力模型的效果。


**为什么使用正弦和余弦计算位置编码？**
在自注意力机制的位置编码中,常用正弦和余弦函数来计算位置编码的主要原因有:

正弦和余弦可形成任意位移相干的圆形函数,可以通过位置参数来生成彼此独立的编码。
通过不同频率的正弦和余弦,可以生成多种频率和相位混合的位置编码。
这些三角函数编码能够引入顺序信息,同时保持编码向量的绝对位置不变。
计算正弦和余弦非常高效,并且对位置偏移具有循环特性,有利于学习位置相对关系。
前面的位置使用低频三角函数,后面的使用高频,这与自然语言中前后位置关系的重要性相关。
正弦和余弦函数的一阶导数相互垂直,表示第一个位置对第二个位置的变化敏感,编码互相独立。
三角函数编码是平滑变化的,符合词向量在语义空间中连续变化的假设。
三角函数组合丰富,可以通过调整频率和相位来学习各种复杂的位置关系。
综上,使用正弦和余弦计算位置编码,既符合位置关系的周期特点,也让编码连续平滑地传递位置信息,是位置编码的有效设计选择。


**除此之外还有其它计算位置编码的方式吗？**
正弦和余弦函数是计算位置编码的常用和有效方法,但也可以考虑其他计算位置编码的方式:

直接使用位置索引:最简单的方法是直接用序列位置的索引值作为编码,这可以明确表示位置信息,但不如三角函数编码平滑。
随机初始化:使用随机数作为位置编码的初始化,在训练中学习编码。但会损失编码的全局稳定性。
统计生成:通过序列的统计分布生成位置权重编码,如位置的tf-idf等。但可能难以适应不同任务。
学习位置嵌入:将位置索引映射为embedding向量,在训练中学习这些向量。需要更大的模型容量。
分层正弦编码:除了位置,还使用层级信息生成多组频率不一的三角函数编码。
指数衰减:使用指数衰减函数来表示更远位置的编码强度衰减,这类Encoding较为平滑。
结合内容表示:将位置编码与输入序列的内容表示结合,加入内容信息。
外部记忆模块:使用外部存储模块追踪序列位置,不仅作为位置编码入力。
总之,位置编码的设计可以根据不同需求进行改进和扩展,关键是要提供序列顺序信息,且编码要可学习和优化。


**transformer训练完成作预测时，如果预测第t+1个输出？**
您好，这是Bing。感谢您的提问。Transformer是一种基于自注意力机制的神经网络模型，它可以用于序列到序列的生成任务，如机器翻译、文本摘要等。Transformer在训练完成后，进行预测时，需要使用一个循环的过程，逐个生成输出序列中的每个元素。具体来说，Transformer的预测过程如下：

首先，将输入序列（例如源语言句子）编码成一个隐藏状态序列，作为编码器的输出。这一步只需要进行一次，不随着输出序列的生成而改变。
然后，初始化一个输出序列，只包含一个起始符号（例如<sos>）。将这个输出序列作为解码器的输入，通过解码器得到一个预测概率分布，表示下一个输出元素的可能性。从这个概率分布中采样或选择一个元素（例如目标语言单词），并将其添加到输出序列中。
接着，重复上一步，将更新后的输出序列再次输入解码器，得到下一个元素的预测概率分布，并从中采样或选择一个元素，添加到输出序列中。
不断重复这个过程，直到生成一个终止符号（例如<eos>）或达到最大长度限制为止。最终得到的输出序列就是Transformer的预测结果。
需要注意的是，在解码器中，为了避免模型看到未来的信息，需要使用一个掩码矩阵来屏蔽掉当前位置之后的所有位置。这样，模型在预测第t+1个输出时，只能看到第t个及之前的输出，而不能看到第t+2个及之后的输出。这样可以保证模型的预测是基于当前已知信息进行的。

https://blog.csdn.net/qq_42599237/article/details/123383691
