---
published: false
layout: post
title: "预告"
categories: 我的AI新书
date: 2023-09-21 00:00:00 +0800
excerpt: "预告"
---


微调



深度学习中最重要的一个技术




标注一个数据集很贵



ImageNet 标注了一千多万张图片，但是实际使用的只有 120 万张图片，类别数是 1000 ，它算是一个比较大的数据集
Fashion-MNIST 一共有 6 万张图片，类别数是 10 ， 算是一个比较小的数据集
通常自己的数据集规模通常在这两者之间，大概在 5 万张图片左右，类别数大概是 100 左右，平均下来每一类物体大概有 500 张图片左右


适合 ImageNet 的复杂模型可能会在自己的数据集上过拟合，另外由于训练样本有限，训练模型的准确性可能无法满足实际要求，解决以上问题有两种解决方案：

1、收集更多的数据。数据集越大越好，但是收集和标记数据可能需要大量的时间和金钱。

2、应用迁移学习（transfer leanring）。将从源数据集学到的知识迁移到目标数据集，通常来说希望在大数据集上训练好的模型能够提取到更通用的图像特征，有助于识别边缘、纹理、形状和对象组合，从而帮助提升在自己数据集上的精度，核心思想是假设模型对整个物体识别有一定的基础的情况下，不需要自己提供太大的数据集就能够获得很好的识别精度，这也是人工智能所追求的目标









网络架构



一个神经网络一般可以分为两块，一部分做特征提取，一部分做线性分类

假设将一张图像输入到模型中，可以认为最下面的一部分是在进行特征提取（特征抽取就是将原始像素变成容易线性分割的特征，深度学习的突破性进展就在于特征提取是可以学习的，而不用人工思考如何提供特征）
最后一部分就是一个全连接层和 softmax 来进行分类（可以认为是一个简单的线性分类器：Softmax 回归）









微调




假设在源数据集（一个比较大的数据集）上已经训练好了一个模型，模型中特征提取的部分对源数据集是有效的，那么它对目标数据集也应该是有效的，这样做是优于随机生成提取特征的
最后一部分是不能直接使用的，因为标号发生了改变，所以最后一部分难以进行重用
微调的核心思想是：在一个比较大的源数据集上训练好的模型中用于特征提取的部分，在目标数据集上提取特征时进行重用








微调中的权重初始化






微调包括四个步骤：

在源数据集（例如 ImageNet 数据集）上预训练神经网络模型，即源模型（pre-train model）
创建一个新的神经网络模型，即目标模型。新模型的初始化不再是随机的初始化，而是复制源模型上的所有模型设计及其参数（输出层除外）。假定这些模型参数包含从源数据集中学到的知识，这些知识也将适用于目标数据集，使得新模型在一开始就能很好地提取特征；同时假设源模型的输出层与源数据集的标签密切相关，因此不在目标模型中使用该层
向目标模型添加输出层，其输出数是目标数据集中的类别数，然后随机初始化该层的模型参数（最后的分类部分由于标号不同，因此还是做随机初始化）
在目标数据集上训练目标模型。输出层将从头开始进行训练，而所有其他层的参数将根据源模型的参数进行微调


因为损失 Loss 是从后往前进行传递的，所以最后的分类部分训练比较快，进行随机初始化也不会有太大的影响；而前面的特征提取的部分本身已经具备很好的特征提取效果，只是根据源数据集和目标数据集的差异进行微调，可能在最开始训练的时候就已经比较接近最终的结果，所以不用做过多的训练和变动








训练



是一个目标数据集上的正常训练任务，但使用更强的正则化（如果不使用预训练模型，直接在自己的数据集上正常训练，在时间足够的情况下也是可以从随机初始化训练到完全 fitting 自己的数据集，但是可能会导致 Overfitting ，这是没有必要的，不如对预训练模型进行微调）

使用更小的学习率（已经比较接近最优解了，因此不需要太大的学习率）
使用更少的数据迭代
源数据集远远复杂于目标数据集，通常微调的效果更好

源数据集的类别数、图片数量、样本个数通常是目标数据集的 10 倍或者 100 倍，才能达到很好的微调效果，否则微调的效果不如直接在目标数据集上进行重新训练








重用分类器权重



源数据集中可能也有目标数据中的标号
可以使用预训练好的模型分类器中对应标号对应的向量来做初始化








固定一些层




神经网络通常学习有层次的特征表示

低层次的特征更加通用（越低层次学习的是一些底层的细节）
高层次的特征则更跟数据集相关（越高层次则更加语义化）
可以认为越到后面和标号的关联度越大，约到前面则越低层，所以底层的特征更加通用，高层的特征和数据的关联度更大
可以固定底部一些层的参数，不参与更新（不做优化，在微调的时候不改变底层类别的权重，因为这些参数不再发生变化，所以模型的复杂度变低了，可以认为是更强的正则的效果）

更强的正则
通常来说，假设数据集很小，直接训练很容易过拟合的情况下，可以固定底部的一些参数不参与更新









总结



1、微调通过使用在大数据上得到的预训练好的模型来初始化目标数据集上的模型权重来完成提升精度

2、预训练模型质量很重要

3、微调通常速度更快，精度更高（可以借助在大数据集上所获得的先验知识）

4、建议尽量从微调开始训练，不要直接从目标数据集上从零开始进行训练

未来从原始数据集上进行训练的会越来越少，主要是学术界或者大公司在很大的数据集上进行重新训练
对于个人或者实际应用来讲，通常是使用微调
5、迁移学习将从源数据集中学到的知识“迁移”到目标数据集，微调迁移学习的常见技巧

6、除输出层外，目标模型从源模型中复制所有模型设计及其参数，并根据目标数据集对这些参数进行微调，但是目标模型的输出层需要从头开始训练

7、通常微调参数使用较小的学习率，而从头开始训练输出层可以使用更大的学习率









Q&A



1、微调这部分是意味着，神经网络进行不同的目标检测，前面层的网络进行特征提取是通用的吗？﻿
QA P3 - 00:00
﻿


2、老师，数据不平衡问题对特征提取器影响大还是分类器影响大？﻿
QA P3 - 00:26
﻿


3、假设 A 、 B 两个数据集都很大，A 是 ImageNet，B 是医学图片。如果我要识别癌症，那是用 pretrained A 的现成的模型然后加上 B 进行微调的效果好，还是直接用 B 从头进行训练比较好？﻿
QA P3 - 00:54
﻿


4、微调是不是就是迁移学习？两者有不同吗？﻿
QA P3 - 01:44
﻿


5、老师，重用标号的话，对于无关的标号是直接删除吗，原始模型中没有的标号怎么加进去呢？﻿
QA P3 - 02:03
﻿


6、标号那块，是不是都是 label 的名称字符串，对应到数字上？微调中有相同标号，怎么做原始标号和目标标号的对应？﻿
QA P3 - 02:28
﻿


7、老师，微调就是 transformer learning 吗？﻿
QA P3 - 02:53
﻿


8、ImageNet 是比较简单的话，能从哪些地方获取一些更好的可用于微调的模型？﻿
QA P3 - 03:10
﻿


9、如果源数据集和目标数据集差异很大，微调的效果会下降吗，例如 ImageNet 上的模型用到医疗影像分类？﻿
QA P3 - 03:45
﻿


10、老师，微调的话，源数据集中的样本是否必须包含目标数据集中的类别？﻿
QA P3 - 04:06
﻿


11、为什么微调中的归一化保持一致很重要？是为了保留数据分布信息吗？﻿
QA P3 - 04:19
﻿


12、normalize 里的那些参数（2 行 3 列）是从哪里来的？﻿
QA P3 - 05:05
﻿


13、finetune 需要更改 normalization 的参数为自己的数据集的均值和方差吗？﻿
QA P3 - 05:17
﻿


14、auto-gluon 这些自动化框架会加入微调吗？﻿
QA P3 - 05:36
﻿
﻿
QA P3 - 05:32
﻿

15、老师，请问比较常用的 CV 预训练模型有哪些？现在流行的 Transformer CV 预训练模型有没有？﻿
QA P3 - 05:56
﻿


16、关于“重用分类器权重”，对于一个 80 类的数据集，只想选用其中 5 类，加上另外的 4 类，怎么重用这 5 类的权重呢？有什么快速的方式么？我能想到的就是手动提取﻿
QA P3 - 06:18
﻿


17、老师好，训练集的精度为什么一开始很高，然后急剧下降再稳定呢？﻿
QA P3 - 06:50
﻿


18、老师，微调是直接把别人在 ImageNet 上训练好的模型参数拿来当作自己模型的初始化吗？还是每次自己先用模型在 ImageNet 数据集上跑一遍，把参数记录下来再跑自己的模型呢？﻿
QA P3 - 07:03
﻿


19、计算损失的时候不都是用 label 对应的标号么，这块用微调对应的话感觉不知道怎么操作？﻿
QA P3 - 07:30
﻿


20、老师，请问微调在学习率上还有什么有用的技巧？﻿
QA P3 - 07:59
﻿


21、老师，迁移学习固定源模型中的层与目标模型的对接的层之间有影响吗？﻿
QA P3 - 08:12
﻿


22、老师，在光学图像上的训练好的模型，能否迁移到其他图像的分类上？比如雷达图像？还是说这种情况，应该在雷达图像上训练好的模型迁移到自己的小问题上？﻿
QA P3 - 09:04
﻿


23、为什么不用微调一开始测试精度也很高？﻿
QA P3 - 10:30
﻿


24、是不是已经有过随机是选择层的实验了呢？如何选择迁移那些层效果更好呢，就是靠测试吗？﻿
QA P3 - 10:53
﻿








----end----

其他参考

1、《动手学深度学习》，课程安排，https://courses.d2l.ai/zh-v2/assets/pdfs/part-2_6.pdf

2、《动手学深度学习》，https://zh-v2.d2l.ai/chapter_computer-vision/fine-tuning.html 作者：如果我是泡橘子 https://www.bilibili.com/read/cv16394315/?from=readlist&jump_opus=1 出处：bilibili