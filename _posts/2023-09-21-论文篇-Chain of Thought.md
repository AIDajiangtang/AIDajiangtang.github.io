---
published: false
layout: post
title: "Chain of Thought论文精读"
categories: 我的AI新书
date: 2023-09-21 00:00:00 +0800
excerpt: "Chain of Thought论文精读"
---


Chain of Thought Prompting Elicits Reasoning in Large Language Models

chain of thought:也就是 COT ，一经提出就引发了社区对它的热烈讨论，类似 AI 是不是也需要鼓励来获得更好的表现之类的问题




CoT readinglist













背景知识


这篇文章中说，只要在每个答案之前加上一句“Let's think step by step”，就可以立即在两个比较困难的数学问题数据上涨点，而且涨点非常明显
由于这个方法比较简单，只是加了一句话就能够非常明显地涨点，所以立即引发了大家对于这一领域的关注，也就是“ AI 是不是也需要鼓励来获得更好的表现”




语言模型的本质是对任意一段文本序列的概率进行建模

﻿
02:59
﻿

如果将语言模型看成一个大黑盒的话，它的输入是一段文本序列，输出也是一段文本序列，通过训练语言模型，就能使得给定的文本序列和输出的文本序列拼接起来所组成的一整段文本序列的概率尽可能比较大
语言模型的细节可以参考《动手学深度学习》的语言模型和数据集章节：https://zh-v2.d2l.ai/chapter_recurrent-neural-networks/language-models-and-dataset.html；笔记：https://www.bilibili.com/read/cv17622666




如何用 GPT-3 这类的大语言模型来做零样本、单样本和少样本学习?

﻿
05:41
﻿


对于 GPT-3 来说，也就是图中的 transformer decoder，无论是在零样本、单样本还是少样本的情况下，它们的输入都是一段文本序列，输出也是一段文本序列
少样本与零样本的唯一区别就是中间多出了一些参考样例，它们其实都是在续写前缀（只是零样本的输入没有任何参考，而少样本的输入有一些参考样例来帮助语言模型推断如何根据任务输入生成相应的任务输出）




用一个训练好的大语言模型来求解推理任务的几种范式

这里以需要推理的数学题举例
1、Zero-shot

﻿
06:44
﻿


文献：Large Language Models are Zero-Shot Reasoners(https://arxiv.org/abs/2205.11916)
语言模型的输入是一道数学题连接一个字符串“The answer is”，然后让语言模型进行续写
2、Zero-Shot-CoT


语言模型的输入还是一道数学题连接一个字符串“Let's think step by step”，然后让语言模型进行续写
这种情况下，语言模型会续写出中间推理步骤，并最终生成答案
3、Manual-CoT


文献：Chain of Thought Prompting Elicits Reasoning in Large Language Models（https://arxiv.org/abs/2201.11903）
这种情况下使用到了少样本学习，在输入问题之前，手动设计一些问题和答案的样例（样例的答案给出中间推理步骤），这些问题和答案都需要手动构造，所以叫 Manual-CoT
语言模型的输入是一些手动设计的问题和答案的参考样例连接一个真正需要求解的问题，然后让语言模型进行续写
这里少样本训练中的问题和答案的样例都需要人为构造并手动设计，因此为了和第四种自动 CoT 做区分，这里称为 Manual-CoT
Manual-CoT 比 Zero-Shot-CoT 的性能要好，因为它采用的是 few shot ，在输入中提供了一些问题、中间推理步骤以及答案的样例给语言模型进行参考。但是，提供这些样例需要进行人工设计，这就需要一定的人工成本
4、Auto-CoT

文献：Automatic Chain of thought Prompting in Large Language Models（https://arxiv.org/abs/2210.03493）
Auto-CoT 其实也是受到了 Manual-CoT 的启发，既然Manual-CoT 比 Zero-Shot-CoT 的性能要好，而且性能好的关键就在于人工设计的问题、中间推理步骤和答案的样例，那么就可以考虑将这部分进行自动化，从而节省人工成本

实时发现是可行的，做法主要分为两步

通过多样性选取有代表性的问题
对于每一个采样的问题拼接上“Let's think step by step”（类似于 Zero-Shot-CoT ）输入到语言模型，让语言模型生成中间推理步骤和答案，然后把这些所有采样的问题以及语言模型生成的中间推理步骤和答案全部拼接在一起，构成少样本学习的样例，最后再拼接上需要求解的问题一起输入到语言模型中进行续写

最终模型续写出了中间的推理步骤以及答案，并且质量非常高
值得一提的是，在十个数据集上 Auto-CoT 是可以匹配甚至超越 Manual-CoT 的性能，也就说明自动构造的 CoT 的问题、中间推理步骤和答案样例比人工设计的还要好，而且还节省了人工成本
在 Auto-CoT 中，其实也是用到了很多个“Let's think step by step”对每个采样的问题分别触发中间推理步骤和答案，这也是为什么叫它“Let's think not just step by step but also one by one”，也就是AI需要多鼓励几次
代码实现

﻿
11:42
﻿

https://github.com/amazon-research/auto-cot 目录下 try_cot.ipynb 文件
CoT 这个领域刚发展不久，所以还有很多未解之谜，尽管刚提出才几个月，但是已经受到了社区的广泛关注









Abstract

衡量语言模型规模的三个角度

训练计算量：FLOPs
训练数据的大小：num of tokens
模型本身参数量的大小
这三个量往往是协同增长的，文中所提到的规模主要还是从模型本身的参数量来看的

现在语言模型的规模越来越大，但是即便是现在最大的语言模型，它们也往往很难在涉及到推理方面的任务取得很好的表现，也就是说，他们通常很难在数学，符号，以及常识的推理上取得尚佳的表现

这篇文章主要是针对大语言模型在遇到语言推理任务时的局限性，提出了 chain of thought，也就是思维链

文中也给出了 CoT 的定义：人类在遇到一系列问题时所产生的推理步骤，而它们的表现形式就是一系列的短句子（比如说在背景介绍中所提到的遇到数学问题时所产生的中间推理步骤）
最终的实验效果非常好，比如说在使用谷歌内部的 540B 参数量的 PaLM 大语言模型，CoT 能够在像 GSM8K 这样比较难一点的数学问题数据集上取得新的 state of art









1.Introduction

语言模型的规模达到 100B 的参数量之后，就能够在像 sentiment analysis and topic classification 这种分类任务上取得非常好的结果

作者将这类任务归纳为 system-1，也就是能够人类很快很直观地理解的任务
还有一类任务需要很慢而且是很仔细的考虑，作者将其归纳为 system-2 （比如一些设计逻辑、常识的推理任务）
作者发现，即便语言模型的规模达到了几百B的参数量，也很难在 system-2 这类任务上获得很好的表现

作者将这种现象称为 flat scaling curves：如果将语言模型参数量作为横坐标，在 system-2 这类任务上的表现作为纵坐标，则折线就会变得相当平缓，不会像在 system-1 这类任务上那么容易就实现模型的性能随着模型参数量的增长而提升，也就是说，在 system-2 这类任务上语言模型就很难大力出奇迹了
针对这个问题，作者提出了 chain of thought （CoT）这种方法来利用大语言模型求解推理任务





图1


上图展示了在 CoT 诞生之前是怎样使用标准的 prompting 方法来求解推理任务的
首先这是一个少样本学习的方法，需要给出一些问题和答案的样例，然后拼接这正想要求解的问题，最后再拼接一个字符串“A:”之后输入到大语言模型中，让大语言模型进行续写
大语言模型会在所提供的问题和答案的样例中学习如何求解，结果发现很容易出错，也就是上面提到的大语言模型在 system-2 上很容易遇到瓶颈

上图展示了 CoT 的做法，CoT 与 Standard prompting 唯一的区别就是，CoT 在样例中在给出问题的同时，不仅给出了答案，在答案之前还给出了人为写的中间推理步骤
在把问题、中间推理步骤和答案的若干样例拼接上所想要求解的问题和字符串“A”，再输入到语言模型之后，语言模型会自动地先续写中间推理步骤，有了这些推理步骤之后，它就会更容易地给出正确答案，也就是能够更好地解决 system-2 这类的问题








2.Chain of thought

chain of thought 的定义：在应对推理任务时，在给出最终答案之前所产生的中间推理步骤，他们载体是一系列的短句子

chain of thought 也可以和最后的答案合在一起，作为一个整体。但是作者还是将中间解题步骤叫做 CoT ，这样才能更好地表达模拟人类一步一步思考而最终得出答案的过程这一内涵

“step by step”其实在这篇文章中就已经提到了
因为 CoT 是作者所提出的一个新事物，所以作者强调了 CoT 中几个比较有意思的地方

首先，CoT 原则上能够让模型把一个多步的问题分解出各种中间步骤，使那些具有更多推理步的问题有机会分配到更多的计算量（如果是从最后的将拼接好的问题、答案样例以及所要求解的问题和前缀输入到语言模型中产生最后的答案这一步来看，对于一个更难的问题，在续写的时候，CoT就使得语言模型能够产生更多的中间推理步骤，因为语言模型在生成输出的时候是一个一个 token 进行生成的，那么如果问题越难，CoT 又使得生成的中间步骤越多，那么整体上生成的 token 的数量也会越多，自然而然在求解更难的问题的时候就会使用到更多的计算量。就好比人类在遇到更难得问题的时候，可能就会耗费更多的脑力，这样 CoT 也能够让计算机能够对更难的问题分配更多的计算资源）
CoT 提供了可解释性，也就是在不知道答案的情况下，也能够知道答案是怎样得来的，也就是所谓的中间推理步骤
作者认为 CoT 在原则上能够适用于任何人类能够用语言所能解决的问题，而不仅仅是数学、逻辑、常识这类的问题。因为 CoT 本身的载体就是一系列的短句子，本身也是人类语言
当一个语言模型训练好之后，就能够通过 few-shot prompting 这种范式，在每个样例中写上中间推理步骤，再拼接好所要求解的问题输入到语言模型，就能够引发语言模型续写中间推理步骤，再得出最后的答案（像 Zero-Shot CoT 就发现，甚至都不需要在 few-shot 这些样例中添加 CoT ，可以仅凭“let's think step by step”作为 CoT 的推理；而 Auto CoT ，也就是“Let's think not just step by step but one by one”使用了多个“let's think step by step”就可以自动地构造 few-shot 的样例，从而弥补了 Zero-shot 和 Few-shot 之间的性能差异）








3.Arithmetic Reasoning

算数推理，也是本文中最重要的部分
这里的算数推理所考虑的问题范围集中在小学数学问题，也就是 6-10 岁的小朋友所能解决的数学问题





实验设计

作者人工设计了一套 8 个带有 CoT 推理链条的 few-shot 样例，而且作者在六个数据集中统一使用了这 8 个带有 CoT 推理链条的 few-shot 样例

其中的一个原因是因为人工构造 CoT 推理链条的 few-shot 样例的成本是很高的，因为不仅要找到具有代表性的问题，还要为每个问题设计中间推理步骤以及答案，而最后的性能对这些人工设计非常敏感，所以需要反复进行调试。
所以在后续的 auto CoT 的工作中就想将人工设计的这部分工作自动化，自动化的一大好处是能够对这六个数据集中的每个数据集分别地自动构造带有 CoT 推理链条的 few-shot 样例，而且这也能够带来性能的提升
所以多读这种对细节描述比较详细的优秀论文有时候也能够启发我们研究的灵感




对于这一节中的主要实验看一下图2和图3就可以了，实验是基于 LaMDA 和 PaLM 这两个谷歌内部的大语言模型来做实验（这里是 arxiv 的 V2 版本，作者在后续的 arxiv 版本中也补充了其他模型的实验）

图 2

﻿
26:08
﻿


图 3


在图 2 中展示了在四个相对比较简单的数据集上， standard prompting 可以随着语言模型大小的不断增加而获取性能上的提升，这样的话 CoT prompting 就没有产生特别碾压性的优势
而在图 3 中，当换成两个更加具有挑战性的数学推理数据集时，standard prompting 就出现了之前引言中所提到的 flat scaling curve 现象（随着语言模型规模的增加，性能并没有获得显著性的提升）；而 CoT prompting 确实可以随着语言模型大小的不断增加而不断地提升性能，这里就体现了 CoT 的重要性




CoT 的方法很简单，但是一个好的工作往往会告诉你：“我的方法是很简单，但是不能再简单了，也就是 It is simple,but it can't be simpler”，所以作者也进行了一系列的消融实验，消融结果实验如表 3 所示

表 3


Equation Only：把 CoT 替换成只包含 CoT 中的算式部分
Variable compute only：把 CoT 替换成与中间 CoT 长度相等的”`````“
结果发现上述两种方法都比 CoT 差了很多，这就说明了 CoT 虽然简单，但是不能再简单了，而且这也更能体现出 CoT 中自然语言所起的作用
Thought after answer：把中间推理步骤放在答案的后面
实验结果显示：把中间推理步骤放在答案的后面所得到的结果也不是很好，这就说明，在训练数据集中大部分情况下依然还是先给出中间推理步骤再给出答案，而不是先给出答案再给出中间推理步骤




在本节的最后，作者也提到 CoT 的性能可能也会对人工设计的 prompt 比较敏感，因此有必要评测所提出的方法的鲁棒性，有关鲁棒性的实验结果如图 4 所示

图 4


粉红色的五角星表示有三个作者分别设计了一套带有 CoT 的样例
颜色淡一点的五角星表示作者专门又设计了一套更加简单的带有 CoT 的样例
最后的棕色十字代表从带有中间推理步骤的数据集中随机地选取一些问题，并附上这些数据集中自带的中间推理步骤构成 CoT 的样例，再拼接上答案来进行评测
最后的结果显示：带 CoT 的方法都要比不带 CoT 的 standard prompting 要带来更加显著的性能提升，尤其是在图三中的两个更加困难的数学推理数据集中，这种性能提升更加明显








4.Symbolic Reasoning

文章中在本节和下一节中还提到了一些其他类型的任务

符号推理任务

﻿
30:01
﻿










5. Commonsense Reasoning

常识推理任务

﻿
31:33
﻿






图 6


图 6 中展示了 CoT 模型在这些任务的性能，和之前类似，尤其是当你的语言模型的规模很大的时候，CoT 相对普通的 prompting 方法能够带来更加显著的性能提升








6.Discussion

在文章的末尾，作者强调，在 CoT 诞生之前的标准的 prompting 只是大语言模型语言能力的一个下限









----end----

其他参考：

1、《动手学深度学习》，英文版教程，https://d2l.ai/chapter_attention-mechanisms-and-transformers/large-pretraining-transformers.html

2、Chain of Thought Prompting Elicits Reasoning in Large Language Models，https://arxiv.org/pdf/2201.11903v2.pdf 作者：如果我是泡橘子 https://www.bilibili.com/read/cv19283943/?from=readlist&jump_opus=1 出处：bilibili