---
published: true
layout: post
title: "一起学AI 5-学会用概率的思维看世界"
categories: 一起学AI

banner:
  video: https://vjs.zencdn.net/v/oceans.mp4
  loop: true
  volume: 0.1
  start_at: 8.5
  image: https://bit.ly/3xTmdUP
  opacity: 0.618
  background: "#000"
  height: "100vh"
  min_height: "38vh"
  heading_style: "font-size: 4.25em; font-weight: bold; text-decoration: underline"
  subheading_style: "color: gold"
excerpt: "一起学AI 5-学会用概率的思维看世界"
top: 5
---




什么是概率？
概率是一种用来描述不确定事件发生的可能性的数值。



理性主义和经验主义
在哲学世界里，有一类人，他们相信​这世上存在看某种先验的真理，世间万象都是真理的表象，这种人被称为理性主义者，与之相对应的是经验主义者，经验主义者认为，要了解世界，就必须观察、实验和归纳，而不是仅仅依靠逻辑或直觉。


我们要明白，概率论和哲学都是关于我们如何认识和描述世界的学科，它们都试图解决不确定性和可能性的问题。概率论是一种数学工具，可以用来量化和推理不确定性，而哲学是一种思辨的方法，可以用来探索和评估不同的观点和理论。


频率主义和贝叶斯主义
在概率论中，对于概率的认知，同样分为两派，一是频率主义，二是贝叶斯主义，
频率主义和贝叶斯主义是统计学中两种不同的流派，它们对概率的含义和推断的方法有不同的理解和应用。
首先，我们要明白什么是概率。概率是一种用来描述不确定性的数学工具，它可以用来表示某件事情发生的可能性，或者某个命题为真的可信度。例如，我们可以说明天下雨的概率是30%，或者说某个人是罪犯的概率是80%。但是，这些概率是怎么得到的呢？它们是客观的事实，还是主观的判断？它们是固定的常数，还是随着数据的变化而变化？这就涉及到了频率主义和贝叶斯主义的不同观点。
频率主义认为，概率是客观的事实，它反映了事件在长期重复实验中出现的频率。因此，要确定概率，就需要进行大量的观测和统计，而且概率是固定的常数，不受个人的主观影响。例如，频率主义者会说，一枚公平的硬币正面朝上的概率是50%，这是因为如果我们无限次地抛这枚硬币，正面朝上的次数会趋近于一半。频率主义的优点是它比较客观和可重复，它的缺点是它需要大量的数据，而且不能利用先验知识或者个人经验。
贝叶斯主义认为，概率是主观的判断，它反映了我们对事件或命题的信念程度。因此，要确定概率，就需要结合先验知识和观测数据，而且概率是随着数据的变化而变化的随机变量，不同的人可能有不同的概率。例如，贝叶斯主义者会说，一枚硬币正面朝上的概率是我对这个事件的信念，这个信念可能是基于我对这枚硬币的了解，也可能是基于我抛过这枚硬币的结果，而且这个信念会随着我获得更多的信息而更新。贝叶斯主义的优点是它比较灵活和适应，它可以利用先验知识或者个人经验，也可以处理数据稀缺或不确定的情形。它的缺点是它比较主观和复杂，它需要选择合适的先验分布，而且计算后验分布可能比较困难。

假设你有一枚硬币，你想知道它正面朝上的概率是多少。你可以用以下两种方法来估计这个概率：

频率主义的方法：你可以把硬币抛很多次，比如1000次，然后数一数有多少次是正面朝上的，再除以总次数，就得到了一个近似的概率。比如，如果你抛了1000次，有520次是正面朝上的，那么你就认为这个概率是0.52。这个方法的优点是比较客观和简单，但是缺点是需要大量的数据，而且不能考虑其他的信息，比如硬币的重量、形状等。
贝叶斯主义的方法：你可以先假设一个先验的概率，比如0.5，表示你最初的信念是这个硬币是均匀的。然后你可以把硬币抛一些次，比如10次，然后根据贝叶斯定理，用你观察到的数据来更新你的信念，得到一个后验的概率。比如，如果你抛了10次，有7次是正面朝上的，那么你就认为这个概率是0.65。这个方法的优点是可以利用先验知识和少量的数据，但是缺点是比较主观和复杂，而且需要选择合适的先验分布。





为什么会有概率？
你在信息不完整的情况下做决策，所以需要一种方法量化不确定性-概率论就提供了这种方法。
与线性代数和微积分相比，概率最接地气，最贴近生活。

两暗一黑三起源：暗物质，暗能量，黑洞，宇宙起源，生命起源，意识起源
黑洞的奇点附近物理定律失效
没有什么是确定的，这世界充满了不确定性。

正是因为没有什么是确定的，所以本文的观点也是建立在某种假设情况下。
这个世界上有什么事情是100%确定的吗？这是一个很有意思的问题，也是一个很难回答的问题。不同的人可能会有不同的看法，也可能会从不同的角度来探讨这个问题。例如，有些人可能会从 哲学 、 物理学 、 数学 、 逻辑学 、 心理学 等不同的学科来思考这个问题，也可能会得出不同的结论。有些人可能会认为，这个世界是由一些 客观的规律 和 定律 所决定的，因此有些事情是可以确定的，比如数学中的公理和定理，物理学中的牛顿定律和爱因斯坦相对论等。有些人可能会认为，这个世界是由一些 随机的事件 和 概率 所构成的，因此没有什么事情是可以确定的，比如量子力学中的不确定性原理，统计学中的大数定律和中心极限定理等。还有些人可能会认为，这个世界是由我们的 主观的感知 和 认知 所塑造的，因此有些事情是可以确定的，比如我们的存在和意识，我们的感受和思想等。
P(人一定会死)=1？在现有科技医疗水平下可能成立，但是否有外星文明？是否有未公开的技术？

生活中的概率：

天气预测

赌球，赛马

政策，投机

销售预测

当你学会运用概率的眼光看世界，你才能运筹得当，才能做出最有力的决策。


概率论中的三大件：随机变量，事件，样本空间


请用通俗易懂的话讲解概率中的随机变量，事件，样本空间，概率分布之间的关系，最好举个例子

概率中的随机变量，事件，样本空间，概率分布之间的关系，可以用以下的例子来解释：
假设我们要掷一个骰子，每次掷出的点数就是一个随机变量，它是一个可以随机取值的数值，比如1，2，3，4，5，6。
我们可以对这个随机变量的取值进行一些判断，比如是否为偶数，是否大于3，是否等于4等，这些判断的结果就是事件，它是一个可以用是或否来回答的问题，比如是偶数，是大于3，不是等于4等。
我们可以把所有可能的点数放在一起，形成一个集合，这个集合就是样本空间，它是一个包含了所有可能的随机变量取值的集合，比如 {1,2,3,4,5,6}。
我们可以对每个点数出现的可能性进行度量，这个度量就是概率，它是一个表示事件发生的可能性的数值，比如掷出偶数的概率是1/2，掷出大于3的概率是1/2，掷出等于4的概率是1/6等。
我们可以把每个点数的概率列出来，形成一个函数，这个函数就是概率分布，它是一个描述了随机变量取值和概率之间的关系的函数，比如 P(X=x) = 1/6, x = 1,2,3,4,5,6。
总结一下，随机变量是一个可以随机取值的数值，事件是一个可以用是或否来回答的问题，样本空间是一个包含了所有可能的随机变量取值的集合，概率是一个表示事件发生的可能性的数值，概率分布是一个描述了随机变量取值和概率之间的关系的函数。


随机变量

我们要知道什么是随机变量。随机变量是指随机事件的数量表现，也就是把随机事件用数值来表示的变量。例如，掷一枚硬币，我们可以用随机变量X来表示出现正面或反面的事件，规定X=1表示出现正面，X=0表示出现反面。这样，X就是一个随机变量，它的取值是随机的，有可能是1，也有可能是0，而且每种取值的概率都是1/2。随机变量的取值范围称为样本空间，例如，X的样本空间是{0,1}。


概率分布

https://www.adrian.idv.hk/2018-03-19-distribution/


数据存在概率分布还是随机变量存在概率分布？

概率分布是概率论中的一个重要概念，它描述了随机变量的概率性质，即随机变量的可能取值及取得对应值的概率。1 概率分布可以分为离散概率分布和连续概率分布，根据随机变量的类型不同，有不同的计算方法和表示形式。2

一般来说，我们说数据存在概率分布，是指数据的分布可以用某种概率分布来描述或近似，例如，我们可以用正态分布来描述身高数据的分布，或者用泊松分布来描述某段时间内发生的事件次数的分布。3 但是，这并不意味着数据本身就有概率分布，而是数据是由某个随机变量生成的，而随机变量才是具有概率分布的。4

因此，从严格的数学角度来说，应该说随机变量存在概率分布，而不是数据存在概率分布。数据只是随机变量的观测值或样本，它们反映了随机变量的概率分布的特征，但不等同于随机变量的概率分布。4



当我们说数据符合某个分布意味着什么？

数据是随机变量的观测
当我们说数据符合某个分布意味着什么？这是一个很好的问题，它涉及到概率论和统计学的基本概念。我从网上搜索了一些相关的资料，希望对您有所帮助。
简单地说，当我们说数据符合某个分布，就是指数据的出现概率可以用某个数学函数来描述或近似。这个数学函数就是概率分布函数，它可以是离散的或连续的，取决于数据的类型。概率分布函数可以反映数据的特征，如均值、方差、偏度、峰度等，也可以用来计算数据的期望、方差、置信区间、假设检验等。

举个例子，如果我们说身高数据符合正态分布，就是指身高数据的出现概率可以用正态分布函数来描述，即 P (X=x) = 1 / (σ√2π) e(-(x-μ)2 / (2σ^2))，其中 μ 是均值，σ 是标准差。这个函数的图形是一个钟形曲线，它的特点是对称、单峰、两侧渐近于零。正态分布函数可以告诉我们，身高数据的均值是 μ，方差是 σ^2，大约 68% 的数据在 μ±σ 范围内，大约 95% 的数据在 μ±2σ 范围内，大约 99.7% 的数据在 μ±3σ 范围内。2

当然，并不是所有的数据都符合某个已知的分布，有些数据可能是复杂的或者随机的，无法用一个简单的函数来表示。这时候，我们可以用一些方法来检验数据是否符合某个分布，或者用一些方法来拟合数据的分布，比如最大似然估计、最小二乘法、卡方检验、Kolmogorov-Smirnov检验等。
希望这些内容对您有所帮助。






我们通常都会假设理想情况下，但现实中由于数据噪声的存在，不存在确定的模型，只能求得最大概率满足训练数据的模型。

AI中都有哪些地方运用了概率？

概率模型
机器学习是一门利用数据和算法来模拟人类学习的科学，它在很多领域都有广泛的应用。概率论是机器学习的重要数学基础，它可以用来描述和处理不确定性，分析变量之间的依赖关系，实现因果推理，以及生成随机样本数据。在机器学习中，有很多算法都是基于概率的，例如：

贝叶斯分类器：它使用贝叶斯公式来计算样本属于每个类的概率，然后选择概率最大的类作为预测结果。
logistic回归：它使用逻辑函数来估计样本是正样本的概率，然后根据概率值的大小来进行二分类。
softmax回归：它是logistic回归的多分类版本，它使用softmax函数来估计样本属于每个类的概率，然后选择概率最大的类作为预测结果。2
变分推断：它使用KL散度来度量两个概率分布的相似度，然后找到一个近似的概率分布来替代难以计算的后验概率分布。
高斯过程回归：它假设要预测的函数在任意点处的函数值服从高斯分布，然后根据已知的样本值来计算任意点处的函数值的概率分布。
SNE降维算法：它使用正态分布和KL散度来度量高维空间和低维空间中的近邻关系，然后将高维数据投影到低维空间中。
神经网络：它使用多层的人工神经元来模拟人脑的信息处理，它可以用来进行分类、回归、聚类、生成等任务。


生成模型和判别模型

生成模型和判别模型是两种常见的机器学习方法，它们的区别和联系可以用以下的方式来理解：
生成模型是一种基于概率的方法，它试图学习数据的联合概率分布 P (X,Y) ，也就是说，它想要知道数据是怎么生成的，以及不同的数据和标签之间有什么样的关系。生成模型可以用来生成新的数据，也可以用来计算后验概率分布 P (Y|X) ，从而进行分类或预测。生成模型的例子有朴素贝叶斯、隐马尔可夫模型、混合高斯模型等。
判别模型是一种基于函数的方法，它试图学习数据的条件概率分布 P (Y|X) ，也就是说，它只关心给定的数据属于哪个类别或者有什么样的输出，而不关心数据是怎么生成的。判别模型只能用来进行分类或预测，不能用来生成新的数据。判别模型的例子有线性回归、逻辑回归、支持向量机、神经网络等。
生成模型和判别模型的区别可以用一个比喻来说明：假设我们要判断一只羊是山羊还是绵羊，生成模型的方法是先根据山羊的特征学习一个山羊的模型，然后根据绵羊的特征学习一个绵羊的模型，最后根据这只羊的特征，看它更可能是由哪个模型生成的，就判断它属于哪个类别。判别模型的方法是直接根据历史数据，学习一个函数，可以根据这只羊的特征，直接输出它是山羊的概率或者是绵羊的概率，然后选择概率最大的作为最终类别。




条件概率：语言模型和马尔科夫链
语言模型和马尔科夫链是两个在自然语言处理中经常使用的概念，它们都是基于概率的方法来描述和生成语言序列的。下面是一些相关的信息：
语言模型是一种用来计算一个语言序列（如一个词、一个句子或一个文档）出现的概率的数学模型。语言模型的目的是为了捕捉语言的规律性和规则性，从而可以用来进行语言理解、生成、翻译、纠错等任务。语言模型的基本假设是，一个语言序列的概率可以分解为其组成的子序列的概率的乘积，即 P ( w 1 , w 2 , … , w n ) = ∏ i = 1 n P ( w i | w 1 , w 2 , … , w i − 1 ) {\displaystyle P (w_ {1},w_ {2},\ldots ,w_ {n})=\prod _ {i=1}^ {n}P (w_ {i}|w_ {1},w_ {2},\ldots ,w_ {i-1})} ，其中 w i {\displaystyle w_ {i}} 是第 i 个词， P ( w i | w 1 , w 2 , … , w i − 1 ) {\displaystyle P (w_ {i}|w_ {1},w_ {2},\ldots ,w_ {i-1})} 是第 i 个词在给定前 i − 1 个词的条件下出现的概率1。
马尔科夫链是一种随机过程，它描述了一个系统在不同状态之间的转移。马尔科夫链的特点是，系统的下一个状态只取决于当前状态，而与过去的状态无关，这就是所谓的马尔科夫性或无后效性。马尔科夫链是通过状态转移的概率分布体现的，称为马尔科夫链的转移矩阵。如果马尔科夫链有 N 个可能的状态，则转移矩阵的大小是 N 阶，矩阵的项（i,j）是状态 i 到状态 j 的转移概率，另外矩阵的每一行的转移概率和等于 1，表示状态 i 的概率分布。1马尔科夫链是如何应用概率的呢？首先，我们需要知道系统的初始状态，即每个状态出现的概率，称为初始概率。然后，我们可以根据转移矩阵，计算系统在任意时刻的状态分布，称为绝对概率。具体地，如果我们用一个向量 \pmb {\pi} (n) = (\pi_1 (n),\pi_2 (n),\cdots,\pi_N (n)) 表示系统在时刻 n 的状态分布，其中 \pi_i (n) 是状态 i 出现的概率，那么我们有以下的公式：\pmb {\pi} (n+1) = \pmb {\pi} (n)\pmb {P}，其中 \pmb {P} 是转移矩阵。这个公式表示，系统的下一个状态分布等于当前状态分布乘以转移矩阵。如果我们知道系统的初始状态分布 \pmb {\pi} (0)，那么我们可以用这个公式递推地计算出任意时刻的状态分布。2



判别模型对条件概率建模

什么模型对联合概率建模
根据我的搜索结果，有以下几种模型对联合概率建模：
生成模型：生成模型旨在学习数据生成的过程，因此，它试图模拟输入数据和其相关标签的联合概率分布。例如，朴素贝叶斯和隐马尔可夫模型就是生成模型。1
高斯过程回归：高斯过程回归假设我们要预测的函数在任意各个点x处的函数值f (x)形成一个高斯过程，即服从多维正态分布。这样，我们就可以根据已知的样本值和函数值，得到任意点处的函数值的概率分布，也是一个正态分布。2
SNE降维算法：SNE降维算法是一种基于概率的流形降维算法，它假设在高维空间中距离很近的点投影到低维空间中之后也要保持这种近邻关系，这种关系通过正态分布的概率体现。





贝叶斯定理

贝叶斯定理是概率论中的一个重要定理，它可以帮助我们在不完全的信息下，根据已知的条件，推断出未知的概率。我从网上搜索了一些相关的资料，希望对您有所帮助。
贝叶斯定理的基本思想是：后验概率 = 先验概率 x 调整因子。后验概率是指在得到新的信息后，对某个事件发生的概率的重新估计。先验概率是指在没有新的信息之前，对某个事件发生的概率的初始估计。调整因子是指根据新的信息，对先验概率的修正程度。
贝叶斯定理的数学表达式是：P(A|B) = P(A)P(B|A)/P(B)，其中P(A|B)表示在事件B发生的条件下，事件A发生的概率，也就是后验概率；P(A)表示事件A发生的概率，也就是先验概率；P(B|A)表示在事件A发生的条件下，事件B发生的概率，也就是调整因子；P(B)表示事件B发生的概率，也就是标准化常数。
举一个简单的例子，假设有两个罐子，罐子A里有3个红球和7个白球，罐子B里有8个红球和2个白球。现在随机从一个罐子里抽出一个球，发现是红球，那么这个球来自罐子A的概率是多少？
根据贝叶斯定理，我们可以这样计算：
P(A)表示这个球来自罐子A的先验概率，由于没有任何其他信息，我们可以假设它是1/2，即两个罐子的概率相等。
P(B)表示这个球是红球的概率，由于两个罐子里共有11个红球和9个白球，所以P(B) = 11/20。
P(B|A)表示在这个球来自罐子A的条件下，这个球是红球的概率，由于罐子A里有3个红球和7个白球，所以P(B|A) = 3/10。
P(A|B)表示在这个球是红球的条件下，这个球来自罐子A的概率，也就是后验概率，根据贝叶斯定理，P(A|B) = P(A)P(B|A)/P(B


如今的社会，越成功的人越是贝叶斯主义者，要及时根据新的反馈调整对某一件事的信念，即使你当初是多么的信任。



在后来的机器学习和深度学习中有哪些地方存在频率主义和贝叶斯主义的思想？

频率主义是一种统计学派，认为概率是事件发生的长期频率，而不是对事件的信念或知识的度量。频率主义的方法通常使用极大似然估计（MLE）来估计模型参数，即选择使观测数据出现的概率最大的参数值。频率主义的优点是可以保证估计量的一致性和渐近正态性，但缺点是不能直接处理参数的不确定性，而需要使用置信区间或假设检验等方法。
贝叶斯主义是另一种统计学派，认为概率是对事件的信念或知识的度量，而不是事件发生的长期频率。贝叶斯主义的方法通常使用贝叶斯定理来更新模型参数的后验分布，即在观测到数据后，根据参数的先验分布和数据的似然函数，计算参数的后验分布。贝叶斯主义的优点是可以直接处理参数的不确定性，而不需要使用置信区间或假设检验等方法，但缺点是计算后验分布可能很困难，而且需要选择合适的先验分布。


在机器学习和深度学习中，频率主义和贝叶斯主义的思想都有一定的影响和应用。例如，神经网络是一种常用的机器学习和深度学习模型，它可以使用频率主义的方法，如反向传播和梯度下降，来优化网络的权重和偏差，也可以使用贝叶斯主义的方法，如变分推断和马尔可夫链蒙特卡罗，来估计网络的权重和偏差的后验分布。频率主义的方法通常更快速和简单，但贝叶斯主义的方法可以提供更多的不确定性信息和正则化效果。
另一个例子是支持向量机（SVM），它是一种常用的机器学习模型，它可以使用频率主义的方法，如核技巧和拉格朗日对偶，来求解最大间隔分类器，也可以使用贝叶斯主义的方法，如高斯过程和贝叶斯优化，来求解最大后验分类器。频率主义的方法通常更适合高维和大规模的数据，但贝叶斯主义的方法可以提供更多的灵活性和鲁棒性。
总之，在机器学习和深度学习中，频率主义和贝叶斯主义的思想都有一定的影响和应用，它们各有优缺点，适用于不同的场景和问题。如果您想了解更多的信息，您可以查看以下的网页和图片搜索结果。希望这些内容对您有所帮助。


为什么说极大似然估计是频率主义方法？只要求解的参数是固定值的方法都是频率主义方法？

极大似然估计是一种频率主义的方法，因为它基于以下的假设：
参数是未知的固定值，而不是随机变量。
样本是从参数固定的分布中独立同分布地抽取的。
似然函数是样本出现的概率，而不是参数的信念或知识的度量。
参数的估计是使得似然函数达到最大值的值，即最有可能导致观测数据的值。
这些假设都符合频率主义的观点，即概率是事件发生的长期频率，而不是对事件的信念或知识的度量。频率主义的方法通常使用极大似然估计或置信区间等方法来估计参数，而不使用贝叶斯定理或先验分布等方法。
并不是所有求解的参数是固定值的方法都是频率主义的方法。有些方法是基于最小化误差或损失函数的，而不是最大化似然函数的。例如，最小二乘法是一种常用的参数估计方法，它是基于最小化残差平方和的，而不是最大化似然函数的。最小二乘法可以看作是一种特殊的极大似然估计，当假设误差服从正态分布时，它们是等价的。但是，最小二乘法并不一定是频率主义的方法，它也可以从贝叶斯的角度来理解，当假设参数服从均匀分布或正态分布时，它们是等价的。






概率与信念的区别

概率与信念的区别是一个涉及到概率论的哲学问题，一般被称为概率的解释（interpretations of probability）。不同的概率的解释会给概率赋予不同的含义和本质，主要可以分为两大类：客观概率和主观概率。
客观概率是指概率是客观世界的一种特性，与人的知识或信念无关。客观概率的主要流派有频率主义和倾向主义。频率主义认为概率是事件发生的长期频率，例如掷硬币正面朝上的概率是50%，意味着如果掷硬币的次数趋近于无穷，正面朝上的比例会趋近于50%。倾向主义认为概率是物理系统具有的一种倾向或倾斜，例如掷硬币正面朝上的概率是50%，意味着硬币本身有一种倾向使得正面朝上的可能性和反面朝上的可能性相等。
主观概率是指概率是人对事件的信念或知识的度量，与客观世界无关。主观概率的主要流派有贝叶斯主义和逻辑主义。贝叶斯主义认为概率是人对事件的主观信念的强度，例如掷硬币正面朝上的概率是50%，意味着人对正面朝上这一命题的信念和对反面朝上这一命题的信念相等。逻辑主义认为概率是人对事件的逻辑推理的结果，例如掷硬币正面朝上的概率是50%，意味着在给定的证据和理由下，正面朝上这一命题的真值和反面朝上这一命题的真值相等。




大数定律

大数定律是概率论中的一个重要定律，它描述了随机事件发生的频率和概率之间的关系。
大数定律的基本思想是：当随机事件发生的次数足够多时，随机事件发生的频率趋近于预期的概率。也就是说，当样本数量越多，其平均值就越接近于期望值。例如，如果我们掷一枚均匀的硬币，每次出现正面或反面的概率都是1/2，但是如果我们只掷几次，可能会出现正面或反面的次数不相等的情况。但是如果我们掷很多次，比如1000次，那么正面和反面出现的次数就会非常接近于500，也就是说，出现正面或反面的频率就会非常接近于1/2。这就是大数定律的一个例子。
大数定律有不同的表达形式，其中最常见的有两种：弱大数定律和强大数定律。它们的区别在于所断言的收敛的方式。弱大数定律也称为辛钦定理，它表明，样本均值依概率收敛于期望值。也就是说，对于任意给定的正数ε，样本均值与期望值的差的绝对值大于ε的概率趋近于零，当样本数量趋近于无穷时。强大数定律也称为伯努利定理，它表明，样本均值几乎必然收敛于期望值。也就是说，样本均值与期望值的差的绝对值大于ε的概率等于零，对于任意给定的正数ε，当样本数量趋近于无穷时。
大数定律的意义在于，它为我们提供了一种用频率来估计概率的方法，也为我们提供了一种用样本均值来估计期望值的方法。这些方法在统计学和实际问题中有着广泛的应用。例如，我们可以用大数定律来估计一个国家的人口平均年龄，或者一个企业的产品平均质量，或者一个游戏的中奖概率等。
希望这些内容对您有所帮助。


中心极限定理



熵增定律





参考
[1]https://www.datasciencecentral.com/understanding-the-applications-of-probability-in-machine-learning/

[2]https://www.adrian.idv.hk/2018-03-19-distribution/


