---
published: false
layout: post
title: "预告"
categories: 我的AI新书
date: 2023-09-21 00:00:00 +0800
excerpt: "预告"
---


深层循环神经网络



回顾：循环神经网络


如何将循环神经网络变深，以获得更多的非线性性？

通过添加多个隐藏层的方式来实现（和 MLP 没有本质区别），每个隐藏状态都连续地传递到当前层的下一个时间步和下一层的当前时间步


类似于多层感知机，隐藏层数目和隐藏单元数目都是超参数（它们是可以进行调整的）
使用门控循环单元或长短期记忆网络的隐状态替代上图中深度循环神经网络中的隐状态计算，就能够很容易地得到深度门控循环神经网络或长短期记忆神经网络






总结

1、深度循环神经网络使用多个隐藏层来获得更多的非线性性

GRU、RNN、LSTM 在结构上都是相同的，只是隐状态 H 的计算方式有区别，所以它们加深神经网络的原理都是相同的
2、在深度循环神经网络中，隐状态的信息被传递到当前层的下一时间步和下一层的当前时间步

3、存在许多不同风格的深度循环神经网络，如长短期记忆网络、门控循环单元或经典循环神经网络

4、深度循环神经网络需要大量的调参（如学习率和修剪）来确保合适的收敛，模型的初始化也需要谨慎









Q&A

1、单层的输出是 [y1,y2,...] 和 ht ，那多层的就是把 [y1,y2,...] 作为输入吗，那每层不就都有一个 ht 输出吗，直接就不用了吗？
﻿
QA P3 - 00:00
﻿


2、老师请问目前 NLP 领域哪些方向最容易找到工作，我有一个疑惑，例如文本翻译这个工作除了学术研究，是不是在工作中几乎不可能自己去实现？
﻿
QA P3 - 02:23
﻿


3、能将一下 BPTT 的原理与实现吗？
﻿
QA P3 - 04:52
﻿


4、深层 RNN ，是不是每层都需要一个初始的 hidden state？
﻿
QA P3 - 05:21
﻿


5、可不可以手动实现 hidden_size 不一样的多层 RNN ？
﻿
QA P3 - 05:42
﻿


6、老师，nn.LSTM GRU RNN 这些，图里面的o好像是有的。您说的那个不带 classifier 的意思是不是在这个 o 的基础上，根据一对一、多对一等，再加一个 MLP 映射 o 到 y ？
﻿
QA P3 - 06:19
﻿








----end----

其他参考：

1、《动手学深度学习》，PPT，https://courses.d2l.ai/zh-v2/assets/pdfs/part-3_6.pdf

2、《动手学深度学习》，教材，https://zh-v2.d2l.ai/chapter_recurrent-modern/deep-rnn.html 作者：如果我是泡橘子 https://www.bilibili.com/read/cv18357204/?from=readlist&jump_opus=1 出处：bilibili