---
published: false
layout: post
title: "预告"
categories: 我的AI新书
date: 2023-09-21 00:00:00 +0800
excerpt: "预告"
---


注意力分数




α（x,xi）：注意力权重（权重一般是一组大于等于零，相加和为 1 的数）
注意力分数：高斯核的指数部分（相当于是注意力权重归一化之前的版本）
上图所展示的是：假设已知一些 key-value 对和一个 query，首先将 query 和每一个 key 通过注意力分数函数 a 和 softmax 运算得到注意力权重（与 key 对应的值的概率分布），将这些注意力权重再与已知的 value 进行加权求和，最终就得到了输出 




如何将 key 和 value 拓展到更高的维度




假设 query 是一个长为 q 的向量，ki 是长为 k 的向量，vi 是长为 v 的向量（这里 key 和 value 的长度可以不同）
其中 query 和 ki 的注意力权重（标量）是通过注意力评分函数 a 将两个向量映射成标量，再经过 softmax 运算得到的 




掩蔽 softmax 操作（masked softmax operation）



softmax 操作用于输出一个概率分布作为注意力权重，但是在某些情况下，并非所有的值都应该被纳入到注意力汇聚中

在处理文本数据集的时候，为了提高计算效率，可能会采用填充的方式使每个文本序列具有相同的长度，便于以相同形状的小批量进行加载，因此可能会存在一些文本序列被填充了没有意义的特殊词源（比如“<pad>”词元）
因此，为了仅仅将有意义的词元作为值来获取注意力汇聚，可以指定一个有效序列长度（即词元的个数），任何超出有效长度的位置都被掩蔽并置于 0，便于在计算 softmax 的时候过滤掉超出指定范围的位置，这也就是掩蔽 softmax 操作






注意力分数函数 α 的设计



1、加性注意力（Additive attention）



当 query 和 key 是不同长度的矢量时，可以使用加性注意力作为注意力分数
可学参数（ 3 个）：

h：超参数
Wk：key 从 k map 到 h（将 key 向量的长度从 k 转化为 h）
Wq：query 从q map 到 h（将 query 向量的长度从 q 转化为 h）
Wk、Wq 都是矩阵，v 是向量
tanh：激活函数
上式中，k 是长为 k 的向量，q 是长为 q 的向量
等价于将 query 和 key 合并起来变成一个长度为 k+q 的向量，然后将其输入到一个隐藏层大小为 h （ h 是一个超参数），输出大小为 1 的但隐藏层的 MLP（没有偏置项），最终得到输出
优点是 key、value 向量可以是任意的长度，可以不同




2、缩放点积注意力（Scaled Dot-Product Attention）



使用点积可以得到计算效率更高i的评分函数，但是点积操作要求 query 和 key 具有相同的长度


针对 key 和 query 长度相同的情况


假设 query 和 key 的所有元素都是独立的随机变量，并且都满足零均值和单位方差，那么两个向量的点积的均值为 0 ，方差为 d
这里不需要学习任何东西，直接利用 <q,ki> 将 q 和 ki 做内积然后除以根号 d （除以根号 d 的目的是为了降低对 ki 的长度的敏感度，使得无论向量的长度如何，点积的方差在不考虑向量长度的情况下仍然是 1 ）








总结



1、注意力分数是 query 和 key 的相似度（没有经过 normalize ），注意力权重是注意力分数的 softmax 结果（ 0 到 1 之间的数）

2、两种常见的注意力分数的计算

有参数的版本：将 query 和 key 合并起来进入一个单隐藏层单输出的 MLP（在 query 和 key 向量长度不同的情况下）
无参数的版本：直接将 query 和 key 做内积（在 query 和 key 向量长度一定的情况下），效率更高








Q&A



1、核回归中算法后面的那个红色的热图是什么意思？﻿
QA P3 - 00:00
﻿


2、query、key、value 的主要区别是什么？﻿
QA P3 - 00:51
﻿


3、把之前的 x 和 x_i 换成了 q 和 k_i 吗，这两个值是哪里的呢？﻿
QA P3 - 01:09
﻿


4、为什么注意力分数可以理解为二者的相似度呢？﻿
QA P3 - 01:34
﻿


5、masked_softmax() 就相当于把注意力分数低的过滤掉吗？那多低算低呢，这个阈值怎么设置的呢？﻿
QA P3 - 03:13
﻿


6、看懂了数学逻辑，还是不太理解 k、q、v 的具体含义 ﻿
QA P3 - 04:28
﻿








----end----

其他参考

1、《动手学深度学习》，https://zh-v2.d2l.ai/chapter_attention-mechanisms/attention-scoring-functions.html 作者：如果我是泡橘子 https://www.bilibili.com/read/cv15934199/?from=readlist&jump_opus=1 出处：bilibili