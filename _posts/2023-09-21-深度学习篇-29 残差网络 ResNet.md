---
published: false
layout: post
title: "预告"
categories: 我的AI新书
date: 2023-09-21 00:00:00 +0800
excerpt: "预告"
---


残差网络



现在经常使用的网络之一




问题：随着神经网络的不断加深，一定会带来好处吗？



不一定。

蓝色五角星表示最优值
标有Fi的闭合区域表示函数，闭合区域的面积代表函数的复杂程度，在这个区域中能够找到一个最优的模型（可以用区域中的一个点来表示，该点到最优值的距离可以用来衡量模型的好坏）
从上图中可以看出，随着函数的复杂度的不断增加，虽然函数的区域面积增大了，但是在该区域中所能找到的最优模型（该区域内的某一点）离最优值的距离可能会越来越远（也就是模型所在的区域随着函数复杂度的增加，逐渐偏离了原来的区域，离最优值越来越远）（非嵌套函数（non-nested function））
解决上述问题（模型走偏）的方法：每一次增加函数复杂度之后函数所覆盖的区域会包含原来函数所在的区域（嵌套函数（nested function）），只有当较复杂的函数类包含复杂度较小的函数类时，才能确保提高它的性能，如下图所示

也就是说，增加函数的复杂度只会使函数所覆盖的区域在原有的基础上进行扩充，而不会偏离原本存在的区域
对于深度神经网络，如果能将新添加的层训练成恒等映射（identify function）f(x) = x，新模型和原模型将同样有效；同时，由于新模型可能得出更优的解来拟合训练数据集，因此添加层似乎更容易降低训练误差


针对这个问题，何恺明等人提出了残差网络ResNet，它拿到了2015年ImageNet图像识别挑战赛的冠军，并深刻影响了后来深度神经网络的设计









核心思想



残差网络的核心思想是：每个附加层都应该更容易地包含原始函数作为其元素之一



由此，残差块（residual blocks）诞生了









残差块






之前增加模型深度的方法都是层层堆叠的方法，ResNet的思想是在堆叠层数的同时不会增加模型的复杂度
上图中左侧表示一个正常块，右侧表示一个残差块
x：原始输入
f(x)：理想映射（也是激活函数的输入）
对于正常块中来说，虚线框中的部分需要直接拟合出理想映射 f(x)；而对于残差块来说，同样的虚线框中的部分需要拟合出残差映射 f(x) - x
残差映射在现实中往往更容易优化
如果以恒等映射 f(x) = x 作为所想要学出的理想映射 f(x)，则只需要将残差块中虚线框内加权运算的权重和偏置参数设置为 0，f(x) 就变成恒等映射了
在实际中，当理想映射 f(x) 极接近于恒等映射时，残差映射易于捕捉恒等映射的细微波动
在残差块中，输入可以通过跨层数据线路更快地向前传播 
﻿
ResNet P1 - 04:04
﻿




左边是ResNet的第一种实现（不包含1 * 1卷积层的残差块），它直接将输入加在了叠加层的输出上面
右边是ResNet的第二种实现（包含1 * 1卷积层的残差块），它先对输入进行了1 * 1的卷积变换通道（改变范围），再加入到叠加层的输出上面
ResNet沿用了VGG完整的3 * 3卷积层设计
残差块中首先有2个相同输出通道数的3 * 3卷积层，每个卷积层后面接一个批量归一化层和ReLu激活函数；通过跨层数据通路，跳过残差块中的两个卷积运算，将输入直接加在最后的ReLu激活函数前（这种设计要求2个卷积层的输出与输入形状一样，这样才能使第二个卷积层的输出（也就是第二个激活函数的输入）和原始的输入形状相同，才能进行相加）
如果想要改变通道数，就需要引入一个额外的1 * 1的卷积层来将输入变换成需要的形状后再做相加运算（如上图中右侧含1 * 1卷积层的残差块）








不同的残差块












ResNet架构



原本的ResNet和VGG类似，ResNet块有两种：


第一种是高宽减半的ResNet块。第一个卷积层的步幅等于2，使得高宽减半，通道数翻倍（如上图下半部分所示）
第二种是高宽不减半的RexNet块，如上图上半部分所示，重复多次，所有卷积层的步幅等于1


通过ResNet块数量和通道数量的不同，可以得到不同的ResNet架构，ResNet-18架构如下图所示


ResNet架构类似于VGG和GoogLeNet的总体架构，但是替换成了ResNet块（ResNet块的每个卷积层后增加了批量归一化层）
ResNet的前两层和GoogLeNet中的一样，也分成了5个stage：在输出通道数为64、步幅为2的7 * 7卷积层后，接步幅为2的3 * 3的最大汇聚层
GoogLeNet在后面接了4由Inception块组成的模块；ResNet使用了4个由残差块组成的模块，每个模块使用若干个同样输出通道数的残差块，第一个模块的通道数同输入通道数一致；由于之前已经使用了步幅为2的最大汇聚层，所以无需减小高和宽；之后每个模块在第一个残差块里将上一个模块的通道数翻倍，并将高和宽减半
通过配置不同的通道数和模块中的残差块数可以得到不同的ResNet模型：ResNet-18：每个模块都有4个卷积层（不包含恒等映射的1 * 1卷积层），再加上第一个7 * 7卷积层和最后一个全连接层，一共有18层；还有更深的152层的ResNet-152








ResNet 152




图中所示的是ResNet-152(经过两三次改良之后的版本)在ImageNet数据集上分类任务的精度
模型的层数越少通常速度越快，精度越低，层数越多，精度越低
ResNet 152是一个经常用来刷分的模型，在实际中使用的比较少








总结



残差块使得很深的网络更加容易训练（不管网络有多深，因为有跨层数据通路连接的存在，使得始终能够包含小的网络，因为跳转连接的存在，所以会先将下层的小型网络训练好再去训练更深层次的网络），甚至可以训练一千层的网络（只要内存足够，优化算法就能够实现）
学习嵌套函数是神经网络的理想情况，在深层神经网络中，学习另一层作为恒等映射比较容易
残差映射可以更容易地学习同一函数，例如将权重层中的参数近似为零
利用残差块可以训练出一个有效的深层神经网络：输入可以通过层间的残余连接更快地向前传播
残差网络对随后的深层神经网络的设计产生了深远影响，无论是卷积类网络还是全连接类网络，几乎现在所有的网络都会用到，因为只有这样才能够让网络搭建的更深








Q&A



1、为什么 LeNet batch size 大于1000收敛会出现问题？会有什么样的问题？nan 还是？﻿
QA P3 - 00:01
﻿


2、f(x) = x + g(x)，这样就能保证至少不会变坏吗？如果 g(x) 不是变好，也不是什么都不干，而是变坏了呢？﻿
QA P3 - 00:32
﻿


3、cos 学习率会比 step 或者固定学习率好吗？﻿
QA P3 - 01:52
﻿


4、残差这个概念体现在什么地方？就是因为 f(x) = x + g(x)，所以 g(x) 可以视为 f(x) 的残差？﻿
QA P3 - 03:11
﻿


5、ResNet 实现里最后 *ResNet block 里的*号是什么意思﻿
QA P3 - 04:56
﻿


6、请问在__init__里为什么定义两个bn，这两个bn的参数一样？为什么定义了self.relu在forward里面没有用？麻烦老师可以解释解释nn.ReLu（inplace=True）中的inplace这个参数吗？﻿
QA P3 - 05:17
﻿


7、这里输入尺寸96* 96、224 * 224 是怎么确定的？也是调参？﻿
QA P3 - 06:14
﻿


8、训练acc是不是正常训练时就是会稍微大于测试acc？这是不是意味着永远达不到100%识别？﻿
QA P3 - 06:43
﻿


9、组队的话，一组是每个人一本书吗？﻿
QA P3 - 08:04
﻿


10、这次的数据集有没有错误的 label ？﻿
QA P3 - 08:44
﻿








----end----

其它参考：

1、《动手学深度学习》，课程安排，https://courses.d2l.ai/zh-v2/assets/pdfs/part-1_13.pdf

2、GluonCV Model Zoo，https://cv.gluon.ai/model_zoo/classification.html

3、《动手学深度学习》，https://zh-v2.d2l.ai/chapter_convolutional-modern/resnet.html 作者：如果我是泡橘子 https://www.bilibili.com/read/cv15415155/?from=readlist&jump_opus=1 出处：bilibili



除此之外，在图像生成领域中也利用了残差思想