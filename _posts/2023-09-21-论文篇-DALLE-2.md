---
published: false
layout: post
title: "DALLE.2论文精读"
categories: 我的AI新书
date: 2023-09-21 00:00:00 +0800
excerpt: "DALLE.2论文精读"
---


图像生成与文本生成有很多不同之处，文本生成依靠GPT技术得到很好的解决，图像生成需要不一样的技术方案。openAI发布的DALLE2结合了扩散模型和CLIP模型等多种新技术，在图像生成的真实性、多样性、创意性等多方面都达到了很高的水准。

2021年1月，OpenAI发布初代AI画图师DALL-E[1]。一年之后，2022年4月，OpenAI发布升级版DALL-E2[2]。继GPT后再一次引爆AIGC。

DALLE这一名字来源于电影《机器人总动员》中的机器人WALL-E和西班牙超现实主义画家席尔瓦多-达利(DALI)。

DALLE沿用了OpenAI擅长的基于GPT的技术路线（GPT+VQ-VAE）。在此之前，OpenAI已于2020年6月发布了Image-GPT[3]，在图像生成大模型上小试牛刀。

但是DALEE2采用了不同的技术方案：扩散模型。其效果比DALLE提升很多。

本文主要介绍DALLE2。DALLE2原论文对模型介绍非常简略，没有基础基本看不懂。网上对扩散模型的介绍汗牛充栋，但是质量参差不齐，很多在照抄公式，不清不楚。

我在6月份之前对扩散模型和图像生成的理解都是白纸一张，看完几个视频后终于有所认识。遂作此记录，希望对大家有所帮助。

0、主要问题与核心思想
彼时，GPT已经在文本生成领域取得了巨大的成功，但是图像生成任务依然没有很好地解决。

图像生成和文本生成，输入都是文本，只不过前者输出图像，后者输出文本。

那么，图像生成与文本生成有什么本质区别？

首先，文本生成是“一对一”的任务，而图像生成是“一对多”的任务。

文本生成中的主流任务，机器翻译，有比较确定性的正确答案，一个输入句子，对应一个输出句子。但是，图像生成任务没有标准答案。输入“一只奔跑的小狗”， 输出可以多种多样，既可以是往左跑的小狗，也可以是往右跑的小狗。图像生成时机器需要大量“脑补”。

“一图胜千言”，图像的信息量远远大于文本，同一张图片可以有不同的文本描述，同一张图片可以包含多个视觉概念，生成任务需要把它们有机地融合在一起。

一般的机器学习任务，一个输入有一个确定性的标签（label）。图像生成任务不一样，对一个输入，其输出是一个分布。

Image-GPT这种直接拿GPT做图像生成效果不好，因为每个pixel单独生成，无法表达分布的约束。同样是画“一只奔跑的小狗”，前一个pixel可能画往左跑，后一个pixel可能画往右跑，单独看每个pixel都没有问题，但是放在一起就出问题了。

再者，文本生成任务的基本概念单元——token——是有限的、离散的、可枚举的；而图片任务不一样，它是无限的、连续的、不可穷举的。

图像的像素表征有很多冗余。图像概念更像是在一个连续、渐变空间。例如，同一张图片，可以稍微做一点变换，生成一个大体相似，但有细微差别的图片。

凡此种种，可以通过扩散模型比较好地解决。

扩散模型在文本之外引入一个额外的输入：随机噪声分布。使得输入变成了一个分布，输出也是一个分布。有了分布就有了约束，使得输出的视觉概念满足概率约束。

扩散模型的输出不是一步到位，而是多步到位。中间过程生成了大体相似却又有所差异的图片。

1、图像生成任务的历史演进
图像生成是一个很活跃的研究领域，诞生过很多技术。在介绍DALLE2论文之前，先简单介绍主流技术。

1.1、 GAN
GAN的核心思想是“左右手互搏”，有两个网络，一个生成器G，一个判别器D。它刚提出时，被认为是深度学习领域当时最具创意的思想。

生成器的输入是随机噪声，输出是图像x’。判别器的输入是x‘和真实图像x，输出是二分类，表示x’是否是真实图片。生成器的目标是以假乱真，糊弄判别器。而判别器的目标是练就一双火眼金睛，识别伪造的图片。训练过程G和D不断提升，最后G生成非常逼真的图片。

GAN的目标函数就是为了以假乱真，所以GAN生成的图片保真度非常高。即便人眼也很难区分真假。使用GAN的DeepFake曾经十分火爆。

经过多年的优化，GAN现在很好用，但是它还有一些缺点。首先，训练不稳定。因为它要训练两个网络，不太好平衡。其次，它生成过程的随机性来自初始的随机噪声。这导致GAN生成的图片缺乏多样性和创造性。再者，GAN不是一个概率模型。它的生成都是隐式的，通过一个网络完成的。我们没法知道它具体做了什么，遵循什么分布。GAN在数学上不如后期的VAE、diffusion模型优美。

1.2、 AE大家族
1.2.1、 AE
原始图片x，经过编码器E得到中间向量z，再经过解码器D，输出图片x‘。z的维度通常比原始图片小很多，所以又被称为bottleneck。

训练目标：x’尽量逼近x。即重构原始图片。

图片

图-1

1.2.2、 DAE：denoising AE
DAE与AE只有一个差别：输入的原始图片x先加噪变成，再接入后续流程。训练目标仍然是使输出x‘逼近原始图片x而非。

图片

图-2

事实证明，加噪声很有用。它使得模型更稳健，鲁棒性更强，不容易过拟合。

究其原因，可能在于，图片信息冗余很大。即使原始图片被污染了，模型仍然能够抓住它的本质，把它重构出来。这一思想和扩散模型以及何恺明的MAE（Mask auto-encoder）比较相似。MAE在训练的时候mask掉75%的区域，仍能把图片重构出来。

无论是AE、DAE还是MAE，都是为了学习中间的bottleneck特征。然后再拿bottleneck特征做分类等任务。它并不是为了做生成式任务。原因是它学到的是固定的特征向量，而不是一个概率分布，不能用来做采样。于是，顺着这条思路衍生出来VAE。

1.2.3、 VAE
VAE学习概率分布，它先假设这个分布符合高斯分布。高斯分布可由均值和方差唯一确定，于是VAE的encoder部分的学习目标简化成学习高斯分布的均值和方差。

具体方法如下：

原始图片x encode之后经过FC层预测得到均值 、方差。
从该高斯分布采样得到 。
通过decoder生成 x‘。
整个过程从数学上看比较优雅。第一步，从x得到z，可以写作，是（z的）后验概率。中间的是先验概率prior。后面一步可以写作，是likelihood（条件概率）。
图片

图-3

VAE提出之后，有很多基于它的工作，包括VQ-VAE、VQ-VAE2等。DALLE-1就是在VQ-VAE的基础上做的。

1.2.4、 VQ-VAE
VQ的含义是Vector Quantised，就是把VAE做量化。

虽然现实世界的很多信号，例如语音、图像等都是连续信号，但是我们在计算机处理它们时大多把它们处理成离散信号。把针对连续信号的回归任务转化成针对离散信号的分类任务。

在VQ-VAE中，不是直接学习中间变量的分布，而是用一个codebook代替它。codebook的大小是K * D。K一般是8192，D一般是512或者768。codebook存储的向量可以理解为聚类的中心，也可以看作是embedding。即8192个长度为D的embedding向量。

x encode之后得到特征图f，f中的每一维向量都从codebook中找一个离它最近的向量D替换，这样得到量化之后的特征向量 。和原始特征图f维度一样，语义一样，只不过它的元素取值只能从codebook中来，相当于缩小了空间。

图片

图-4

个人觉得，这个过程可以看作二维连续数据的分桶离散化操作。

接下来，继续经过解码器D生成图片x‘。目标函数仍然是希望x’尽量和x保持一致。

VQ-VAE的codebook虽然需要学习，但它是固定的，不是概率分布，没法随机采样。准确地说，VQ-VAE不像是VAE，更像是AE。它学到的codebook和中间特征图，不是用来做图像生成，而是做分类等其它图像任务。

如何用VQ-VAE做图像生成呢？需要再给它训练一个prior网络。在VQ-VAE原论文中，通过训练一个pixelCNN当作prior网络，来做图像生成。

pixelCNN其实是一个auto regressive的模型，而openAI的看家本领GPT也是auto regressive模型，于是他们很快想到把pixelCNN替换成GPT。这便有了DALLE-1。

1.2.5、 DALL-E
DALLE的模型十分简洁。输入文本通过编码（BPE）得到文本向量（256维），图像通过事先训练好的VQVAE编码得到图片向量（32 * 32 = 1024维）。二者concat到一起得到一个序列（1280 token）。有了输入序列接下来就是很常规的操作，接入GPT，并通过mask等方式训练。

图片

图-5

推理的时候，输入文本，得到文本序列，输入GPT，用自回归的方式生成图像token序列，得到图片。

DALLE输出得到多张图片，将图片的CLIP embedding与输入文本的CLIP embedding做对比，找到最相似的图片作为最终输出。

AE系列之后是扩散模型，由于扩散模型太重要了，我们单起一章。

2、扩散模型
2.1、”扩散“二字的含义
”扩散“二字十分唬人，我先遵从标准的解读作一介绍。

它来自物理学中的扩散过程。将一滴墨汁滴到一瓶清水中，它会逐渐扩散开来，最后墨汁和清水浑然一体，达到”各向同性的正态分布“。

据说，计算机科学家脑洞大开，把它和图像生成联系起来。

对一张原始图片，逐步加噪声，它最终会变成面目全非的白噪声。这个过程比作上述扩散过程。称作前向扩散，forward diffusion。

生成图片可以看作，输入高斯白噪声，然后一步一步地对它去噪，最后生成清晰的图片。这一过程称作反向扩散，reverse diffusion，是前向扩散的逆过程。

个人觉得，上述比喻虽然清新，但是牵强，无法提供理论上的说服力。毕竟，物理学中的扩散过程可逆吗？如果可逆，它和图像生成的反向扩散遵循同样的机理吗？

我怀疑，科学家是先有了图像生成这一套加噪/去噪的操作，再去给它取一个好名字时想到了扩散过程。（当然也可能是我孤陋寡闻了。）我发现，抛开物理学的扩散过程，反而更容易理解扩散模型。

其实，理解扩散模型的窗户纸一捅就破。加噪声是为了构造自监督学习的label。它和Bert及GPT通过mask或者预测下一个单词等方式构造label有异曲同工之妙。有了稳健的自监督label，我们才能构造模型消费取之不尽、用之不竭的图片、文本等数据集，才能实现“大力出奇迹”。

源源不断的大数据集、参数超多且容量巨大的模型结构，以及稳健的自监督label信号（及相应的训练方法和目标函数）是我认为驱动生成式大模型成功的几大底层要素。

图像生成及推理过程，就是逐步去噪。只要能够预测噪声就能去噪。前面提到，前向过程中，噪声是模型的label，模型的目的就是预测噪声。模型推理时不是一步到位，而是步步为营N次到位。输入是白噪声（或者prior网络得到的分布，例如DALLE2）以及步数t，经过模型推理得到预测的（前一步添加的）噪声，从输入中减去预测噪声得到一个稍微清晰一点的图片。重复这一过程N次，最终得到输出图片。

由于推理时需要串行迭代N次，速度很慢，后续有很多相关的优化。

这里我有几个疑问：

为何每一步的噪声预测网络参数可以共享？
为何不同图片的噪声预测网络参数可以共享？
噪声预测网络本质上学习并且存储的是什么？
GPT中不同序列能共享相同的网络参数是假设输入的文本数据集中同一个token不管在哪个句子出现，它遵循相同的生成概率。模型学习并存储（所有）token以及token组合（句子）的生成概率。

上述扩散模型的几个疑问我还未找到答案，只能说实验表明it just works（现阶段的AI还是一门实验学科）。

2.2、扩散模型的结构
对于扩散过程中的噪声预测网络，openAI选用了非常常规的U-Net。U-Net最早是2015年提出，它是一种CNN网络。输入图片x经过编码器逐步下采样，压缩到一个中间结果，然后经过解码器逐步上采样恢复到和输入同样维度的x‘。为了恢复得更好，encoder和decoder间有一些skip connection。U-Net输入和输出维度一致，刚好非常适合diffusion的场景。

2.3、扩散模型的演进
2.3.1、 DDPM
扩散模型早在2015年就提出来了，但是它真正产生好的效果走入人们的视野是2020年DDPM[4]论文之后。

DDPM由Berkeley的三位大佬提出，它算是扩散模型在图像生成领域的开山之作。它的贡献主要有两个：

首先，之前人们在扩散过程中想直接实现X(t)到X(t-1)即图像到图像的转换。DDPM认为直接预测图像比较困难，它转而预测噪声，类似ResNet的思想。

其次，如果要预测正态分布（噪声），只需要学习它的均值和方差。DDPM发现甚至连方差都不用学习（设置成一个常数），只学习均值就能取得很好的效果，再次降低模型优化的难度。

也可以从VAE的角度理解DDPM，只不过有以下差别：

第一，DDPM的编码器是前向扩散过程，不需要学习。

第二，扩散过程每一步中间结果的维度都和输入一样，而VAE的中间结果维度比输入小。

第三，扩散模型有步数，time embedding的概念。每一步的U-Net模型共享参数。

2.3.2、 improved DDPM
DDPM证明扩散模型确实有效，openAI的几位研究者下场继续改进，几个月后便有了improved DDPM[5]。它的改进主要包括：

第一，它没有采用常数方差，而是和均值一样，通过模型学习方差。

第二，改变增加噪声的schedule，从线性schedule变成余弦schedule，和学习率的schedule类似。

第三，尝试大模型，发现扩散模型scale非常好，增加模型参数量可以提升效果，可以大力出奇迹。

2.3.3、 Diffusion Beats GAN
有了iDDPM之后，openAI乘胜追击，几个月后发表了Diffusion Models Beat GAN[6]。

在这篇文章里，openAI祭出传统艺能，将模型加深加宽，增加注意力head数，把模型弄的又大又复杂。还提出了一个新的思想，classifier guidance，引导模型做采样和生成。文章不仅提高了图片的质量，还大大提高了推理速度，只需25步就能完成反向扩散生成图片。

文章发现，单看diffusion生成的图片质量很高，但是评估指标IS（Inception score）和FID 等分数仍然比不过GAN。为了提升评估指标，加快模型推理，文章提出了guidance方法。

事先训练好一个图像分类器，反向扩散过程中生成的中间图像，经过分类器得到一个分类分数，然后与分类标签计算交叉熵loss（备注，这里的分类标签如何知道？），根据loss计算梯度g，这个梯度可以用来引导模型采样和图像生成。guidance的作用是期望生成的图片含有某类物体，而且图片的颜色形状纹理等各种细节都需要尽量跟真实的物体匹配上，从而提升图片逼真度。

使用guidance技术，大幅度提高了IS、FID等评估指标，但是为图片真实度牺牲了一部分多样性。

classifer guidance之后又涌现了很多其它的guidance方法。它们本质上是在采样时，输入Xt和时间步t以外又引入了其它的条件y。例如引入CLIP、image、text等各种guidance条件。

但是这些方法都需要另一个模型来做引导，要么用预训练好的模型，要么自己训练一个模型，不仅成本比较高，而且训练过程不可控。

于是openAI又提出了classifer-free guidance，诞生了GLIDE[7]模型。

2.3.4 GLIDE
GLIDE在引导时不需要额外训练一个classifier。它用什么作为引导信号呢？

它在训练阶段，得到两个输出。一个是有条件的输出，一个是无条件的输出。举例来说，如果训练是图像-文本对，一个输出是有文本作为条件的输出Xy，一个是没有文本作为条件，即将文本随机置成空序列，得到的输出X，保留X与Xy之间的差距作为引导信号。在反向扩散时，先得到无条件的输出X，再叠加引导信号，可以得到有条件的输出。

这种方法的缺陷是训练开销比较大，需要产生两个输出。但是它效果很好，一经提出就被广泛采纳。

在吸收前面多种技巧之后，GLIDE终于利用扩散模型取得了成功。它只用了3.5B的参数就取得了比之前12B参数的DALLE-1更好的效果。于是，openAI放弃了DALLE-1采用的VQ-VAE的技术路线，全面拥抱扩散模型，诞生了DALLE2。

2.3.5、 DALLE2
DALLE2可以看作是GLIDE+CLIP，它在GLIDE基础上进一步改进，引入了prior网络，层级式结构，CLIP embedding等优化。后文详细介绍。

2.4、扩散模型的实现
DDPM论文提到的扩散模型算法如图所示。

图片

图-6

有几个值得注意的地方：

第一，训练时并没有迭代T步，即没有扩散T步。X(t)一次得到。2.5节解释原因。

第二，推理时，X(t)去噪之后，又叠加了一个高斯噪声扰动项z。这个小小扰动是为了增加noise，增加不确定性，实验证明它非常有效。正如在文本生成时选取概率最大的token输出效果不好，需要采样，增加随机性，增加多样性。再比如语音合成时，也需要增加概率采样。

吴恩达的deeplearning.ai推出了一门介绍扩散模型的入门课程[8]，里面有基础版的代码实现，摘录如下。

2.4.1、训练
图片

图-7

2.4.2、推理
图片

图-8

2.5、扩散模型的数学原理
DDPM有大段的数学推理，此处不做详细介绍。

从数学上可以证明，前向过程中，X(t)可以直接从X(0)叠加高斯噪声一步得到，无需t步迭代。也就是说扩散t步的结果，与中间步骤的扩散结果无关，直接从初始状态X(0)和步数t就能得到。这样就免去了串行扩散的计算开销。背后的原理是t个高斯分布的和仍然是高斯分布。这是上述2.4节训练时X(t)通过采样得到的原因。

反向过程无法一步得到。

3、标题与摘要
文章[9]的标题“Hierarchical Text-Conditional Image Generation with CLIP Latents”包含几个关键词：

Hierarchical：先生成64* 64的图片，再上采样成256* 256，继续上采样到1024 * 1024。
Text-Conditional：输入文本作为prompt，根据文本生成图片。
CLIP Latents：decoder部分的扩散模型输入不是白噪声，而是CLIP多模态空间中的图片embedding。输入文本先通过CLIP编码成文本向量，再通过Prior模型转换成图片embedding。Prior模型在训练时以CLIP生成的图片embedding做label，所以它推理生成的embedding符合CLIP多模态空间的语义。
摘要主要内容包括：

本文采用两阶段方案而非end-to-end方案：先由prior模型，根据文本输入，生成CLIP向量空间的图片embedding。再由decoder生成最终图片。
decoder采用扩散模型。
prior尝试了auto regressive模型和扩散模型，实验表明后者计算更高效且生成的图片质量更高。
4、导言
DALLE2构建在两大基础工作之上：CLIP和扩散模型。

CLIP的优势是对图像和文本进行联合建模，在统一的多模态空间中生成图片embedding和文本embedding。

扩散模型已经取代了GAN成为最热门的图像生成技术。

扩散模型本身不依赖CLIP，DALLE2用CLIP模型做文本编码器，主要是为了得到多模态的文本embedding，方便控制图片生成过程。

5、方法
训练数据集包括图像-文本对(x,y)，和CLIP模型类似。系统架构如图-1所示。

虚线上面是CLIP模型，下面是text-to-image图像生成模型。CLIP模型预先训练好，在text-to-image训练过程中，CLIP模型参数不变，不参与训练。
文生图模型包含prior和decoder两个部分。
输入文本y通过CLIP模型得到CLIP 文本embedding 。
Prior模型根据生成CLIP 图片embedding ，记作。
decoder根据生成图片x。
文生图的过程像是CLIP的逆过程，所以在原论文中DALLE2被称为unCLIP。
图片

图-9

5.1、Decoder
Decoder继承自GLIDE模型。它同时采用了CLIP guidance和classifier free guidance。

为了生成高清晰度的图片，文章采用了级联的方式，训练了两个上采样模型。一个模型负责从64 * 64到256 * 256，另一个模型负责从256 * 256到1024 * 1024。

5.2、Prior
Prior模型是为了从输入文本生成一个符合CLIP多模态空间的图片向量。文章采用了AR和diffusion两种方式。

这里主要介绍diffusion方式：

输入：encoded text、CLIP text embedding、timestep embedding、noised CLIP image embedding、占位符embedding用于做输出。
label：unnoised CLIP image embedding。
模型结构：decoder-only Transformer。
简而言之，该模型用文本作为输入，用该文本的CLIP image embedding作为label，目的是对任意文本输入都能生成CLIP image embedding。
5.3、讨论
模型结构部分文章介绍很少。各种技术细节如果不看源代码很难解释清楚。在此只能简要介绍如上，以后有机会看源代码在继续介绍。

图像生成有很多技巧，有些有用，有些没用。例如AR方式到底好不好用，到底预测噪声好还是预测原始图片好，不同的文章结论不完全一样。其实，这些奇技淫巧不是最重要的，最重要的是规模（scale），只要能把规模做上去，模型结构上的差异没有那么重要。

6、效果展示
6.1、以图生图
输入图片，DALLE2能够生成很多类似的图片。这些图片能够保留原图的风格，以及图片中的主要物体，即保留主要的语义信息，只改动非关键信息。

从unCLIP的架构图就能明白DALLE2是如何做到这一点的。

输入一张图片x，先经过CLIP得到图片向量zi，再得到对应的CLIP文本向量zt，再经过prior模型得到图片向量zi‘，最后经过decoder得到图片x’。

由于prior模型以CLIP图片向量作label，这里的zi‘能够尽量逼近zi，所以x’能够保留原图的主要语义信息。同时由于扩散模型的概率采样特性，能使生成的图片局部出现多样性。

这个能力对插画师、设计师很有用，可以帮助人们发散思维，进行创意组合。

6.2、图像内插
输入两张图片，在它们的图像特征之间按照不同比例做内插操作，能够生成不同风格的图片。

6.3、图像-文本内插
输入两段文本，模型在这两个文本之间做内插，生成风格渐变的图像。

这个能力可以用来做PS。输入文本，模型根据文本调整图片的风格和样式。

6.4、实验结果
图像生成的评估通常是用MS-COCO数据集的FID指标。结果如下图所示。

图片

图-10

7、 局限性
DALLE2不能很好地把物体和它的属性结合起来。例如，输入prompt “a red cube on top of a blue cube”，DALLE2不能识别“on top of”的语义。

其原因可能在于CLIP模型只考虑了文本和图片的相似性，而没法学习“on top of”这种属性信息。CLIP模型只能学习到“红方块和蓝方块同时出现”，不能学习到二者的相对位置信息。

另一个例子，输入prompt “a sign that says deep learning”，生成的图片确实像一个标语牌，但是上面的文字五花八门，基本都是错误的。

其原因可能是文本编码采用了BPE，是编码词根词缀，不是编码整个单词。

其它，例如公平性、伦理性、安全性等生成模型老生常谈的问题不再赘述。

8、脑洞时刻
自动数据增强，无限套娃
给GPT-3输入一段prompt，用它生成一段文本，再将文本扔给DALLE2生成图片，得到图片-文本对，通过这种方式，可以无穷无尽地生成图像-文本对，接下来可以用这些图像-文本对训练CLIP和DALLE2模型。

DALLE2方言
twitter上有个小哥认为DALLE2有一套自己的秘密语言[10]，虽然在我们看起来是乱码，但是DALLE2能够理解。输入特定的文本（乱码），它能够生成包含特定物体的图片。例如，有些单词表示“鸟”，有些单词表示”昆虫“，把这些单词混在一起，能够生成”鸟吃虫子“的画面。

当你发现DALLE2生成一堆乱码文字时，不妨把它继续扔给DALLE2试一试看能生成什么图像，说不定有惊喜。

DALLE2发明的这些黑话使得监管更加困难。

参考资料
[1]
2021-openai-dalle-blog: https://openai.com/research/dall-e

[2]
2022-openai-dalle2-blog: https://openai.com/dall-e-2

[3]
2020-openai-imagegpt: https://openai.com/research/image-gpt

[4]
2020-DDPM: https://arxiv.org/abs/2006.11239

[5]
2020-iDDPM: https://arxiv.org/abs/2102.09672

[6]
2021-diffusionbeatgan: https://arxiv.org/pdf/2105.05233v2.pdf

[7]
2021-openAI-GLIDE: https://arxiv.org/pdf/2112.10741.pdf

[8]
2023-course-diffusion: https://www.deeplearning.ai/short-courses/how-diffusion-models-work/

[9]
2022-openai-dalle2-paper: https://arxiv.org/abs/2204.06125

[10]
2022-dall2-hidden vocabulary: https://arxiv.org/abs/2206.00169,
