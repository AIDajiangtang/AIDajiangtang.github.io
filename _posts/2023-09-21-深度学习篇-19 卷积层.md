---
published: false
layout: post
title: "预告"
categories: 我的AI新书
date: 2023-09-21 00:00:00 +0800
excerpt: "预告"
---


卷积层



卷积是整个深度学习中最重要的概念之一



从一个简单的图片分类的例子出发：分类猫和狗


假设用手机拍摄了一张1200万像素的照片，而且是RGB图片，有R、G、B三个通道（channel），也就是有3600万个像素，每个像素对应有一个值
假设用一个单隐藏层的MLP来进行训练，隐藏层的大小是100，那么这个模型就有36亿个参数，这个数量远多于世界上所有的猫和狗的总数，还不如将所有的猫和狗全部记下来
所以这就是当使用MLP，特别是比较大的图片的时候所遇到的问题


MLP


输入有3600万个特征
隐藏层中有100个神经元，那么隐藏层一共就有100*3600万个参数，共36亿，大概是14GB，单层在不算运算的情况下就需要14GB的GPU内存，那么多层就更加困难了





在图片中找出指定的人物形象

在找的时候有两个原则


平移不变形：在任何一个地方，识别器不会因为图片像素出现的位置而发生改变
局部性：只需要看到局部的信息就可以了，而不需要看到全局的信息




怎样从全连接层出发，应用上面两个原则，得到卷积


之前的全连接层的输入输出是一维的向量，这里为什么又还原成了矩阵？因为要考虑空间的信息，所以将输入和输出变成矩阵，它有宽度和高度这两个维度
对应的可以将权重变成一个4维的张量，之前是输入的长度到输出的长度的变化，现在变成了输入的高宽到输出的高宽的变化，所以可以将它reshape成为一个4D的张量（加了高和宽两个维度，变成了4维，这里i、j对应的是输出的值在输出矩阵上的位置，k、l对应的是输入的值在输入矩阵上的位置，所以这里就抽象成了一个4维的张量，之前的权重之所以是2维的张量是因为之前的输入输出都是一维的向量，也可以理解成第一维表示输出向量的位置，第二维表示输入向量的位置，这里因为又还原成了矩阵，所以输入输出都变成了二维向量，所以说权重也变成了4维的张量）
w表示全连接层的权重，之前是2维的现在变成了4维，求和的是k和l两个坐标，遍历两个维度再求和，其实和二维张量表示的权重求和是类似的如下图所示

然后对W进行重新索引，将W中的元素进行重新排列，组成V，将W的下标变化一下从而引出卷积
总的来说就是将全连接层的输入输出变成二维的，然后将权重做一些变换




平移不变性


上图中第一个公式所存在的问题在于：输入x的平移会最终导致输出h的平移（hij等于后面的多项式的和，如果x换了一个新的坐标的话，则x中对应的每一项的v也会发生变化，因为vijab不仅考虑了输出h的绝对位置ij，也考虑了输入x相对于h的相对位置ab），所以根据平移不变性，不希望v考虑ij，不管输入x如何变化，输出h应该始终是不变的，所以需要加一个限制让v对ij不变，只随着ab的变化而变化。这里其实就已经和卷积类似了，这个求和可以理解成为一个a*b的矩阵感受野，ij分别对应的是输入x和输出h的某一个元素的坐标
这就是二维卷积，其实在严格意义上来说是二维上的交叉相关
平移不变性使得对权重做了一定的限制，去掉了权重的i、j维度，只剩下a、b维度，最后直接得到了二维卷积交叉相关的计算，所以可以认为二维卷积就是全连接或者说是矩阵的乘法，但是改变了一些权重，使得他里面的一些东西是重复的，也就是说，他不是每一个元素都是可以自由变换的（当把一个模型的取值范围限制之后，模型的复杂度就降低了，同样也就意味着不需要存取大量的元素）





局部性


局部性是说假设需要计算hij的输出的话，输入以ij为中心所有的地方都会去看，但是实际上不应该看得太远，hij的结果只应该由输入xij附近的点决定就可以了。也就是说，可以做一些限制（当a或者b的绝对值大于某一个数值的时候，就使得vab等于0），也就是说对于xij这个点，如果某点到它的距离超过某一个数值的时候就可以不用考虑了
最下面的公式等于是说对于Xij，只需要对于i往前Δ距离和往后Δ距离，对于j往前Δ距离和往后Δ距离的范围内进行求和，如下图所示





总结


所以说卷积是一个特殊的全连接层
将a和b限制在一个很小的值








卷积层






二维交叉相关


kernel：卷积核




二维卷积层


这里在边界上会丢掉一点东西（可以用池化操作来解决）
这里的 * 代表的是二维交叉相关的操作




例


不同的卷积核可以带来不同的效果，神经网络可以通过学习一些类似的核来得到类似效果




交叉相关和卷积


交叉相关和卷积是没有太多区别的，唯一的区别是在卷积上a、b的值有负号（卷积在索引W的时候是反过来走的，但是因为它是对称的，所以在实际使用的时候没有任何区别，神经网络为了方便，所以没有使用数学上严格定义的卷积，严格定义的话应该是要反过来的）
这里虽然是卷积，但是实际的计算实现做的是交叉相关，严格定义上卷积应该是反过来的交叉相关




一维和三维交叉相关


对一维来讲W实际上就是一个向量




总结




超参数就是卷积核的大小，卷积核的大小控制了局部性，卷积核越大看到的范围越广，卷积核越小看到的范围也就越小
卷积层可以看成一个特殊的全连接层
卷积解决了权重参数随着输入规模的增大而增大的问题，通过不变性减小了权重参数的数量








----end---- 作者：如果我是泡橘子 https://www.bilibili.com/read/cv14566185/?from=readlist&jump_opus=1 出处：bilibili