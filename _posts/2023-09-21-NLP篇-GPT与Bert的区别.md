---
published: false
layout: post
title: "字节对编码（Byte-Pair Encoding，BPE）：基于子词的分词算法"
categories: 我的AI新书
date: 2023-09-21 00:00:00 +0800
excerpt: "字节对编码（Byte-Pair Encoding，BPE）：基于子词的分词算法"
---

BERT和GPT是两种基于Transformer架构的预训练语言模型，它们在自然语言处理领域中有着重要的作用。它们的主要区别如下：

BERT是双向的，GPT是单向的。这意味着BERT可以同时利用上下文信息来理解和表示文本，而GPT只能利用上文信息来预测下一个词。这使得BERT在语言理解任务上更有优势，而GPT在语言生成任务上更有优势。
BERT是基于自编码的，GPT是基于自回归的。这意味着BERT的预训练目标是从被遮盖的输入中重建原始数据，而GPT的预训练目标是根据前面的词生成下一个词。这使得BERT可以更好地捕捉文本的整体语义，而GPT可以更好地生成流畅和连贯的文本。
BERT和GPT使用了不同的注意力机制。BERT使用了遮盖的多头自注意力（Masked Multi-Head Attention），这使得它可以关注到输入序列中任意位置的词，而忽略被遮盖的词。GPT使用了因果多头自注意力（Causal Multi-Head Attention），这使得它只能关注到当前位置之前的词，而不能看到当前位置之后的词。这符合了语言生成任务的因果性原则。


我们知道GPT，Bert以及LLAMA都是预训练模型，要想完成具体任务，需要再次基础上进行微调，那有几种微调技术呢？

GPT，Bert以及LLAMA都是基于Transformer架构的预训练模型，它们可以通过在大量无标注的文本数据上进行自监督学习，从而学习到语言的通用表示。要想完成具体的自然语言处理任务，如文本分类、命名实体识别、机器翻译等，需要在预训练模型的基础上进行微调（fine-tuning），即在任务相关的有标注数据上进行有监督学习，从而调整模型的参数以适应特定任务。

目前，有几种微调技术可以用于预训练模型，主要有以下几种：

全参微调（Full-Parameter Fine-Tuning）：这是最常见的一种微调技术，即在预训练模型的最后添加一个任务特定的输出层，然后对所有的模型参数进行更新。这种方法可以充分利用预训练模型的能力，但也存在一些问题，如过拟合、灾难性遗忘、计算资源消耗等12。
部分微调（Partial Fine-Tuning）：这种微调技术是对全参微调的一种改进，即只对预训练模型中的部分参数进行更新，而固定其他参数不变。这样可以减少计算资源的消耗，也可以避免对预训练模型中重要的知识进行破坏。部分微调可以根据参数的位置、类型、重要性等进行选择34。
提示微调（Prompt Fine-Tuning）：这种微调技术是基于提示学习（Prompt Learning）的思想，即通过构造一些特定的输入和输出格式来引导预训练模型完成任务。提示微调不需要修改预训练模型的结构，只需要在输入和输出之间添加一些可学习的参数作为提示（Prompt），然后对这些参数进行更新。这样可以保持预训练模型的完整性，也可以提高微调的效率和泛化能力5 。


主要有以下几种预训练语言模型的微调技术:

线性探测(Linear Probing):直接在预训练模型输出上添加线性分类层,进行迁移学习。适用于简单的文本分类任务。
微调(Fine-Tuning):使用下游任务的数据,通过梯度下降算法对整个预训练模型进行再训练,调整所有参数。这是最常用的微调技术。
适配微调(Adapter Tuning):只调整向预训练模型中插入的 lightweight adapter layers,而非调整预训练参数。加速调优速度。
前融合(Prefixed-Tuning):只调整预训练模型中的前几层,保持后面层的参数固定。
提示学习(Prompt Learning):将下游任务转换为Cloze形式,类似阅读理解,作为模型输入提示。
局部微调(Local Fine-tuning):仅调整预训练模型的一小部分参数,如Attention layers。
低精度训练(Low-precision Training):使用低精度参数来训练,以减少存储和加速推理。
知识蒸馏(Knowledge Distillation):让大模型去训练小模型,传递知识。
模块替换(Module Replacing):替换预训练模型的关键模块,如用轻量级的head。
选择合适的微调技术可以获得模型快速适应下游任务的能力,同时控制计算资源成本。