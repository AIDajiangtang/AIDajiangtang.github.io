---
published: false
layout: post
title: "词、子词和字符级Tokenization"
categories: 我的AI新书
date: 2023-09-21 00:00:00 +0800
excerpt: "词、子词和字符级Tokenization"
---

自然语言处理（NLP）是人工智能（AI）的一个分支，它使机器（计算机）能够以与人类相同的方式理解书面和口头的人类语言。NLP几乎无处不在，帮助人们完成日常任务。它已经成为一种常见的技术，我们经常将其视为理所当然。一些例子包括拼写检查、自动完成、垃圾邮件检测、Alexa或谷歌助手。NLP可能被视为理所当然，但人们永远不能忘记，机器处理的是数字而不是字母/单词/句子。因此，为了处理互联网上大量的文本数据，我们需要进行文本操作和清洗，通常在NLP中称为文本预处理。
预处理是处理文本和构建解决业务问题的模型的第一步。预处理本身是一个多阶段的过程。在本文中，我们只讨论分词和分词器。那么，让我们开始吧。
注意：我们主要关注英语语言。


分词
分词是文本预处理中最重要的步骤之一。无论您是使用传统的NLP技术还是使用先进的深度学习技术，都不能跳过这一步骤。🙅🏻
简单来说，分词是将短语、句子、段落、一个或多个文本文档分割成更小的单位的过程。🔪 这些较小的单位被称为token。现在，这些token可以是任何东西 - 单词、子词甚至是字符。不同的算法在执行分词时采用不同的过程，但下面给出的示例将让您对这三者之间的区别有一个基本的了解。
考虑以下句子/原始文本。
“Let us learn tokenization.”
基于单词的分词算法将将句子分解为单词。最常见的方法是基于空格进行分割。
[“Let”, “us”, “learn”, “tokenization.”]

基于子词的分词算法将把句子分解为子词。

[“Let”, “us”, “learn”, “token”, “ization.”]

基于字符的分词算法将把句子分解为字符。

[“L”, “e”, “t”, “u”, “s”, “l”, “e”, “a”, “r”, “n”, “t”, “o”, “k”, “e”, “n”, “i”, “z”, “a”, “t”, “i”, “o”, “n”, “.”]

token实际上是NLP的构建模块，所有的NLP模型都在token级别上处理原始文本。这些token用于构建词汇表，词汇表是语料库（NLP中的数据集）中一组唯一token。然后，将这个词汇表转换为数字（标识符），并用于建模。😎
在这里，我们提到了三种不同的分词技术。每种技术的工作方式都不同，并且具有各自的优缺点。让我们详细了解每种技术的细节，以便更深入地了解它们。🏇🏻


基于词的分词
这是最常用的分词技术。它根据分隔符将一段文本分割成单词。最常用的分隔符是空格。您还可以使用多个分隔符，例如空格和标点符号，来分割文本。根据所使用的分隔符，您将获得不同的单词级token。
可以使用自定义的正则表达式或Python的split()方法轻松进行基于词的分词。此外，Python中有许多库，如NLTK、spaCy、Keras、Gensim，可以帮助您轻松进行分词操作。


例如：

“Is it weird I don’t like coffee?”

通过使用空格作为分隔符进行基于词的分词，我们得到：

[“Is”, “it”, “weird”, “I”, “don’t”, “like”, “coffee?”]

如果我们观察到token“don't”和“coffee?”，我们会注意到这些单词后面有标点符号。如果在我们的语料库中有另一个原始文本（句子），如“I love coffee.”，这次会有一个标记“coffee.”，这可能会导致模型学习到不同的“coffee”表示（“coffee?”和“coffee.”），使单词（标记）的表示不够优化。🙆🏻
我们在进行分词时应该考虑标点符号的原因是，我们不希望我们的模型学习到同一个单词的每个可能的标点符号（当然是可以跟在单词后面的标点符号）的不同表示。如果我们允许模型这样做，我们将面临模型学习到的表示数量爆炸的问题（每个单词×在语言中使用的标点符号数量）。😳 因此，让我们考虑标点符号。


[“Is”, “it”, “wierd”, “I”, “don”, “’”, “t”, “like”, “coffee”, “?”]



这比之前的结果要好一些。然而，如果我们注意到，分词将单词“don't”分为了三个标记 — “don”、“’”和“t”。更好的分词方式应该是将“don't”分为“do”和“n't”，这样如果模型在未来看到一个单词“doesn't”，它会将其分词为“does”和“n't”，由于模型在过去已经学习到了“n't”的知识，它会在这里应用它的知识。这个问题听起来很复杂，但可以使用一些规则来处理。🤓
你可能已经注意到，最先进的NLP模型都有自己的分词器，因为每个模型都使用不同的规则来执行分词，除了使用空格进行分词外。因此，不同NLP模型的分词器可以为相同的文本创建不同的标记。空格、标点符号和基于规则的分词都是基于词的分词的示例。
然后，每个单词都使用一个标识符（ID）来表示，每个标识符包含了大量的信息，因为句子中的一个单词通常具有很多上下文和语义信息。😲


这种技术听起来令人印象深刻，但这种类型的分词会导致庞大的语料库，从而导致庞大的词汇表。😏最先进的模型Transformer XL使用空格和标点符号的分词，并且其词汇表大小为267,735。这是一个庞大的数字！这个庞大的词汇表大小导致模型的输入和输出层具有庞大的嵌入矩阵，使得模型变得更庞大，需要更多的计算资源。
这种分词还会给像“boy”和“boys”这样的单词分配不同的标识符，而这些单词在英语中几乎是相似的（一个是单数，另一个是复数）。实际上，我们希望我们的模型知道这些单词是相似的。


为了解决这个庞大词汇表的问题，我们可以限制可以添加到词汇表中的单词数量。例如，我们可以只保存在语料库中出现频率最高的5000个单词。模型将为这5000个常见单词创建标识符，并将其余的单词标记为OOV（词汇外）。但这会导致信息的丢失，因为模型将不会学习有关OOV单词的任何内容。对于模型来说，这可能是一个很大的妥协，因为它将为所有未知单词学习相同的OOV表示。🙄
另一个缺点与拼写错误的单词有关。如果在语料库中将“knowledge”拼写为“knowldge”，模型将为后者单词分配OOV标记。
因此，为了解决所有这些问题，研究人员提出了基于字符的分词方法。


基于字符的分词
基于字符的分词器将原始文本分割成单个字符。这种分词的逻辑是，语言中有很多不同的单词，但字符的数量是固定的。这导致了一个非常小的词汇表。😍
例如，在英语中，我们使用256个不同的字符（字母、数字、特殊字符），而它的词汇表中有接近17万个单词。因此，与基于词的分词相比，基于字符的分词将使用较少的标记。
基于字符的分词的主要优点之一是几乎没有或非常少的未知或OOV（词汇外）单词。因此，它可以使用每个字符的表示来创建未知单词（在训练期间未见过的单词）的表示。另一个优点是可以纠正拼写错误的单词，而不是将它们标记为OOV标记并丢失信息。


这种类型的分词方法相当简单，可以大大降低内存和时间复杂度。那么，它是最好或完美的分词算法吗？🤔答案是否定的（至少对于英语）！一个字符通常不像一个单词那样携带任何含义或信息。😕
注意：有一些语言在每个字符中携带了大量的信息，因此基于字符的分词在这些语言中可能是有用的。
此外，减小词汇表的大小与基于字符的分词中的序列长度之间存在权衡。每个单词被分割为每个字符，因此分词后的序列比初始的原始文本要长得多。例如，“knowledge”这个单词将有9个不同的标记。🙄


注意：Karparthy、Radford等人，Kalchbrenner等人以及Lee等人的研究人员展示了字符级分词的使用，并取得了一些令人印象深刻的结果。阅读这些论文以了解更多信息！
尽管字符级分词存在一些问题，但已经解决了词级分词面临的许多问题。让我们看看我们是否能够解决字符级分词面临的问题。




基于子词的分词
另一种常见的分词方法是基于子词的分词，它介于基于词和基于字符的分词之间。其主要思想是解决词级分词（词汇表非常庞大，大量的OOV标记和非常相似的单词有不同的含义）和字符级分词（序列非常长，单个标记含义较少）所面临的问题。
基于子词的分词算法遵循以下原则：
不要将常用的单词拆分为更小的子词。
将罕见的单词拆分为更小的有意义的子词。
例如，“boy”不应该被拆分，但“boys”应该被拆分为“boy”和“s”。这将帮助模型学习到“boys”这个词是由“boy”这个词形成的，具有略有不同的含义，但是相同的词根。
在本文的开头，我们将单词“tokenization”拆分为“token”和“ization”，其中“token”是根词，“ization”是附加信息的第二个子词标记。子词的拆分将帮助模型学习到具有与“token”相同词根的词（例如“tokens”和“tokenizing”）在含义上是相似的。它还将帮助模型学习到“tokenization”和“modernization”由不同的词根组成，但具有相同的后缀“ization”并在相同的句法环境中使用。另一个例子是单词“surprisingly”。基于子词的分词将其拆分为“surprising”和“ly”，因为这些独立的子词出现的频率更高。




基于子词的分词算法通常使用特殊符号来表示哪个单词是标记的开始，哪个单词是标记开始的完成。例如，“tokenization”可以被分割为“token”和“##ization”，其中“token”表示单词的开始，“##ization”表示单词的完成。
不同的自然语言处理模型使用不同的特殊符号来表示子词。BERT模型使用“##”作为第二个子词的表示。请注意，特殊符号也可以添加到单词的开头。
在英语语言中，大多数取得最先进结果的模型都使用了某种形式的基于子词的分词算法。一些常见的基于子词的分词算法包括BERT和DistilBERT使用的WordPiece，XLNet和ALBERT使用的Unigram，以及GPT-2和RoBERTa使用的Byte-Pair Encoding。
基于子词的分词使得模型可以拥有较小的词汇表，并能够学习到有意义的上下文无关表示。甚至可能模型处理从未见过的单词，因为拆分可能会产生已知的子词。
因此，我们看到分词方法随着时间的推移不断发展，以适应自然语言处理领域不断增长的需求，并提供更好的解决方案来解决问题。


References:

https://huggingface.co/docs/tokenizers/python/latest/
The links to the related research papers are provided in the article.


https://towardsdatascience.com/word-subword-and-character-based-tokenization-know-the-difference-ea0976b64e17
